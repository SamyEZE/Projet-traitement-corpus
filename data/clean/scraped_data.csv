instruction,context,response,category
[D] How are subspace embeddings different from basic dimensionality reduction?,"I have been struggling to understand how more basic dimensionality reduction techniques differ from more advanced methods, mainly in whether the same intuition about subspaces, manifolds, etc. extends to the more basic methods. I understand how things like PCA, t-SNE, UMAP, etc etc work (and these are 90% of what comes up when looking for dimensionality dimensionality reduction), but when I read about subspace clustering, manifold learning, or things in this area, they rarely mention these more basic dim reduc techniques and instead opt for more advanced methods and I'm not sure why, especially given how prolific PCA, t-SNE, and UMAP seem to be.

It is unclear to me whether/how things like PCA are different from say manifold learning, particularly in their usefulness for subspace clustering. I think the goals of both are to find some latent structure, with the intuition that working in the latent space will reduce noise, useless / low info features, reduce the curse of dimensionality, and also potentially more clearly show how the features and labels are connected in the latent space. In terms of the actual algorithms, I am understand the intuition but not whether they are ""real"". For instance, in the case of manifold learning (which, FWIW, I don't really see any papers about anymore and don't know why this is), a common example is the ""face manifold"" for images, that is a smooth surface of lower dims than the original input dimensions, and smoothly transitions from every face to another. This may be a little more trivial for images, but for general time series data, does this same intuition extend? 

For instance, if I have a dataset of time series caterpillar movement, can I arbitrarily say that there exists a manifold of catepillar size (bigger catepillars move slower) or a manifold of caterpillar ability (say, some kind of ability/skill manifold, if the caterpillars are completing a task/maze)? Very contrived example, but basically the question is if it is necessarily the case that I should be able to find a latent space based on what my priors tell me should exist / may hold latent structure (given enough data)?

I know Yann LeCun is a big proponent of working in latent spaces (more so with joint embeddings, which I am not sure whether that is applicable to me and my time series data), so I am trying to take my work more in that direction, but it seems like there's a big divide between basic PCA and basic nonlinear techniques (eg the ones you would see built into scipy or sklearn or whatever) and techniques that are used in some other papers. Do PCA (or basic nonlinear methods) and the like achieve the same thing but just not as well?","This is an excellent question and a potentially extremely deep topic depending how into the weeds you are willing to get. 

The TLDR here is that modern SOTA approaches to basically any deep learning task utilize some kind of ""pre-training"" component aimed at utilizing ""self-supervised"" learning to model the data distribution (e.g. by applying an autoencoder learning objective) before specializing the model for a given task. Our current understanding of why this paradigm is so effective is that it is equivalent to a manifold learning objective. Deep neural networks are universal approximators and we deliberately train them in a heavily over-parameterized regime: this means that we are making very few assumptions about the underlying distribution of the data which grants the model a lot of flexibility to discover an efficient representational basis space. PCA assumes that this representational basis is linear. 

* I think you'll find this article especially interesting in the context of your ""face manifold"" discussion: https://arxiv.org/pdf/2310.02557
* Oldie but goodie: https://papers.nips.cc/paper_files/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html
* Additional related topics:
  * https://en.wikipedia.org/wiki/Mercer%27s_theorem
  * https://en.wikipedia.org/wiki/Manifold_hypothesis
  * https://en.wikipedia.org/wiki/Spectrum_(functional_analysis)#Spectrum_of_a_bounded_operator
  * https://en.wikipedia.org/wiki/Compact_operator_on_Hilbert_space#Spectral_theorem",general_qa
[D] How are subspace embeddings different from basic dimensionality reduction?,"I have been struggling to understand how more basic dimensionality reduction techniques differ from more advanced methods, mainly in whether the same intuition about subspaces, manifolds, etc. extends to the more basic methods. I understand how things like PCA, t-SNE, UMAP, etc etc work (and these are 90% of what comes up when looking for dimensionality dimensionality reduction), but when I read about subspace clustering, manifold learning, or things in this area, they rarely mention these more basic dim reduc techniques and instead opt for more advanced methods and I'm not sure why, especially given how prolific PCA, t-SNE, and UMAP seem to be.

It is unclear to me whether/how things like PCA are different from say manifold learning, particularly in their usefulness for subspace clustering. I think the goals of both are to find some latent structure, with the intuition that working in the latent space will reduce noise, useless / low info features, reduce the curse of dimensionality, and also potentially more clearly show how the features and labels are connected in the latent space. In terms of the actual algorithms, I am understand the intuition but not whether they are ""real"". For instance, in the case of manifold learning (which, FWIW, I don't really see any papers about anymore and don't know why this is), a common example is the ""face manifold"" for images, that is a smooth surface of lower dims than the original input dimensions, and smoothly transitions from every face to another. This may be a little more trivial for images, but for general time series data, does this same intuition extend? 

For instance, if I have a dataset of time series caterpillar movement, can I arbitrarily say that there exists a manifold of catepillar size (bigger catepillars move slower) or a manifold of caterpillar ability (say, some kind of ability/skill manifold, if the caterpillars are completing a task/maze)? Very contrived example, but basically the question is if it is necessarily the case that I should be able to find a latent space based on what my priors tell me should exist / may hold latent structure (given enough data)?

I know Yann LeCun is a big proponent of working in latent spaces (more so with joint embeddings, which I am not sure whether that is applicable to me and my time series data), so I am trying to take my work more in that direction, but it seems like there's a big divide between basic PCA and basic nonlinear techniques (eg the ones you would see built into scipy or sklearn or whatever) and techniques that are used in some other papers. Do PCA (or basic nonlinear methods) and the like achieve the same thing but just not as well?","In the case of unsupervised dimensionality reduction (PCA, t-SNE, UMAP, ...), more complex doesn't necessarily mean better. You need select the technique that is best suited to your specific data. Either you have a clear performance objective and you can easily compare the results, or you can't and need to choose the dimensionality reduction technique based on specific characteristics of the data (linear or nonlinear data, global or local structure, size of dataset, ...).

You talk about LeCun joint embeddings (probably JEPA). In this case, this is very different as the embedding is found by neural networks trained with a specific task (encoders). They learn a representation that is specifically suited to extract the high-level features of the problem you are trying to solve. For example, if you train a classifier to distinguish breeds of dogs, its bottleneck layer (i.e. latent space) will probably have features that represent color, shape, etc...  
""Can I arbitrarily say that there exists a manifold of catepillar size, ability, ..."" If I was clear, you guessed that yes you can train a neural network that will map your caterpillars in a manifold (subspace of the latent space) according to these features.

While I'm not really familiar with time series, I advise that if you have a clear objective function you use neural networks to obtain a latent space that is most suited to your problem.",general_qa
[D] How are subspace embeddings different from basic dimensionality reduction?,"I have been struggling to understand how more basic dimensionality reduction techniques differ from more advanced methods, mainly in whether the same intuition about subspaces, manifolds, etc. extends to the more basic methods. I understand how things like PCA, t-SNE, UMAP, etc etc work (and these are 90% of what comes up when looking for dimensionality dimensionality reduction), but when I read about subspace clustering, manifold learning, or things in this area, they rarely mention these more basic dim reduc techniques and instead opt for more advanced methods and I'm not sure why, especially given how prolific PCA, t-SNE, and UMAP seem to be.

It is unclear to me whether/how things like PCA are different from say manifold learning, particularly in their usefulness for subspace clustering. I think the goals of both are to find some latent structure, with the intuition that working in the latent space will reduce noise, useless / low info features, reduce the curse of dimensionality, and also potentially more clearly show how the features and labels are connected in the latent space. In terms of the actual algorithms, I am understand the intuition but not whether they are ""real"". For instance, in the case of manifold learning (which, FWIW, I don't really see any papers about anymore and don't know why this is), a common example is the ""face manifold"" for images, that is a smooth surface of lower dims than the original input dimensions, and smoothly transitions from every face to another. This may be a little more trivial for images, but for general time series data, does this same intuition extend? 

For instance, if I have a dataset of time series caterpillar movement, can I arbitrarily say that there exists a manifold of catepillar size (bigger catepillars move slower) or a manifold of caterpillar ability (say, some kind of ability/skill manifold, if the caterpillars are completing a task/maze)? Very contrived example, but basically the question is if it is necessarily the case that I should be able to find a latent space based on what my priors tell me should exist / may hold latent structure (given enough data)?

I know Yann LeCun is a big proponent of working in latent spaces (more so with joint embeddings, which I am not sure whether that is applicable to me and my time series data), so I am trying to take my work more in that direction, but it seems like there's a big divide between basic PCA and basic nonlinear techniques (eg the ones you would see built into scipy or sklearn or whatever) and techniques that are used in some other papers. Do PCA (or basic nonlinear methods) and the like achieve the same thing but just not as well?","Read up on the Manifold Hypothesis but the idea is that the higher dimensional data exist in a simpler lower dimensional manifold (latent space).   The purpose of these methods is to find this manifold or at least a ""useful"" approximation of it.  All of these methods make assumptions as to the construction of the latent space and its relationship to the higher dimensional observed output.  

For example, PCA is linear.   This means PCA models the higher dimensional data as a linear combination of the latent features.  PCA assumes that the ""basis"" vectors that describe the latent space are orthogonal which is how it can find them.  

Not all data have linear generators, hence the explosion of non-linear methods.  And not all latent spaces are connected (in the manifold sense), hence the explosion of DNNs.  Note that this is a crude approximation but should work.  Also a latent space can be described in different ways.  The main approaches we use to describe latent spaces are as ""physical"" systems or as ""statistical"" systems.  There are also ""statistical physics"" systems aka energy based descriptions.  The jury is still out on ""computational systems"" (this is what Wolfram is investigating) i.e. simple computational rules that give rise to the observed higher dimensional data.  

In addition to go back to DNNs, different datasets can be embedded (aka connected) into a joint latent space.





#",general_qa
[D] How are subspace embeddings different from basic dimensionality reduction?,"I have been struggling to understand how more basic dimensionality reduction techniques differ from more advanced methods, mainly in whether the same intuition about subspaces, manifolds, etc. extends to the more basic methods. I understand how things like PCA, t-SNE, UMAP, etc etc work (and these are 90% of what comes up when looking for dimensionality dimensionality reduction), but when I read about subspace clustering, manifold learning, or things in this area, they rarely mention these more basic dim reduc techniques and instead opt for more advanced methods and I'm not sure why, especially given how prolific PCA, t-SNE, and UMAP seem to be.

It is unclear to me whether/how things like PCA are different from say manifold learning, particularly in their usefulness for subspace clustering. I think the goals of both are to find some latent structure, with the intuition that working in the latent space will reduce noise, useless / low info features, reduce the curse of dimensionality, and also potentially more clearly show how the features and labels are connected in the latent space. In terms of the actual algorithms, I am understand the intuition but not whether they are ""real"". For instance, in the case of manifold learning (which, FWIW, I don't really see any papers about anymore and don't know why this is), a common example is the ""face manifold"" for images, that is a smooth surface of lower dims than the original input dimensions, and smoothly transitions from every face to another. This may be a little more trivial for images, but for general time series data, does this same intuition extend? 

For instance, if I have a dataset of time series caterpillar movement, can I arbitrarily say that there exists a manifold of catepillar size (bigger catepillars move slower) or a manifold of caterpillar ability (say, some kind of ability/skill manifold, if the caterpillars are completing a task/maze)? Very contrived example, but basically the question is if it is necessarily the case that I should be able to find a latent space based on what my priors tell me should exist / may hold latent structure (given enough data)?

I know Yann LeCun is a big proponent of working in latent spaces (more so with joint embeddings, which I am not sure whether that is applicable to me and my time series data), so I am trying to take my work more in that direction, but it seems like there's a big divide between basic PCA and basic nonlinear techniques (eg the ones you would see built into scipy or sklearn or whatever) and techniques that are used in some other papers. Do PCA (or basic nonlinear methods) and the like achieve the same thing but just not as well?",I would love to see more posts exactly like this in this sub,general_qa
[D] How are subspace embeddings different from basic dimensionality reduction?,"I have been struggling to understand how more basic dimensionality reduction techniques differ from more advanced methods, mainly in whether the same intuition about subspaces, manifolds, etc. extends to the more basic methods. I understand how things like PCA, t-SNE, UMAP, etc etc work (and these are 90% of what comes up when looking for dimensionality dimensionality reduction), but when I read about subspace clustering, manifold learning, or things in this area, they rarely mention these more basic dim reduc techniques and instead opt for more advanced methods and I'm not sure why, especially given how prolific PCA, t-SNE, and UMAP seem to be.

It is unclear to me whether/how things like PCA are different from say manifold learning, particularly in their usefulness for subspace clustering. I think the goals of both are to find some latent structure, with the intuition that working in the latent space will reduce noise, useless / low info features, reduce the curse of dimensionality, and also potentially more clearly show how the features and labels are connected in the latent space. In terms of the actual algorithms, I am understand the intuition but not whether they are ""real"". For instance, in the case of manifold learning (which, FWIW, I don't really see any papers about anymore and don't know why this is), a common example is the ""face manifold"" for images, that is a smooth surface of lower dims than the original input dimensions, and smoothly transitions from every face to another. This may be a little more trivial for images, but for general time series data, does this same intuition extend? 

For instance, if I have a dataset of time series caterpillar movement, can I arbitrarily say that there exists a manifold of catepillar size (bigger catepillars move slower) or a manifold of caterpillar ability (say, some kind of ability/skill manifold, if the caterpillars are completing a task/maze)? Very contrived example, but basically the question is if it is necessarily the case that I should be able to find a latent space based on what my priors tell me should exist / may hold latent structure (given enough data)?

I know Yann LeCun is a big proponent of working in latent spaces (more so with joint embeddings, which I am not sure whether that is applicable to me and my time series data), so I am trying to take my work more in that direction, but it seems like there's a big divide between basic PCA and basic nonlinear techniques (eg the ones you would see built into scipy or sklearn or whatever) and techniques that are used in some other papers. Do PCA (or basic nonlinear methods) and the like achieve the same thing but just not as well?","I was taught to think of autoencoders as just nonlinear PCA. Really, the whole difference is that PCA just has a single linear layer whereas  an autoencoder has multiple layers with activation functions that let it learn nonlinear relationships. 

Most other types of subspace embeddings reference autoencoder literature, so you can use that as the jumping off point. 

How I like to think about it (computer vision background) is that autoencoders care about improving reconstruction accuracy in *pixel space* meaning you just get the difference between each pixel in each image and get the MSE and that's the loss you're minimizing. Kinda makes sense initially, but when you dig into it you'll realize that the exact same image just shifted 10 pixels to the right has a terrible reconstruction accuracy even though semantically it is almost the exact same image. The result of this is that autoencoders tend to make really blurry reconstructions because the loss will be less impacted by a few pixel shifts in any direction. 

Most of the moves from there have been to change the objective to something we care about more than just raw pixel space accuracy. Where autoencoders just try to recreate the image in pixel space, GANs try to create an image that looks like it comes from the training distribution (i.e. it should be realistic and not blurry). Things like BYOL/SimCLR use a different objective saying that the neural network should create a latent space that is invariant to image augmentation, so you pass in the same image twice but with different rotations/scales/cropping/etc and tell the neural network it should create the same embedding both times. PixelRNN tries to condition each pixel on the previous pixels, i.e. a guess the next pixel game, which works kinda similarly to generative pre training. Lots of different objectives you can care about here depending on what you care about.",general_qa
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","Do you mean like the Trainer API? I do use it often. It's quite convenient for a lot of stuff like checkpointing/logging/eval. You can use it with any custom model (defined as a class), so you can definitely define your own loss & metrics.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","My two cents: From an academic research view, I personally would prefer if a paper’s GitHub repo did not rely on massive frameworks & can be implemented in simple, modular PyTorch. Whatever you do to make experimentation faster internally is all well and fine, but I would make sure whatever code you publicly put out to not be clouded by boilerplate & framework-specific code other than like raw PyTorch/jax/TF unless you really need a custom library",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","W&B is very different from Lightning. Weights and biases adds a lot of observability features but you can remove it and your code still works. 

Lightning handles a couple common patterns for you but in many ways puts itself between you and torch. I volunteer teach scientific computing and AI to 11-17 kids and have considered Lightning for that because otherwise, training a torch based NN can be ""verbose"". Ultimately, I think that verbosity is a feature, though and a little bit of good structure goes a long way. Every attempt to convert my labs to lightning gets reverted quickly because it gets more confusing rather less and then I struggle to implement certain operations in lightning.

I don't have experience with Trainer but I have a lot of trust in the HF team so I'll check it out.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","Since I started W&B, tracking my experiments has been significantly easier. Highly recommend it! As /u/sqweeeeeeeeeeeeeeeps mentioned, when you want to publish the paper, you will want to keep it as clean as possible, but 95% of your work is going to be in the development stage. It's much easier to get things working and clean them up later rather than writing thousands of unnecessary lines.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.",Lightning had started out to make research code easier to standardize and reduce boilerplate code. So that might be a reason to use it. And it does allow you to write custom train eval loops. But abstraction has its problems. E.g. hugging face can fail silently and is a nightmare to debug.,misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.",`accelerate` and `deepspeed` are definitely good to know if you have access to distributed hardware,misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","I really like Pytorch Lightning. I started using it a while ago when I need to do a multi-GPU setup and it was at the time (not sure if it still is) a complete pain to setup DDP in native Pytorch. I still use it today regardless of whether I'm running multi-GPU setups because it does abstract a lot of the boilerplate out of the process.

I would say that if do use one of these frameworks, don't over-invest in it. Lightning is a good example of this, where they offer things like CLI parsing up until 2.0, then they completely drop support and you have to use a completely different way.  Having a consistent setup both reduces friction, but also allows your to go back to previous projects and port things over quickly to build new setups fast.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","I do train models very often and they are very helpful if you do a lot of end-to-end R&D and training at work or in the academe where you need to train a new model or an existing model out there. Pytorch lightning is pretty good and i think it is already enough.

So a lot of papers out there publish their model's code. Most of them aren't ""trainable"" because 
- they dont publish the training code or 
- the training code are just for parallel 8 massive GPUs
- no monitoring, logs, graphs, images during training
- no modularity (capability to change optimizer, learning rate, steps, etc.)
- using your own dataset adapter
- poor documentation
- for academics, their coDe is utterly SH*T

Even lots of libraries out there use pytorch lightning (i think segmentation-models-pytorch or even some YOLOv). They have very cranky documentation that you have to dive deeper into their code and familiarity of the framework is crucial.

They make life easier and your experiments easier to document, replicate, scale, and design.

I saw a few trainers of Segment-Anything out there but they are all barely usable so I built our own using Pytorch lightning and it works.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","I've used Pytorch Lightning in the past and liked it. It makes things simple so you can focus on the data science instead of the programming. I remember that you can also add a bunch of callbacks to the training process if you want, so if you ever want to dig into the internals of the training loop, they're still accessible from the outside.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","I prefer if it was EASY to just get the `nn.Module` and a .ckpt file that can be loaded 'dumbly' via `model.load_state_dict(torch.load(path))`.

I work on my free time on a little library that has to integrate with various vision models to extract frame-level predictions (https://github.com/Meehai/video-representations-extractor) that I use for my PhD where i do various work on multitask/multimodel video models. I have to do SO many extra hoops just to extract the 'simple' part of the models.


[Rant on] Most notably, I've had 2 really long days for both Mask2Former (meta's internal detectron2 library) and FastSAM (ultralytics library) to extract the 'simple' torch model without 100+ import issues it's not even funny. M2f/Detectron needs a [800+ CFG yaml/python code monstrosity](https://github.com/Meehai/video-representations-extractor/blob/master/vre/representations/semantic_segmentation/mask2former/convert.py#L832) file that inherits from various sub CFG files just to be able to instantiate the M2F module and properly load the weights. It's so convoluted and tied with the library itself it's not even funny. [/rant off]

Good examples: DPT (depth estimation) and dexined (edge detection) were so easy to port it's night and day...",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.",skorch is nice,misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","Is your code a mess without a framework? If yes, then use one. If not, don't.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","meh I’ve found standard non-frills PyTorch to be more than adequate, but YMMV",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","pytorch lightning is meh fabric is better

ignite is cool

accelerate seems quite similar to fabric need to spend more time with it

huggingface trainer is like pytorch-lightning, too high level for my liking",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.",Pytorch Lightning can be convenient and it can also be a pain to debug. I very much like Lightning Fabric which is a nice balance between the features of Lightning and the flexibility and flow of standard pytorch. I've had pretty bad experiences using huggingface for anything research related besides downloading models.,misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","Yes. Most of my PhD research code is written in Keras. Use the tool that suits you best.

> Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

Yes. I *despise* writing boilerplate repeatedly. For me, Keras is low-enough level that I can do the experiments I need. If you prefer Torch Lightning/etc., use that.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","Honestly, use whatever you want as long as (a) it’s open source and (b) your code is readable. You don’t want to re-invent the wheel to track experiments or rewrite optimizations etc… if there are good libraries out there. It’ll make your life easier and it might help out some confused PhD student or researcher reading your code.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","I want to have full control over my code so I never used Lightning especially since code complete serves me well.

As others already mentioned

- use config file for every necessary arguments: LR, batch size, epochs, metadata etc. i’d recommend OmegaConf
- wandb for experiment tracking, make sure to pass the args which contains all info in the config file to wandb init. That’s what I found particularly useful
- you may want to learn a bit more about MLOps as well",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.",Don't use high-level frameworks. Spend time on the basics and write your own stuff. You'll benefit in a long term.,misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","Honestly I take back a code in pytorch lightning and it can be quite a mess to  tune  technical things 
 If you are fast processing a lot of différents models or dataset use it 
If you want to deep dive into sth just take the variable name and  write yourself the code 
But high level logging and command line interface  is a bit hard at the beginning but it ease so much things afterwards",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","MLflow is good, weights and biases (“wandb”) is good, the lightning trainer is useful but the rest of lightning is overcomplicated trash",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","Lightning is the only one I really like. It doesn’t crowd the code with framework specific code (like sqweeps mentioned), and actually reduces boilerplate a decent bit. I think even someone with zero Lightning experience could fully understand what is going on .",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","Yes, the Trainers. Interesting, which one do you use? And do you use it paired with any experiment tracker (or logger)?",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","That's a really good point. Would you say it shouldn't even have hyperparameter and result trackers to keep it as clean as possible? Or is it alright to use those? Currently I just save it in local json files, but I'm curious if it's better to use a tracker.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","I agree with the sentiment of your argument, but I feel the line you draw to be a but arbitrary. PyTorch will already be the most massive framework in the codebase, PyTorch Lightening specifically can be used to simplify an implementation to illustrate an idea more effectively. So I dont think „pure PyTorch“ is a good objective. Instead, in the context of research code, focus should be on a readable implementation. 

Important other points are imho:

 - I love WandB, but reliance on an external service should be minimized. Lightning will actually help with that, since it should be „logging provider“ agnostic. 
 - Docker is useful to bundle whatever „massive frameworks“ are used to make sure other researchers can run your code at all and a few years in the future",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","There's a fine balance, no? I would vastly prefer to be able to reproduce a paper's results easily rather than just having some dummy PoC code.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.",Do you use it paired with any PyTorch wrapper as well for Trainers? Just out of curiosity.,misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","The way Pytorch handles multi-gpu setups makes me really appreciate the simple/elegant way how jax approaches things. 
jax.pmap.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","Totally. Pytorch Lightning does bring a cool modularity vibe which translates great for understanding the core of things.

I also like Hydra. I don't like Pytorch ""only"" codes because experimentation is not only about the model per se. There's logging, visualization, repetition and statistics. It quickly becomes a mess.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","I just use the [Huggingface Trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer). You can easily work with chatGPT to modify your code for it. 

I don't use an experiment tracker for now, just a txt file, but I have been thinking about getting one.

Accelerate is also very good for fp16 training.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.",In the final code I wouldn’t. Have a config file for hyperparameters if u have a lot of different training recipes.,misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","True, it’s arbitrary. This is just my desires, not everyone’s. My objective or hope is that I should be able to quickly grab the piece/module of code that is the main advancement of the paper, so that I can use it with my models. A lot of repos require a large mess of dependencies where it’s difficult to get what you need",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.",I am not referring to code that doesn’t reproduce results. It should always reproduce,misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","I've heard about Huggingface's one as well, I'll take a look at the options and try something out. Thanks! I might try it paired up with Weights and Biases for tracking.",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","Correct me if I'm wrong, I believe these frameworks tend to reduce boilerplate code. But I do agree with you that code from published works should be as clean as possible for others to easily understand and possibly convert to the desired library, or are there other reasons to it?",misc
[D] Are PyTorch high-level frameworks worth using?,"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.

Would these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?

If you think they're useful, let me know which one you'd recommend.","Lightning is basically vanilla PyTorch these days. You don’t lose anything unless you’re doing something really niche, just makes it cleaner for the most part. 

Just use tensorboard or W&B to track metrics and experiments, and you’re all set. Very doable",misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","I feel like you probably won’t ever be able to cover all your basis here. What I’d do is: find a paper that you think is interesting, and try to read it. If the paper talks about some concept (e.g. diffusion) and you find yourself not fully comfortable with the concept, then check which paper the cite or just google the concept and you’ll find your way from there. I think the field has gotten a little too broad (and is developing very rapidly) to be able to cover all the important papers if your goal is to understand the current SOTA in some sub-field.",misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","GPT2, GPT3, DDPM, latent diffusion model, RLHF and DPO, AlphaZero, AlphaFold. Those are some of the most influential papers of the last 5 years.",misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",The NeRF and guassian splatting papers are big ones too.,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",ULMFIT was seminal in bringing transfer learning to NLP. Happened right before Bert and friends iirc,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",Someone compile the list in github and share here,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",Facebook's Segment Anything Model basically solved image segmentation,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","My hot take is DETR should be included. Using a transformer decoded to do single stage object detection is revolutionary, and inspired a lot of other works like PETR for the robotics / autonomous vehicle space.",misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","See https://punkx.org/jackdoe/30.html:

> List of 27 papers (supposedly) given to John Carmack by Ilya Sutskever: ""If you really learn all of these, you’ll know 90% of what matters today.""",misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","https://www.nature.com/articles/s42254-021-00314-5

Physics informed ml.",misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",[removed],misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",Why would GPT 2 and 3 be seminal? The only real change from the original GPT was scale iirc.,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",visual transformer instead of GPT3 i'd argue,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",i feel like you’re missing attention is all you need,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","If anyone tries to open it, remove the column at the end if it doesn't work",misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","I will be messaging you in 1 day on [**2024-05-18 01:57:41 UTC**](http://www.wolframalpha.com/input/?i=2024-05-18%2001:57:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/MachineLearning/comments/1cts99m/d_seminal_papers_list_since_2018_that_will_be/l4eabqm/?context=3)

[**3 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F1cts99m%2Fd_seminal_papers_list_since_2018_that_will_be%2Fl4eabqm%2F%5D%0A%0ARemindMe%21%202024-05-18%2001%3A57%3A41%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201cts99m)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","I remember when considerable parts of the big conferences were committed to the field of meta-learning.

Well, since the ""Language Models are Few-Shot Learners"", that is completely gone. Solved problem. The title is one of those things that seems obvious in hindsight, but it wasn't in 2020.",misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",They are influential for sure. LLMs wouldn’t have caught on that huge without OpenAI showing all those capabilities that scaling up models gives you.,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",They also moved the layer norm,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","""only"" scale, as if scale hasn't been the most important idea of the decade.

You're either blind or lying to yourself if you don't see GPT-3 as a seminal paper. It kicked off the current era of hyperscaling LLMs and billion-parameter pretrained models.",misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",I don’t include papers OP already know,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",OP already know that one,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",Isn't that 2017?,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","You think GPT-2 invented the idea of scale? What kind of kool-aid are you drinking?

We’ve understood the benefits of scale since Alex-Net. Since earlier even.",misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",aaah true,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.",What kind of cynicism are you drinking if you think it wasn't seminal? It resulted in tens of thousands of papers and billions of dollars of investment.,misc
[D] Seminal papers list since 2018 that will be considered cannon in the future,"Hi there,

A recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  

Where should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","In my opinion, research impact is different from how interesting/important a ""paper"" is, especially in the context of getting a good overview of such a big and diverse field.",misc
"[D] Machine Learning Engineers, what portion of your work is focused on deployment pipelines vs. model building/tuning?","I’m currently a machine learning engineer, but I focus much more heavily on the pipelines in a way that is similar to when I was a data engineer. I’d love to get more into the model building side of things, but my model knowledge has gotten a bit rusty since I finished my M.S. in Statistics.

What portion of your day to day work is focused on deploying compared to model building?","I'm responsible for the full ML life cycle, so probably 50/50 or close to that depending on the project.",general_qa
"[D] Machine Learning Engineers, what portion of your work is focused on deployment pipelines vs. model building/tuning?","I’m currently a machine learning engineer, but I focus much more heavily on the pipelines in a way that is similar to when I was a data engineer. I’d love to get more into the model building side of things, but my model knowledge has gotten a bit rusty since I finished my M.S. in Statistics.

What portion of your day to day work is focused on deploying compared to model building?",Do you mind telling me a bit about your educational/career background? I’d love to get into a role with more breadth in terms of the full life cycle,general_qa
[D] Real chances to be accepted in NeurIPS 2024 - Other conferences,"Hey!

This is my first time submitting to NeurIPS.

Does anyone know when the reviews are visible to the authors? August, or is it possible that earlier? If we have really bad reviews... The best thing is to exit the submission path, right? In that case, which alternatives do you recommend on those dates?

My topic is NN reliability, but I am always underconfident about my research and I always think that it is not enough, more if I think in a conference as Neurips. Do you think that everybody submits good papers or is there a large quantity of rubbish papers? I read a lot of bad opinions here about the reviewing process... So, I am a little afraid.

This year, there are 20000ish submissions. So, I don't know what to do, if continue the submission or submit to another conference. As the gap that I am filling is clear, I am sure that others are covering that gap and submitting it to NeurIPS. Is there any other conference that outputs the results first than NeurIPS? I am trying to think in a smart way. So hard to be a researcher...

Thank you!","Very few paper are so good that they have a high chance for acceptance. Some papers are so bad, their chance is close to zero. For the majority of submissions, it's down to luck and the chance is (clearly, looking at the number of submissions) far below 50%. Just don't be discouraged by a reject.",misc
[D] Real chances to be accepted in NeurIPS 2024 - Other conferences,"Hey!

This is my first time submitting to NeurIPS.

Does anyone know when the reviews are visible to the authors? August, or is it possible that earlier? If we have really bad reviews... The best thing is to exit the submission path, right? In that case, which alternatives do you recommend on those dates?

My topic is NN reliability, but I am always underconfident about my research and I always think that it is not enough, more if I think in a conference as Neurips. Do you think that everybody submits good papers or is there a large quantity of rubbish papers? I read a lot of bad opinions here about the reviewing process... So, I am a little afraid.

This year, there are 20000ish submissions. So, I don't know what to do, if continue the submission or submit to another conference. As the gap that I am filling is clear, I am sure that others are covering that gap and submitting it to NeurIPS. Is there any other conference that outputs the results first than NeurIPS? I am trying to think in a smart way. So hard to be a researcher...

Thank you!","The major 3 conferences (NeurIPS, ICLR, ICML) have deadlines and notification dates aligned in such a way that if you were rejected from say NeurIPS, you will be able to resubmit to the next one (ICLR) very soon. And you will have some idea of the result based on your reviews, so you will have some time to incorporate feedback and improve your paper (which you should be doing during rebuttal anyway).

I can't judge your paper unless I've reviewed it, but you will acquire a feel for paper quality over time. I don't think you should submit to a different conference - if the paper's good it will have a good chance, and if it's decent it will have a decent chance, and there's really no point in submitting a work you know is subpar to any conference. Generally our team always submits to the top 3 and shoot lower only if it is rejected all 3 times.",misc
[D] Real chances to be accepted in NeurIPS 2024 - Other conferences,"Hey!

This is my first time submitting to NeurIPS.

Does anyone know when the reviews are visible to the authors? August, or is it possible that earlier? If we have really bad reviews... The best thing is to exit the submission path, right? In that case, which alternatives do you recommend on those dates?

My topic is NN reliability, but I am always underconfident about my research and I always think that it is not enough, more if I think in a conference as Neurips. Do you think that everybody submits good papers or is there a large quantity of rubbish papers? I read a lot of bad opinions here about the reviewing process... So, I am a little afraid.

This year, there are 20000ish submissions. So, I don't know what to do, if continue the submission or submit to another conference. As the gap that I am filling is clear, I am sure that others are covering that gap and submitting it to NeurIPS. Is there any other conference that outputs the results first than NeurIPS? I am trying to think in a smart way. So hard to be a researcher...

Thank you!","Submitting has pretty much zero cost. You'll get feedback, and the paper will improve. Just submit, wait for reviews, rebut, hold onto the outcome loosely.",misc
[D] Real chances to be accepted in NeurIPS 2024 - Other conferences,"Hey!

This is my first time submitting to NeurIPS.

Does anyone know when the reviews are visible to the authors? August, or is it possible that earlier? If we have really bad reviews... The best thing is to exit the submission path, right? In that case, which alternatives do you recommend on those dates?

My topic is NN reliability, but I am always underconfident about my research and I always think that it is not enough, more if I think in a conference as Neurips. Do you think that everybody submits good papers or is there a large quantity of rubbish papers? I read a lot of bad opinions here about the reviewing process... So, I am a little afraid.

This year, there are 20000ish submissions. So, I don't know what to do, if continue the submission or submit to another conference. As the gap that I am filling is clear, I am sure that others are covering that gap and submitting it to NeurIPS. Is there any other conference that outputs the results first than NeurIPS? I am trying to think in a smart way. So hard to be a researcher...

Thank you!","If it's your first submission, you are overthinking. Just be patient to wait for the reviews, meanwhile, looking for some new/subsequent work to do. These are sincere suggestions out of my own experience. Less time on anxiety, more time on research/paper-reading.",misc
[D] Real chances to be accepted in NeurIPS 2024 - Other conferences,"Hey!

This is my first time submitting to NeurIPS.

Does anyone know when the reviews are visible to the authors? August, or is it possible that earlier? If we have really bad reviews... The best thing is to exit the submission path, right? In that case, which alternatives do you recommend on those dates?

My topic is NN reliability, but I am always underconfident about my research and I always think that it is not enough, more if I think in a conference as Neurips. Do you think that everybody submits good papers or is there a large quantity of rubbish papers? I read a lot of bad opinions here about the reviewing process... So, I am a little afraid.

This year, there are 20000ish submissions. So, I don't know what to do, if continue the submission or submit to another conference. As the gap that I am filling is clear, I am sure that others are covering that gap and submitting it to NeurIPS. Is there any other conference that outputs the results first than NeurIPS? I am trying to think in a smart way. So hard to be a researcher...

Thank you!","What do you have to lose if your paper is rejected? 

You're exactly where you are now, and it will be good practice to handle rejection in a more structured way and be able to handle it. 

Rejection is always going to be a part of writing academic papers, if it's going to crush you each time it happens that will be a lot to deal with.",misc
[D] Real chances to be accepted in NeurIPS 2024 - Other conferences,"Hey!

This is my first time submitting to NeurIPS.

Does anyone know when the reviews are visible to the authors? August, or is it possible that earlier? If we have really bad reviews... The best thing is to exit the submission path, right? In that case, which alternatives do you recommend on those dates?

My topic is NN reliability, but I am always underconfident about my research and I always think that it is not enough, more if I think in a conference as Neurips. Do you think that everybody submits good papers or is there a large quantity of rubbish papers? I read a lot of bad opinions here about the reviewing process... So, I am a little afraid.

This year, there are 20000ish submissions. So, I don't know what to do, if continue the submission or submit to another conference. As the gap that I am filling is clear, I am sure that others are covering that gap and submitting it to NeurIPS. Is there any other conference that outputs the results first than NeurIPS? I am trying to think in a smart way. So hard to be a researcher...

Thank you!","Hi op, I am in a similar situation. For me, I just thought that I will try to do my best with the paper and not think about the outcome. Helps a lot. Some of my teachers also told me the same - ""just do your best, since it's your first time even if it's not selected or come out bad, you will learn a lot about articulating your research.""",misc
[P] Real Time Emotion Classification with FER-2013 dataset,"So I am doing an internship project at a company that is as the title says.I basically need to classify human faces into 7 categories- Anger, disgust, happy, etc. Currently I'm trying to achieve good accuracy on FER 2013 dataset then I'll move to the Real Time capture part

I need to finish this project in like 2 weeks' time. I have tried transfer learning with models like **mobile\_net, VGG19, ResNet50, Inception, Efficient\_net** and my **training accuracy has reached to like 87% but validation accuracy is pretty low \~56%** (MAJOR overfitting, ik).

Can the smart folks here help me out with some suggestions on how to better perform transfer learning, whether I should use data augmentation or not( I have around 28000 training images), and about should I use **vision transformer**, etc. ?

with VGG19 and Inception , for some reason my validation accuracy gets stuck at 24.71% and doesn't change after it

ResNet50, mobile\_net and Efficient\_net are giving the metrics as stated above

This is a sample notebook I've been using for transfer learning  
[https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing](https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing)

Any and all help is appreciated!","No expertise in emotion classification though. If you have sufficient hardware resources, you can give ViT a shot. You can also check paperwithcode  to see the ranking of models on that dataset, if there’s any.",misc
[P] Real Time Emotion Classification with FER-2013 dataset,"So I am doing an internship project at a company that is as the title says.I basically need to classify human faces into 7 categories- Anger, disgust, happy, etc. Currently I'm trying to achieve good accuracy on FER 2013 dataset then I'll move to the Real Time capture part

I need to finish this project in like 2 weeks' time. I have tried transfer learning with models like **mobile\_net, VGG19, ResNet50, Inception, Efficient\_net** and my **training accuracy has reached to like 87% but validation accuracy is pretty low \~56%** (MAJOR overfitting, ik).

Can the smart folks here help me out with some suggestions on how to better perform transfer learning, whether I should use data augmentation or not( I have around 28000 training images), and about should I use **vision transformer**, etc. ?

with VGG19 and Inception , for some reason my validation accuracy gets stuck at 24.71% and doesn't change after it

ResNet50, mobile\_net and Efficient\_net are giving the metrics as stated above

This is a sample notebook I've been using for transfer learning  
[https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing](https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing)

Any and all help is appreciated!","I have a lot of experience with emotion in general. I know this isn’t what vision scientists want to hear but emotions can be felt without visible changes in face or body movement. Ground truth labels can be highly flawed. I’ve never tried a vision approach specifically for this because I’ve relied on electrodermal feedback signals. That all being said, try ViTs but don’t expect it to be real-time. At least you’ll get a better signal on accuracy with vision I guess.",misc
[P] Real Time Emotion Classification with FER-2013 dataset,"So I am doing an internship project at a company that is as the title says.I basically need to classify human faces into 7 categories- Anger, disgust, happy, etc. Currently I'm trying to achieve good accuracy on FER 2013 dataset then I'll move to the Real Time capture part

I need to finish this project in like 2 weeks' time. I have tried transfer learning with models like **mobile\_net, VGG19, ResNet50, Inception, Efficient\_net** and my **training accuracy has reached to like 87% but validation accuracy is pretty low \~56%** (MAJOR overfitting, ik).

Can the smart folks here help me out with some suggestions on how to better perform transfer learning, whether I should use data augmentation or not( I have around 28000 training images), and about should I use **vision transformer**, etc. ?

with VGG19 and Inception , for some reason my validation accuracy gets stuck at 24.71% and doesn't change after it

ResNet50, mobile\_net and Efficient\_net are giving the metrics as stated above

This is a sample notebook I've been using for transfer learning  
[https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing](https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing)

Any and all help is appreciated!",You should use data augmentation. Maybe try RandAugment. How many epochs are you training on? Usually you will train to full convergence (100% training accuracy). I think that you should definitely improve ResNet/EfficientNet/MobileNet before you try ViT and stuff. If you are overfitting that much its not a model issue.,misc
[P] Real Time Emotion Classification with FER-2013 dataset,"So I am doing an internship project at a company that is as the title says.I basically need to classify human faces into 7 categories- Anger, disgust, happy, etc. Currently I'm trying to achieve good accuracy on FER 2013 dataset then I'll move to the Real Time capture part

I need to finish this project in like 2 weeks' time. I have tried transfer learning with models like **mobile\_net, VGG19, ResNet50, Inception, Efficient\_net** and my **training accuracy has reached to like 87% but validation accuracy is pretty low \~56%** (MAJOR overfitting, ik).

Can the smart folks here help me out with some suggestions on how to better perform transfer learning, whether I should use data augmentation or not( I have around 28000 training images), and about should I use **vision transformer**, etc. ?

with VGG19 and Inception , for some reason my validation accuracy gets stuck at 24.71% and doesn't change after it

ResNet50, mobile\_net and Efficient\_net are giving the metrics as stated above

This is a sample notebook I've been using for transfer learning  
[https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing](https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing)

Any and all help is appreciated!",">... training accuracy has reached to like 87% but validation accuracy is pretty low \~56% (MAJOR overfitting, ik).

Diagnosis of overfitting has nothing to do with the difference between training and validation performance. Of the two, only the validation estimate is statistically unbiased. Typically, as model complexity increases or training proceeds, validation performance begins in an underfit state, reaches an extreme at optimality and (often though not always) degrades into overfit.",misc
[P] Real Time Emotion Classification with FER-2013 dataset,"So I am doing an internship project at a company that is as the title says.I basically need to classify human faces into 7 categories- Anger, disgust, happy, etc. Currently I'm trying to achieve good accuracy on FER 2013 dataset then I'll move to the Real Time capture part

I need to finish this project in like 2 weeks' time. I have tried transfer learning with models like **mobile\_net, VGG19, ResNet50, Inception, Efficient\_net** and my **training accuracy has reached to like 87% but validation accuracy is pretty low \~56%** (MAJOR overfitting, ik).

Can the smart folks here help me out with some suggestions on how to better perform transfer learning, whether I should use data augmentation or not( I have around 28000 training images), and about should I use **vision transformer**, etc. ?

with VGG19 and Inception , for some reason my validation accuracy gets stuck at 24.71% and doesn't change after it

ResNet50, mobile\_net and Efficient\_net are giving the metrics as stated above

This is a sample notebook I've been using for transfer learning  
[https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing](https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing)

Any and all help is appreciated!",Will the free version of Google colab have sufficient gpu compute time to run ViT?,misc
[P] Real Time Emotion Classification with FER-2013 dataset,"So I am doing an internship project at a company that is as the title says.I basically need to classify human faces into 7 categories- Anger, disgust, happy, etc. Currently I'm trying to achieve good accuracy on FER 2013 dataset then I'll move to the Real Time capture part

I need to finish this project in like 2 weeks' time. I have tried transfer learning with models like **mobile\_net, VGG19, ResNet50, Inception, Efficient\_net** and my **training accuracy has reached to like 87% but validation accuracy is pretty low \~56%** (MAJOR overfitting, ik).

Can the smart folks here help me out with some suggestions on how to better perform transfer learning, whether I should use data augmentation or not( I have around 28000 training images), and about should I use **vision transformer**, etc. ?

with VGG19 and Inception , for some reason my validation accuracy gets stuck at 24.71% and doesn't change after it

ResNet50, mobile\_net and Efficient\_net are giving the metrics as stated above

This is a sample notebook I've been using for transfer learning  
[https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing](https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing)

Any and all help is appreciated!",Hmm ohk I see I'll train the models till 95+ training accuracy and then check. I am currently doing only 25 epochs since it takes about an hour to train even with GPU on colab and I've gone through 5 burner accounts in the past 3 days for colab,misc
[P] Real Time Emotion Classification with FER-2013 dataset,"So I am doing an internship project at a company that is as the title says.I basically need to classify human faces into 7 categories- Anger, disgust, happy, etc. Currently I'm trying to achieve good accuracy on FER 2013 dataset then I'll move to the Real Time capture part

I need to finish this project in like 2 weeks' time. I have tried transfer learning with models like **mobile\_net, VGG19, ResNet50, Inception, Efficient\_net** and my **training accuracy has reached to like 87% but validation accuracy is pretty low \~56%** (MAJOR overfitting, ik).

Can the smart folks here help me out with some suggestions on how to better perform transfer learning, whether I should use data augmentation or not( I have around 28000 training images), and about should I use **vision transformer**, etc. ?

with VGG19 and Inception , for some reason my validation accuracy gets stuck at 24.71% and doesn't change after it

ResNet50, mobile\_net and Efficient\_net are giving the metrics as stated above

This is a sample notebook I've been using for transfer learning  
[https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing](https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing)

Any and all help is appreciated!","Not sure. Never tried before. But do make sure all the hyperparams are reasonable, i.e. epochs learning rate.",misc
[P] Real Time Emotion Classification with FER-2013 dataset,"So I am doing an internship project at a company that is as the title says.I basically need to classify human faces into 7 categories- Anger, disgust, happy, etc. Currently I'm trying to achieve good accuracy on FER 2013 dataset then I'll move to the Real Time capture part

I need to finish this project in like 2 weeks' time. I have tried transfer learning with models like **mobile\_net, VGG19, ResNet50, Inception, Efficient\_net** and my **training accuracy has reached to like 87% but validation accuracy is pretty low \~56%** (MAJOR overfitting, ik).

Can the smart folks here help me out with some suggestions on how to better perform transfer learning, whether I should use data augmentation or not( I have around 28000 training images), and about should I use **vision transformer**, etc. ?

with VGG19 and Inception , for some reason my validation accuracy gets stuck at 24.71% and doesn't change after it

ResNet50, mobile\_net and Efficient\_net are giving the metrics as stated above

This is a sample notebook I've been using for transfer learning  
[https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing](https://colab.research.google.com/drive/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing)

Any and all help is appreciated!","Well, if you're overfitting this much I don't think it'll be effective to continue training. But you should look into data augmentation first. You can also mess around with other things, like dropout and regularization and learning rate. I think the models being divergent from the beginning can be a sign that you just have terrible hyperparams/training params (such as stochastic depth). I admittedly have not done a ton of fine-tuning stuff,  so I can't give you good recommendations on that end.

Also, if you just need a model, have you considered just ripping a checkpoint of an existing model that has done well on FER?",misc
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","lol I googled it and there's this paper:
[The Power of Linear Recurrent Neural Networks](https://arxiv.org/pdf/1802.03308)
> Recurrent neural networks are a powerful means to cope with time series. We
show how autoregressive linear, i.e., linearly activated recurrent neural networks
(LRNNs) can approximate any time-dependent function f (t).

As a general matter you can approximate any function by using only linear functions. The key insight is to realize that function composition, e.g. f(g(x)), is a *linear operator*. It's also called the ""Koopman operator"" and you can find machine learning-related work on this.

This works because, given some function f(x), you can think of the scalar input value x as being an (uncountably) infinite dimensional, one-hot coded vector, and the function f as an (uncountably) infinite dimensional permutation matrix. Adjacency matrices on graphs are the discrete version of this.

EDIT: for the folks thinking ""hey but wait, I thought you needed a nonlinearity somewhere???"", consider this: according to quantum mechanics, the entire universe and everything in it is a linear equation. And then, for the folks thinking, ""hey but wait, isn't quantum measurement nonlinear?"": no, it's *nonunitary*, which is different.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","There is basic difference in SSM and RNN. First of all, State Space Model is a filter model. In contrast, RNN targets on the input itself to be another output in pre-learnt data. It is getting ambiguous between SSM and RNN after the appearance of MAMBA, but it is clearly job of filter to make clear input to keep distribution in preset, and job of prediction model to make variance from before-learnt data. So this is it. We use filter to filter out clear state, and use prediction model to figure out make variances in output.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?",I think it might be related to Koopman operators: mapping dynamical systems to high dimensions can be modelled with linear dynamics.,general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","the intuitive explanation is that, for instance in the case of the attention mechanism, its the softmax part that makes it non linear and makes inference so slow. by getting rid of the softmax (techniques such as linear attention and also rkwv etc) you can do inference as if it is an rnn


on the opposite side, what prevents RNN from beeing trained on all of a sequence at the same time is the non linear elements: by making an rnn linear it can be trained on the whole sequence at once like a transformer 


so linear rnn and linear attention ends up being the same thing.


F(a + b) = f(a) + f(b) makes it so you can have both parallel training and RNN like inference",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","Linear systems have fewer local minima, even though their minima is worse than the potential minima a non linear system can reach",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","From what I understand, the general motivation of these models is to use Linear RNN to *memorise* past inputs and still use nonlinear layers in between / afterwards to further process them. It turns out that linear transformations are often enough for reconstructing past inputs.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?",The idea is to separate the source of non-linearity (which is usually an MLP with a non-linear activation function) from the linear part (I.e. SSMs). That will be basically enough to make the linear SSMs universal approximators. This paper has some cool results for universality of S4-like SSMs (I.e. without selectivity): https://arxiv.org/abs/2307.11888,general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?",You can't (accurately) approximate general nonlinear functions with compositions of linear functions without adding a nonlinearity somewhere in your approximation.,general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?",">according to quantum mechanics, the entire universe and everything in it is a linear equation

This is by design as non linearities with that many variables are very difficult to solve. And we know it's not true because quantum mechanics is incomplete. Most physical phenomena are actually nonlinear",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","Sounds a lot like a reproducing kernel Hilbert space, is there a connection between the two?",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?",This is just extremely untrue. The simplest equation of motion s(t) = s(0) + vt + 0.5at^2 is non linear,general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?",Isn't an rnn just a special case of an ssm?,general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","Sure you can:

1. discretize the domain and range, using e.g. binning
2. represent functions as permutation matrices
3. profit

It's usually impractical, but you can definitely do it.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","It's a consequence of agreement with experiment;  it's asymptotically equivalent to classical mechanics as planck's constant goes to zero; and it is a superset all of classical probability theory, including markovian systems.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?",Possibly! I don't know much about reproducing kernel hilbert spaces so I can't give a good answer to that question.,general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?",If you say so I will not reject the point. But it has to be checked twice. Gradient matters. Estimator matters. Distribution differs. There are many properties that differs between SSM and RNN.,general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","You are defining a linear function on a feature space representation of your inputs, where the features are bins. 


But the feature map (binning) is nonlinear. 


Like I wrote before, you have to introduce a nonlinearity somewhere for this to work.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","Reproducing kernel hilbert spaces associate every point in your input space with a function, and evaluation as an inner product with that function.

However, not every function is in a reproducing kernel hilbert space, so if what you are talking about is the same thing, you might expect to end up learning a function ""nearby"", but never exactly on it even with arbitrary amounts of data.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","I agree, why would binning be considered linear?",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","The binning is actually linear. The bins are a choice of a basis set for a linear vector space, which is a subspace of the ""full accuracy"" infinite dimensional linear vector space.

You can represent the original input x as a dirac delta function, and the process of binning then consists of taking inner products between the elements of your basis set - the bins - and the infinite dimensional input vector x. This is a linear operation and an infinite dimensional version of matrix multiplication.

cc u/cajmorgans

EDIT: for anyone who is confused, you can choose your original basis set to be all delta functions. There's no nonlinearity in doing this.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","Your representation of x as a dirac delta function is a nonlinear representation. 


In other words, it's a nonlinear mapping from the original vector space in which x lives to a new vector space of functions.


To see this, note that the delta function representing x+y is not the sum of the two delta functions representing x and y.


You're just pushing the nonlinearity into a different corner. 


Nonlinear functions are not linear. You can't approximate them with linear functions in general. You're misinterpreting Koopman operator theory.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","I agree with other commenter that you can't make non-linear functions by only using linear combinations of linear functions. The mapping from X to dirac(X) is non-linear, hence the nonlinearity is already introduced in the pipeline.

It's like when you use a Linear Regression with a non linear mapping applied to the feature vector. The model is linear wrt the parameters but non-linear wrt the input, which is what we want.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?",Do you know any article discussing this topic? Sounds interesting if it’s the case,general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","(technically, I should have written ""generalized functions"" in place of functions above)",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","To be clear, x here does not represent a function, it's a state vector. Functions are linear operators like matrices.

And I think you're implicitly assuming that x+y isn't a valid state vector, but that's not so. You can interpret x+y as a statistical ensemble; the output of any function (i.e. matrix multiplication) is then an ensemble of possible outputs, represented as a vector.

This also how quantum mechanics works; in QM the entire universe and everything in it is a linear equation, and vectors like x+y are superpositions. Indeed that's a simple and reductive way of seeing how nonlinearity is unnecessary.

In order to use linear algebra you do have to choose a basis set, of course, but it's very weird to think of ""choosing a basis set"" as a nonlinear operation. I think it's better framed as an axiom.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","x is represented as a result of choosing a basis set, which is an *axiom*; all reasoning must begin somewhere.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","This is just a standard thing in functional analysis, which is the application of linear algebra to infinite dimensional vector spaces. It's also the traditional way of solving PDEs etc; if you read about e.g. finite element analysis, you'll see that the discretization is discussed entirely in terms of linear algebra as I have outlined above.

EDIT: for something concrete to look at, read about the Galerkin method: https://en.wikipedia.org/wiki/Galerkin_method",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","I didn't assume that x was a function. I assumed it was an element of a vector space. We need to assume it's an element of a vector space if we want to talk about whether a function f(x) is linear. 


You proposed a representation of x as the delta function, d\_x, centered at x. This delta function also live in a vector space. The mapping from x to d\_x is a nonlinear mapping between vector spaces. This is clear because d\_x+d\_y is not the same as d\_{x+y}.


So if you have a function f(x) that is nonlinear on the vector space in which x lives, and you try to approximate that function by first mapping x to d\_x then this is not a linear approximation. It might be linear in the generalized function space in which d\_x lives, but it is not linear in x.



> Functions are linear operators like matrices.


Only linear functions.


> And I think you're implicitly assuming that x+y isn't a valid state vector


I'm not assuming that. I'm only asserting that it's representation as a dirac delta function is not the sum of the representations you get from x and y, which means that you are using a nonlinear representation of x in the sense that it's nonlinear with respect to the original vector space in which x lives.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","I'm saying that you can say, axiomatically, that x is *always* a delta function, and that it was never anything else.

Also, any function from any set to any other set can be represented as matrix multiplication, because you can interpret it as a bipartite graph with the corresponding adjacency matrix.",general_qa
"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I’m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can’t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?

Are there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","Yes, if you get to choose your axioms willy nilly then you can do that, but I didn't think that's how most people would interpret the statement that a given function can be approximated by a linear function. 


When I talk about a function between two vector spaces, what it means for that function to be linear or nonlinear depends on the properties of those vector spaces. The axioms defining those vector spaces are implicitly assumed.


If you go back and say that x and y were actually members of a different spaces defined by different axioms, then you and I are now talking about two different functions.


In any case, it's an issue of phrasing",general_qa
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.","Congratulations on actually doing the leg work, this is very valuable.

I would not say that RoPE assigns lower scores the further apart tokens are, because it just rotates 2D vectors, which means that, due to that rotation, values can also increase with increasing distance. However it feels like sacrificing half of the usable embedding space in service of positional encoding. This feeling is probably wrong, but I am convinced there are better positional encoding methods possible.

Feel free to share details of your positional encoding method, if you want!",misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.",Looking forward to your experiment results!,misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.","Very cool stuff, any plans on open sourcing this / sharing code?",misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.",Super cool! Can you talk a little bit more about your positional encoding?,misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.","""Wow, the progress you've made with just a single A100 is truly impressive! It's exciting to see how your innovative approach to positional encoding could potentially lead to unlimited context length. Keep pushing the boundaries and leading the way in transformer pretraining!""",misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.",Wow.,misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.","Thanks! My method also sacrifices half of the channels in every head for both queries and keys. The way it works is - it's running a prefix scan, one for queries and one for keys, so the model ""feels"" the order of a sequence. The prefix scan alone doesn't work that well, but it turns out you just have to normalize the resulting vector. You could try dividing the vector by its magnitude, but then you lose the magnitude information which is quite important for calculating attention scores. Thus I settled on dividing by the square root of the vector's magnitude. Seems to work pretty well. Getting lower perplexity than RoPE or ALiBi and it can extrapolate without any modifications to longer sequences than seen during training. No hyperparameters to tune other than how many channels to use for positional encoding per head - but I just fixed it to ""half"" of the channels.",misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.","Sure, I don't know how far I will go with this until I run out of my money, but I can open source the code and weights",misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.",Explained it a bit [here](https://www.reddit.com/r/MachineLearning/comments/1ct9bgc/comment/l4b45ze/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.",This certainly is a new and interesting method. However I don't understand yet - what you mean by 'prefix scan'?,misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.","Do you (half-) normalize ""as you go"" or do you calculate the cumulative sum and then perform the normalization after?",misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.","Thanks for that! I was looking into modifications to BPE tokenisation, but needed a small code base to start experimenting with . This would be extremely useful.",misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.",[Prefix (cumulative) sum](https://en.wikipedia.org/wiki/Prefix_sum),misc
[R] Pretraining a byte-level 0.67B transformer on a single A100,"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. 

[Loss is going down!](https://preview.redd.it/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)

  
It's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt ""Try our brand new Virtual Wallet services"":

    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.
    - Virtual Wallet solution to provide our customers the best and fastest alternative
    - As soon as we have the premium virtual wallet, we can take it appropriately
    - When it is needed to allow perishables to be changed
    - On a demand scale that is extremely high and that is cost-effective
    - All the necessary information about the products we offer the above
    - All information required to contact our service providers and have permission
    - We also deliver Virtual Wallet solutions to providers on a service website
    - Support the customer and others as they are being contacted by the best customers and solutions
    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees
    - When it is needed to allow perishables to be changed
    - We provide brand new as well as exclusive Virtual Wallet services
    - All of our exceptional pricing and quality work
    - Virtual Wallet solutions in regions of our selected customers
    - Best example of our Virtual Wallet service
    - Also integrated Virtual Wallet service
    - Virtual Wallet service
    - Fully equipped independent lifetime assistance
    - Superior customer service
    - Excellent advanced services
    - Reputation as sales and advice
    - An impressive range of products
    - A concise service that meets your expectations and standards
    - A commitment to our customers’ expectations in corporate environment.
    - Professional service to our customers and our community.
    - All streamlined support on credit cards
    - A customer-facing experience
    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.
    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products
    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions
    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.
    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.
    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.","After, something like this:

    # assume we have q and k
    # assume q.shape = k.shape = (..., sequence_length, num_heads, head_dim)
    norm_eps = 1E-6
    qk_head_dim = q.size(-1)
    
    q, q_pos = q.split_with_sizes([qk_head_dim // 2, qk_head_dim // 2], -1)
    k, k_pos = k.split_with_sizes([qk_head_dim // 2, qk_head_dim // 2], -1)
    
    q_pos = q_pos.cumsum(-3)
    k_pos = k_pos.cumsum(-3)
    
    q_pos = q_pos * torch.linalg.vector_norm(q_pos, ord=2, dim=-1, keepdim=True).add(norm_eps).rsqrt()
    k_pos = k_pos * torch.linalg.vector_norm(k_pos, ord=2, dim=-1, keepdim=True).add(norm_eps).rsqrt()
    
    q = torch.cat([q, q_pos], dim=-1)
    k = torch.cat([k, k_pos], dim=-1)",misc
[R] Replace the object in the frame with a similar object,"Hello, I am doing research for my project and I need your help.  
There is a closed pizza box on the table in a 5 sec part of a movie, I want to replace this pizza box with Pizza H\*t, \*ominos, etc. There are only a few Pizza H\*t boxes in my dataset, so the artificial intelligence model will need to use a technique similar to ""Few Shot Learn"". I thought I could do it using Generative Adversarial Networks (GAN) but I couldn't go further than turning a horse into a zebra. I have enough knowledge about AI and software development but new in GAN s.

Which model should I use, how should I research, which keywords should I use etc., where can i start to search for proper result I need your help, thanks.",Why dont u try exemplar diffusion models?,misc
[R] Replace the object in the frame with a similar object,"Hello, I am doing research for my project and I need your help.  
There is a closed pizza box on the table in a 5 sec part of a movie, I want to replace this pizza box with Pizza H\*t, \*ominos, etc. There are only a few Pizza H\*t boxes in my dataset, so the artificial intelligence model will need to use a technique similar to ""Few Shot Learn"". I thought I could do it using Generative Adversarial Networks (GAN) but I couldn't go further than turning a horse into a zebra. I have enough knowledge about AI and software development but new in GAN s.

Which model should I use, how should I research, which keywords should I use etc., where can i start to search for proper result I need your help, thanks.","Greetings, I made progress in my research and found a model called ""Rewriting a Deep Generative Model"". I will try it and share the outputs.

[https://rewriting.csail.mit.edu/](https://rewriting.csail.mit.edu/)",misc
[R] Replace the object in the frame with a similar object,"Hello, I am doing research for my project and I need your help.  
There is a closed pizza box on the table in a 5 sec part of a movie, I want to replace this pizza box with Pizza H\*t, \*ominos, etc. There are only a few Pizza H\*t boxes in my dataset, so the artificial intelligence model will need to use a technique similar to ""Few Shot Learn"". I thought I could do it using Generative Adversarial Networks (GAN) but I couldn't go further than turning a horse into a zebra. I have enough knowledge about AI and software development but new in GAN s.

Which model should I use, how should I research, which keywords should I use etc., where can i start to search for proper result I need your help, thanks.","i didnt know there is a model like that is exist, i'll try thanks",misc
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Because you might want to keep your code for the next set of papers you're considering writing and don't want to help someone else beating you to the punch


Also releasing code implies putting in effort to make it usable by third parties and you as a phd student don't get paid for that. You have your next paper to write",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",lazy reviewers.,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Great question. There are multiple different factors contributing to this fact. I can think of some off the top of my head. 

But first the authors have their rights to not go opensource. They are required to show the validity of their work. Let's say you have a picture of the swan. Can you convince another person that without full disclosure? Probably, if you give them a peek through a reasonably sized hole. The code does not need to be included within the hole all the time.

Second is fairness. Many big tech companies get away with their publication without full disclosure of their training data or model. For example, google with attention all you need paper. ViT was actually much worse than CNNs without google's propietary data, JFT-3B. They claimed that ViT gets much better performance on ImageNet, *only if* it's trained on JFT-3B. How could reviewer replicate this work? Not only it would have taken 10+ years for ordinary researcher to train ViT-XL/16 on 3 billion images with a 1080ti (releaesed in 2017), but also they don't have access to that data. Nonetheless, it got published (NIPS). It wouldn't be fair to reject some random scholar's work because of lack of code / data, but accept big tech's work regardless.

Thrid thing is this: I can tell you though even if the codes are posted by the authors, they don't work majority of the time. And replicating the result is another story. Nonetheless, they get published. Why? Because reviewers are not given enough resources. They are asked to review the *paper* not the *work*. Reviewers shouldn't invest their own resources for validating, because well there is no compensation. It requires substantial amount of the time which is apparently the most important resouce for researchers bc they dont have money to save their time. 

Solution to this problem is complicated. But it is obvious that reviewer's time must be valued. I strongly think all reviews should be paid.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Reproducibility does not mean someone else can copy your work. It means they have enough information to do the same experiment. 

If you have a significant disagreement, then you get into the weeds of specific hardware and software.

So you publish an algorithm or a method of acquiring a dataset.  Someone else should be able to write their own implementation of your algorithm to verify it, or gather data using the same process. They will get different results but they should be statistically similar, and if they aren't then there is a problem and that becomes a discussion. 

In other fields, say physics, you describe the hardware you use and what it does, but you don't just have other people run the experiment in your lab.  They can use their own apparatus that does the same basic thing (a laser with the same power and frequency for example) In psych you might publish the questions asked in a survey and the overall result but not the raw data from the survey and not the web form used to ask the questions.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","I don't think it should be a hard requirement, but needs to be considered regarding reproducibility during review. What should be a requirement though is that code is public at the time of the camera ready version submission if authors list open sourcing their code as one of their contributions.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","code release is not mandatory for most fields. Infact even electrical engineering (not so far removed from CS) it is not commonplace to release code. Other times if the work is from industry there are many hoops to get code released and it may not even be possible. I do agree ideally papers should have code release when possible to minimize noise and value reproducibility.

Try emailing the authors.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",Code? implementation ? how else would most of them confernace papers fake their results then,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","I once helped a project that tried to create automated systems to help assess the reproducibility of publications on the use of NLP in Healthcare. One of their constructs was the availability of code. If you didn't share your code, you'd get points subtracted.
They submitted their work to EMNLP and it got rejected. About one month later, I read a news article on how several journals, among which was EMNLP, were caught using GPT to do their reviewing work for them. Not only did they not read any of the work, even their feedback was AI generated and some responses I found online even had the ""As an AI agent..."" part in the feedback.

So I guess the answer to your question on why things in academics aren't what they're supposed to be, is once again: because of the toxicity of academic journals.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Because very often, the research uses proprietary code from whatever company is paying for it, or the company decides that keeping the code might be more profitable. Another reason that happens with industrial robotics is that you would need some very platform-specific/home-made tools that you would aslo need to release.

Also, releasing and maintaining a decent non-trivial repo requires diverting resources, and not every company can do this.

I think that if the math/idea looks solid and interesting, not providing code shouldn't be an issue. Especially since people can also be dishonest with their code (e.g. I remember a thread here where people were complaining about some repo where the seeds were carefully cherry picked to hide failure cases)

Edit: I'm not super sure why I'm getting downvoted.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",Most code written by phds is going to be a spaghetti mess and you wouldn’t be able to use it anyway,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",In past I also worked in face antispoofing and I can confirm this. May be the reason is that these papers are not published in top tier conferences.,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","I submit code with all my papers and it never fails that I get at least one reviewer who says something like “I have a question about *minor thing* and it would help if the authors provided code” and then I get to explain that I did, in fact, submit my code. 🙃",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","I remember once that I tried for weeks to implement a paper about gate sizing in transistor networks, emailed the author to get their code and it turned out that they'd completely skipped implementing the fancy scheduling algorithm they outlined in their paper, instead doing it one by one - making their results completely suspect to boot. Needless to say I was slightly annoyed.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Providing source code isn't very typical in many scientific fields unless it's a goal of the project to develop an open source software tool. But you can always email the authors and ask for a copy of the code. 

I don't really understand the statement about the importance of reproducibility. Every model is dependent on it's training. Providing a source code is entirely different from working with a live/trained model. Even if you have the source code, your model would be different based on the data used to train it. Usually in the field of machine learning, accuracy is evaluated against holdout data or some performance benchmark. Precise reproducibility is not usually an important factor at least in my experience.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Well...there's absolute no excuse for that, but it is somehow easy to understand.

Today: who are the real, real big players in AI? We're talking about big companies. And they are all about making money, simple as that. And making money also involves marketing, look at a few tricks we see:

1. Some leaders in these companies say ""Oh, science needs to be open"", but we go check their published papers, 90% without code and not thoroughly developed.

2. More published papers means a signal you're a ""big player"" which means more money from investors to buy 1M GPUs. Regardless of the quality of said paper. Making code reproducible means formatting and following some good practices so others can use, since this time could be spent producing more signaling papers, they just won't do it.

In addition, no one is checking, because lo' and behold the same ""leaders""  inside the companies are part of the reviewing committee on conferences who actually have the power to enforce rules. But the question is, why would they shoot themselves in the foot?

The reasoning for companies and leaders is easy to understand. What I personally struggle to understand is the student/researcher who actually repeat, or worse, believe in these arguments. Things are so upside down nowadays that is common to see researchers saying: ""Oh, code is not that important, a well written paper is enough. "" Or ""Why should I run statistical tests to prove hypothesis, they also have their shortcomings"". Just absolute nonsense.

A more honest reasoning would be: ""I am a researcher, I need to publish something regardless, I'll jump some hoops to make it fast otherwise someone will publish it. "" That is more acceptable. But going out in the open AGAINST 100% clean code, and AGAINST hypothesis testing it is just plain stupidity. 

And finally, since nobody seems to care because in the end of the day everyone just wants a piece of the pie, the naive researcher who actually wants to reproduce and compare things is utterly f****, because the mentality in ML is 100% against that.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Most companies (including Google and Meta) make it really hard to release code, so you’d be restricting submissions to only academics and lose out on a lot of good papers",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","I think one of the problems is that there are a few boundary cases organizations use to hide behind. For example, if there are data privacy, or intellectual property concerns, that would in theory prevent some work from being published.

  
Could those issues be worked around? Yeah in nearly all cases it seems like you could release something - but most reviewers aren't actually downloading and running the implementations in the first place so why push for it.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","I read it's not a good practice to add code ( incase you make any update to it) instead to add github link to it, or like there's another page OSF where you can add the data and codes.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","there are too many reasons behind this. Some examples I encountered myself:

The system relies on unpublished APIs

The model trains with data containing pii .

We planned a series of papers so we don’t want the code gets out before they are all published.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Because it depends. If the papers is much more theoretical like the KAN paper or the one I'm curently reading: ""Information-theoretic analysis of generalization capability of learning algorithms"", its not reasonable to expect code in top of the theory since its not the main point or the paper is so general that you need a specific use case (so you write another paper for that).",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Half the time the papers aren't even peer reviewed. Most SOTA goes straight to aRxive, which is *not* an actual journal. Might as well post your paper to Reddit.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","I have a PhD in a computational science. Providing code is **worse** for reproducibility, because if there is an error in the author’s implementation, a sure way for that to be discovered is for other researches to implement it (i.e. attempt to reproduce the results) and fail to reproduce the results. Plus, if you’re a researcher in the field, it should be fairly straightforward to implement it.

Reproducibility doesn’t mean I give you my lab to do the experiment. It means you do your own experiment in your own lab to control for confounding variables.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Oh yes, I've been there and ita absolutely horrible. I researched anti spoofing methods using classical CV algorithms not NNs and played a strange game of did I fk up de algorithm implementation or does this not work.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",Because that would require the reviewers to review the paper as well as the code base. It would be very time consuming and challenging to anonymize that process.,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","And make sure nobody cites you in the process? The truth is that most researchers have ad-hoc code which is far from usable by most people, have no time or no desire to rehaul it in a usable structure,etc. And then you get bombarded with GitHub issues because your code is in fact still bad, only works on your own dev environment, etc. not to mention the amount of papers which cherry pick their results to an extent which releasing code would only reveal how poor their solution actually is. Besides, publishing is what gets you ahead in the academic game ; code is just extra work.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",What are your thoughts on obfuscating the public releases of your code? I had a collaborator recommend this and tbh it felt weird to me. I'm all for FOSS but at the same time agree with your point that as a PhD student I can undermine myself by publicizing code that I'm going to use for future papers.,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Yes and no. As far as I can remember, none of the major ML conferences make submitting and or open sourcing code a strict requirement for acceptance. I think it should be, but as it stands you need to play by the rules and judge a paper fairly even without being able to check the code.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",I always reject papers without code. This is a personal hard requirement.,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","> In psych you might publish the questions asked in a survey and the overall result but not the raw data from the survey and not the web form used to ask the questions.

  
FYI, that is not true anymore. Because a lot of psych research turned out to be not replicable, people nowadays actually are expected to post the raw data. Basically no serious researcher believes a psych study that does not put the raw data and analysis code in a repo like osf.io.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","should definitely have some sort of malus if there is no code and results are not easily reproducable. I doubt that most reviewers will look at the code, but the community after might do that at a later point",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",If it's not a hard requirement during the review period the authors aren't going to upload the code.,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",Why shouldn’t open sourcing the code be a requirement?,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Yep, just ignore those without. It is not worth the time. 



So I need to spend hours or most likely days to try and implement their idea, which might be bad to start with and probably not helping my case? Nope.



What I don't understand is why they are bothering with publishing it.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",wtffffff that's crazy. What do their reviewers do with their time then if GPT is used to do the reviewing work? They're just sitting all day twiddling their thumbs?,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Being allowed to publish at all has been getting more difficult within my company given all the internal hurdles we have to clear.  Releasing code is in most cases simply a bridge too far, and the internal clearance process alone would exceed the timeline of any conference.  So we typically know better than to even ask and try instead to include pseudocode in the paper or its supplemental material that is detailed enough to be implementable.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","> e.g. I remember a thread here where people were complaining about some repo where the seeds were carefully cherry picked to hide failure cases

You're getting downvoted because this shows why providing the code is important -- it allows you to reproduce the results and you will be able to tell if the authors have cherry-picked the results",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",If there's an error in the author's implementation all results are immediately suspect - this is a feature not a bug.,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",Reviewers review code?,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Yeah, junk research is a lot faster and easier than real research.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Even for papers that release code (using e.g. https://anonymous.4open.science/ or just linking github), reviewers never really look at the code. The only people who would look at it are the people who evaluate the artifacts (e.g. if you want some artifact awards for your paper).",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Release it when you're done milking it, imo.
Switching topics? Already have that next paper finished? Graduating next week? Let it loose",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Based on the implication that so many people in this sub wouldn’t even know that submitting code is rarely a requirement, I think that says a lot.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Preparing for a wave of potential negative opinion. Just trying to imagine how we could play with the incentive structure:

What would people think about having a small scoring bonus introduced for code availability/use-ability during publication.

Alternatively, rather than a legit score bonus (changing its publication probability), simply a banner or an icon next to their conference title indicating their score in this category (but not affecting the acceptance probability).",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",You might want to ask them why they dont require code.,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",Which conferences or journals have this as a hard requirement?,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","This is policy entrepreneurism, and not allowed by commonly held scientific review principles.  See the [CVPR reviewer tutorial](https://docs.google.com/presentation/d/e/2PACX-1vT8bmHIEI3fBLTqSJpTV41mSAkf8_Y-yxahXokAaa4KnqfOuFHFvNYtSzyheoh_wiwEebz_YbQV2ivN/pub?start=false&loop=false&delayms=3000&slide=id.g14388b2f8a3_1_661) for brief discussion on this.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",Nice. I would too.,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Just want to double down on this as someone who studied psych and has worked in adjacent fields like neuroscience… fields like this are working _very_ hard to publish reproducible results. It’s primarily why languages like R (common in psych) are so focused on open-source practices nowadays like literate programming with Rmarkdown/Quarto and data sharing with datalad/zenodo/OSF

The “reproducibility crisis” was potentially damning to the field and the practitioners have largely responded in earnest to fix it. As an opinion, I’d say that one of the minor reasons for the divide between R and Python in machine learning practice is that Python has much lower barriers to sharing accomplishments _because_ R users, who are largely part of the “reproducibility crisis” victims, have been criticised quite heavily for their lack of reproducibility.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","That's not true, plenty of papers promise to upload the code for camera ready and they do. Uploading anonymized code is a pain in the ass so I don't judge them",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","It's not just code, it's also the data and you could be using proprietary/private dataset that you can't/don't want to share. Or your code relies on some ""internal"" tools and libraries that are hard to separate and replace. Or your code contains other work-in-progress pieces that you don't want to share yet",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","never read anything that isnt form a reputable university and authors, I've been in the process in my uni seeing how easy it is to get a paper passed with just buzzwords and garbage anyhow that's not news nowadays.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",Well they do their actual job XD,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Same here. It’s quite difficult to publish code when there are proprietary tools or codebases involved. 

I have open sourced stand alone sections, but it is quite difficult to go through the pipelines without some of our automation code or in some cases the datasets have company private information in them so we can’t release them.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Exactly. We see how precarious the situation is when we have to sit down with researchers and discuss the importance of reproducibility in science. Non reproducible data, code, experiments only create mysticism. ""Attention is all you need"" just to 3 years later a MLP architecture reproduce almost the same performance with less parameters. We're walking in circles and people still want to defend this regime.

To be honest, a more realistic approach would be: Create a new journal, conference where the rule of the game is reproducibility 100%. There are a few journals in Statistics where each paper is associated with a 100% working, well developed package. Then we have to line up a few big names to champion that and start playing the game ""Oh, you only published at ICML/Neurips? I'm sorry, good idea, but since it isn't reproducible it's not good enough. "" Then the division is made: Those who want to meaningfully research things go to X, those who want to advertise their papers (while secretly wishing to make a start up out of it) go to Y.

It's way too much noise...",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","I'm not saying that it's not important! My point is that the researchers often don't have much weight in those decisions. 

A more realistics and less expensive approach, I think, is to require papers to include a limitations section and to encourage reporting failures (all the while reducing the promotional tone). Some conferences like CoRL have taken good steps in that direction. Personally I've made it a habit, code or not, to include a limitation section where I severely critisize what we're proposing.  I think it's more informative for readers than a repo they'd have to put two interns on.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","They don't even need to read the supplementary, why would they even read the code",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",I'm even sure that most reviewer do not know how to launch code,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","That's a personal hard requirement. If a result isn't reproducible, I have to trust the author with their result.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","In some fields, like biochemistry, researchers have private blacklists to ignore.  For example, any biochemistry paper of the form ""Finding that {agent} will {inhibit|promote} growth of {target}"" out of China is almost guaranteed to be research-free.   There is an incentive structure requiring publication of full time practicing doctors; so there is a publication infrastructure publishing individual papers for practice doctors.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",Ah so reviewers are just volunteers who have a full-time job and do reviewing once a year on the side? Sorry for asking I am curious and I don't know much about the publishing side of ML,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",and ML code is a pain in the ass to launch with cursed dependencies so I would assume most would just day they did,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",Are you actually checking that the code reproduces the result? I bet you don't. Then you still are trusting the author with their results,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","When you are a researcher you are supposed to also set aside time for reviewing other people's work, this is true for all research fields. The issue is, you generally are already massively overworked and reviewing is something you do either as secondary or tertiary importance level in your job, or in your free time, and, of course, you're not paid to do it.

As you can imagine it is quite hard to find reviewers, so lately a lot of inexperienced people are asked and sometimes ""paid"" by coupons for reducing the publishing fee.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?",You'll be surprised to know that I always do!,general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Cool thanks, that explains it well!",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Yeah, sorry but I call bull :) or you review a single paper in a week and that's your full-time job. Or, you just look at the code and are like ""no way I can run this like that, and I am not spending another hour trying to figure it out, REJECT"" which is not better.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","I don't really care about your opinion. It's a problem of reproducibility: if I cannot run the experiments and/or are not well documented, it's their problem, not mine. I always try to run everything to the best of my effort. I work everyday, sat and sun included. I don't review a lot of papers, fortunately.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","It is a problem of reproducibility but noone said you should be able to reproduce the work in under five minutes with no extra steps. Reproduction studies are hard work and might require an entire team of ppl to achieve, not just one sad reviewer who has been working every day including sat and sun.",general_qa
[D] What's up with papers without code?,"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","Well said.
The best some conferences do is to add a few silly checklists.

Apparently coming up with a basic template for coding (something that any Computer Science undergrad course requires for assignments) suddenly is too much for the people claiming to work on AGI and save the world hunger via universal basic income.

The way I see it: Conferences policy is biased and acting on bad faith (they don't want to cripple the business), and the researchers/students cope with that because they also want to build their portfolio to get a ""FAANG"" position.

All of course, in detriment to science: How many ""truths"" are out there and keep being repeated just because...someone said so and it's almost impossible to verify? How many researchers in the next decade have to waste time and resources just because someone simply didn't do their jobs in properly sharing their work?

It's infuriating...",general_qa
[P] Needle in a Needlestack (NIAN),"**Code**: [https://github.com/llmonpy/needle-in-a-needlestack](https://github.com/llmonpy/needle-in-a-needlestack)

**Website**: [https://nian.llmonpy.ai/](https://nian.llmonpy.ai/)

**Description**:

>Needle in a haystack (NIAH) has been a wildly popular test for evaluating how effectively LLMs can pay attention to the content in their context window. As LLMs have improved NIAH has become too easy. **Needle in a Needlestack** (**NIAN**) is a new, more challenging benchmark. Even GPT-4-turbo struggles with this benchmark.

>NIAN creates a list of limericks from a large database of limericks and asks a question about a specific limerick that has been placed at a test location. Each test will typically use 5 to 10 test limericks placed at 5 to 10 locations in the prompt. Each test is repeated 2-10 times.","> As LLMs have improved NIAH has become too easy

IMO the test was flawed from the beginning. It only tests the attention portion, not the actual usefulness of the fact that it attended to those tokens. NIAN kinda proves what a lot of people were saying regarding NIAH.",misc
[P] Needle in a Needlestack (NIAN),"**Code**: [https://github.com/llmonpy/needle-in-a-needlestack](https://github.com/llmonpy/needle-in-a-needlestack)

**Website**: [https://nian.llmonpy.ai/](https://nian.llmonpy.ai/)

**Description**:

>Needle in a haystack (NIAH) has been a wildly popular test for evaluating how effectively LLMs can pay attention to the content in their context window. As LLMs have improved NIAH has become too easy. **Needle in a Needlestack** (**NIAN**) is a new, more challenging benchmark. Even GPT-4-turbo struggles with this benchmark.

>NIAN creates a list of limericks from a large database of limericks and asks a question about a specific limerick that has been placed at a test location. Each test will typically use 5 to 10 test limericks placed at 5 to 10 locations in the prompt. Each test is repeated 2-10 times.","This new benchmark is based on a limericks dataset published in September 2021. [https://zenodo.org/records/5722527](https://zenodo.org/records/5722527)

 Given that gpt3.5 and 4 training cuts off in September 2021, it seems likely that GPT-4o trained on this dataset, while the others did not. this seems handpicked to make GPt-4o look better than it is?

Why not create a new dataset of synthetic limericks that cannot possibly be in the training set?",misc
[P] Needle in a Needlestack (NIAN),"**Code**: [https://github.com/llmonpy/needle-in-a-needlestack](https://github.com/llmonpy/needle-in-a-needlestack)

**Website**: [https://nian.llmonpy.ai/](https://nian.llmonpy.ai/)

**Description**:

>Needle in a haystack (NIAH) has been a wildly popular test for evaluating how effectively LLMs can pay attention to the content in their context window. As LLMs have improved NIAH has become too easy. **Needle in a Needlestack** (**NIAN**) is a new, more challenging benchmark. Even GPT-4-turbo struggles with this benchmark.

>NIAN creates a list of limericks from a large database of limericks and asks a question about a specific limerick that has been placed at a test location. Each test will typically use 5 to 10 test limericks placed at 5 to 10 locations in the prompt. Each test is repeated 2-10 times.","This. We need a problem in a haystack of problems.

Word or token retrieval is very misleading of what models can actually do at long context.",misc
[P] Needle in a Needlestack (NIAN),"**Code**: [https://github.com/llmonpy/needle-in-a-needlestack](https://github.com/llmonpy/needle-in-a-needlestack)

**Website**: [https://nian.llmonpy.ai/](https://nian.llmonpy.ai/)

**Description**:

>Needle in a haystack (NIAH) has been a wildly popular test for evaluating how effectively LLMs can pay attention to the content in their context window. As LLMs have improved NIAH has become too easy. **Needle in a Needlestack** (**NIAN**) is a new, more challenging benchmark. Even GPT-4-turbo struggles with this benchmark.

>NIAN creates a list of limericks from a large database of limericks and asks a question about a specific limerick that has been placed at a test location. Each test will typically use 5 to 10 test limericks placed at 5 to 10 locations in the prompt. Each test is repeated 2-10 times.","All versions of GPT-4-Turbo have knowledge cutoffs in 2023, as does GPT-4o and the Claude 3 family.

>this seems handpicked to make GPt-4o look better than it is?

Maybe with limericks, but not the benchmark itself. The NIAN benchmark predates GPT-4o by only a few months, gaining traction after the authors discovered that GPT-4o performs well on it.",misc
[P] Needle in a Needlestack (NIAN),"**Code**: [https://github.com/llmonpy/needle-in-a-needlestack](https://github.com/llmonpy/needle-in-a-needlestack)

**Website**: [https://nian.llmonpy.ai/](https://nian.llmonpy.ai/)

**Description**:

>Needle in a haystack (NIAH) has been a wildly popular test for evaluating how effectively LLMs can pay attention to the content in their context window. As LLMs have improved NIAH has become too easy. **Needle in a Needlestack** (**NIAN**) is a new, more challenging benchmark. Even GPT-4-turbo struggles with this benchmark.

>NIAN creates a list of limericks from a large database of limericks and asks a question about a specific limerick that has been placed at a test location. Each test will typically use 5 to 10 test limericks placed at 5 to 10 locations in the prompt. Each test is repeated 2-10 times.","The dataset needs to be downloaded, that wouldn't end up in the pretraining",misc
[P] Needle in a Needlestack (NIAN),"**Code**: [https://github.com/llmonpy/needle-in-a-needlestack](https://github.com/llmonpy/needle-in-a-needlestack)

**Website**: [https://nian.llmonpy.ai/](https://nian.llmonpy.ai/)

**Description**:

>Needle in a haystack (NIAH) has been a wildly popular test for evaluating how effectively LLMs can pay attention to the content in their context window. As LLMs have improved NIAH has become too easy. **Needle in a Needlestack** (**NIAN**) is a new, more challenging benchmark. Even GPT-4-turbo struggles with this benchmark.

>NIAN creates a list of limericks from a large database of limericks and asks a question about a specific limerick that has been placed at a test location. Each test will typically use 5 to 10 test limericks placed at 5 to 10 locations in the prompt. Each test is repeated 2-10 times.","Which was scrapped from X, so it probably is on the training set anyway. We need better benchmarks for sure.",misc
[R] Energy-based Hopfield Boosting for Out-of-Distribution Detection,"https://arxiv.org/abs/2405.08766

Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world. Outlier exposure methods, which incorporate auxiliary outlier data in the training process, can drastically improve OOD detection performance compared to approaches without advanced training strategies. We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy (MHE) to sharpen the decision boundary between the in-distribution and OOD data. Hopfield Boosting encourages the model to concentrate on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data. Our method achieves a new state-of-the-art in OOD detection with outlier exposure, improving the FPR95 metric from 2.28 to 0.92 on CIFAR-10 and from 11.76 to 7.94 on CIFAR-100.","Thanks for posting, I was just about to do it myself :)",misc
[R] Energy-based Hopfield Boosting for Out-of-Distribution Detection,"https://arxiv.org/abs/2405.08766

Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world. Outlier exposure methods, which incorporate auxiliary outlier data in the training process, can drastically improve OOD detection performance compared to approaches without advanced training strategies. We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy (MHE) to sharpen the decision boundary between the in-distribution and OOD data. Hopfield Boosting encourages the model to concentrate on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data. Our method achieves a new state-of-the-art in OOD detection with outlier exposure, improving the FPR95 metric from 2.28 to 0.92 on CIFAR-10 and from 11.76 to 7.94 on CIFAR-100.","This is interesting. Intuitively speaking, Hopfield network related method keeps individuality along the recurrent approach. I guess this conservation property helped the detection of outlier data.",misc
[R] Energy-based Hopfield Boosting for Out-of-Distribution Detection,"https://arxiv.org/abs/2405.08766

Out-of-distribution (OOD) detection is critical when deploying machine learning models in the real world. Outlier exposure methods, which incorporate auxiliary outlier data in the training process, can drastically improve OOD detection performance compared to approaches without advanced training strategies. We introduce Hopfield Boosting, a boosting approach, which leverages modern Hopfield energy (MHE) to sharpen the decision boundary between the in-distribution and OOD data. Hopfield Boosting encourages the model to concentrate on hard-to-distinguish auxiliary outlier examples that lie close to the decision boundary between in-distribution and auxiliary outlier data. Our method achieves a new state-of-the-art in OOD detection with outlier exposure, improving the FPR95 metric from 2.28 to 0.92 on CIFAR-10 and from 11.76 to 7.94 on CIFAR-100.",:D,misc
[D] Correct interpretation of Model.predict output.,"Currently taking the FCC Machine Learning Course.

I dont know how to correctly interpret the probabilities of the Model.predict function output. The CNN is meant to determine whether is an image of a cat or a dog. Some probabilities are negative and very low. I dont know how to interpret that data.  
Also mention the model achieves its goal with an aceptable margin.  
Here an example:

https://preview.redd.it/yyss5v202y0d1.png?width=759&format=png&auto=webp&s=b1141a6e7150fcfdf0b65dc2119de956a9fc8ec8

Here the code.

Link of images:

    wget https://cdn.freecodecamp.org/project-data/cats-and-dogs/cats_and_dogs.zip

Code:

    # 3
    train_image_generator = ImageDataGenerator(rescale=1./255)
    validation_image_generator = ImageDataGenerator(rescale=1./255)
    test_image_generator = ImageDataGenerator(rescale=1./255)
    
    train_data_gen = train_image_generator.flow_from_directory(
        train_dir,
        target_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size = batch_size,
        class_mode = 'binary')
    val_data_gen = validation_image_generator.flow_from_directory(
        directory = validation_dir,
        target_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size = batch_size,
        class_mode = 'binary')
    test_data_gen = test_image_generator.flow_from_directory(
        directory=test_dir,
        target_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size = batch_size,
        class_mode = 'binary',
        shuffle=False)

    # 4
    def plotImages(images_arr, probabilities = False):
        fig, axes = plt.subplots(len(images_arr), 1, figsize=(5,len(images_arr) * 3))
        if probabilities is False:
          for img, ax in zip( images_arr, axes):
              ax.imshow(img)
              ax.axis('off')
        else:
          for img, probability, ax in zip( images_arr, probabilities, axes):
              ax.imshow(img)
              ax.axis('off')
              if probability > 0.5:
                  ax.set_title(""%.2f"" % (probability*100) + ""% dog"")
              else:
                  ax.set_title(""%.2f"" % ((1-probability)*100) + ""% cat"")
        plt.show()
    
    sample_training_images, _ = next(train_data_gen)
    plotImages(sample_training_images[:5])
    
    
    

    # 5
    train_image_generator = train_image_generator = ImageDataGenerator(
        rotation_range = 360,
        horizontal_flip = True,
        vertical_flip = True,
        zoom_range = 0.2,
        shear_range = 60,
        rescale=1./255)
    
    

    # 6
    train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                         directory=train_dir,
                                                         target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                         class_mode='binary')
    
    augmented_images = [train_data_gen[0][0][0] for i in range(5)]
    
    plotImages(augmented_images)

    # 7
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dense(2))
    
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    model.summary()

    # 8
    history = model.fit(x = train_data_gen, 
                        epochs = epochs,
                        validation_data = val_data_gen)
    acc = history.history['accuracy']
    print(acc)

    # 9
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    
    epochs_range = range(epochs)
    print(epochs_range)
    print(acc)
    
    plt.figure(figsize=(8, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')
    
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.show()

    #10
    probabilities = model.predict(test_data_gen)
    print(probabilities)
    probabilities = np.argmax(probabilities, axis = 1)
    sample_test_images, _ = next(test_data_gen)
    plotImages(sample_test_images, probabilities=probabilities)

    # 11
    answers =  [1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
                1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
                1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
                1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
                0, 0, 0, 0, 0, 0]
    
    correct = 0
    
    for probability, answer in zip(probabilities, answers):
      if round(probability) == answer:
        correct +=1
    
    percentage_identified = (correct / len(answers)) * 100
    
    passed_challenge = percentage_identified >= 63
    
    print(f""Your model correctly identified {round(percentage_identified, 2)}% of the images of cats and dogs."")
    
    if passed_challenge:
      print(""You passed the challenge!"")
    else:
      print(""You haven't passed yet. Your model should identify at least 63% of the images. Keep trying. You will get it!"")","Apply a softmax. For instance, in the first entry of your probabilities:

    # Create tensor [-2.02e-01 -5.5332e-01]
    x = torch.tensor([-2.0200e-01, -5.5332e-01])
    # Softmax
    softmax = nn.Softmax(dim=0)
    softmax(x) # tensor([0.5869, 0.4131])

Only after the softmax can they be considered as probabilities, allowing you to do the argmax to get the classification.",misc
[D] Correct interpretation of Model.predict output.,"Currently taking the FCC Machine Learning Course.

I dont know how to correctly interpret the probabilities of the Model.predict function output. The CNN is meant to determine whether is an image of a cat or a dog. Some probabilities are negative and very low. I dont know how to interpret that data.  
Also mention the model achieves its goal with an aceptable margin.  
Here an example:

https://preview.redd.it/yyss5v202y0d1.png?width=759&format=png&auto=webp&s=b1141a6e7150fcfdf0b65dc2119de956a9fc8ec8

Here the code.

Link of images:

    wget https://cdn.freecodecamp.org/project-data/cats-and-dogs/cats_and_dogs.zip

Code:

    # 3
    train_image_generator = ImageDataGenerator(rescale=1./255)
    validation_image_generator = ImageDataGenerator(rescale=1./255)
    test_image_generator = ImageDataGenerator(rescale=1./255)
    
    train_data_gen = train_image_generator.flow_from_directory(
        train_dir,
        target_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size = batch_size,
        class_mode = 'binary')
    val_data_gen = validation_image_generator.flow_from_directory(
        directory = validation_dir,
        target_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size = batch_size,
        class_mode = 'binary')
    test_data_gen = test_image_generator.flow_from_directory(
        directory=test_dir,
        target_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size = batch_size,
        class_mode = 'binary',
        shuffle=False)

    # 4
    def plotImages(images_arr, probabilities = False):
        fig, axes = plt.subplots(len(images_arr), 1, figsize=(5,len(images_arr) * 3))
        if probabilities is False:
          for img, ax in zip( images_arr, axes):
              ax.imshow(img)
              ax.axis('off')
        else:
          for img, probability, ax in zip( images_arr, probabilities, axes):
              ax.imshow(img)
              ax.axis('off')
              if probability > 0.5:
                  ax.set_title(""%.2f"" % (probability*100) + ""% dog"")
              else:
                  ax.set_title(""%.2f"" % ((1-probability)*100) + ""% cat"")
        plt.show()
    
    sample_training_images, _ = next(train_data_gen)
    plotImages(sample_training_images[:5])
    
    
    

    # 5
    train_image_generator = train_image_generator = ImageDataGenerator(
        rotation_range = 360,
        horizontal_flip = True,
        vertical_flip = True,
        zoom_range = 0.2,
        shear_range = 60,
        rescale=1./255)
    
    

    # 6
    train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                         directory=train_dir,
                                                         target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                         class_mode='binary')
    
    augmented_images = [train_data_gen[0][0][0] for i in range(5)]
    
    plotImages(augmented_images)

    # 7
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dense(2))
    
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    model.summary()

    # 8
    history = model.fit(x = train_data_gen, 
                        epochs = epochs,
                        validation_data = val_data_gen)
    acc = history.history['accuracy']
    print(acc)

    # 9
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    
    epochs_range = range(epochs)
    print(epochs_range)
    print(acc)
    
    plt.figure(figsize=(8, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')
    
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.show()

    #10
    probabilities = model.predict(test_data_gen)
    print(probabilities)
    probabilities = np.argmax(probabilities, axis = 1)
    sample_test_images, _ = next(test_data_gen)
    plotImages(sample_test_images, probabilities=probabilities)

    # 11
    answers =  [1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
                1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
                1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
                1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
                0, 0, 0, 0, 0, 0]
    
    correct = 0
    
    for probability, answer in zip(probabilities, answers):
      if round(probability) == answer:
        correct +=1
    
    percentage_identified = (correct / len(answers)) * 100
    
    passed_challenge = percentage_identified >= 63
    
    print(f""Your model correctly identified {round(percentage_identified, 2)}% of the images of cats and dogs."")
    
    if passed_challenge:
      print(""You passed the challenge!"")
    else:
      print(""You haven't passed yet. Your model should identify at least 63% of the images. Keep trying. You will get it!"")","Your loss is computed from logits:

    from_logits=True

Therefore, your model is trained to output logits and not probabilities.

A logit for a given class is a logarithm of ratio of probability of being positive class (1) to probability of being negative class (0). As such, a logit isn't directly intrepretable as a certain probability in a binary/multi-class setting but shows the model's uncertainty with regards to a single given class. Higher magnitude of a logit corresponds to higher certainty for a given class. Logits can be any real number.

People already suggested to use softmax to convert logits to probabilities.",misc
[D] Correct interpretation of Model.predict output.,"Currently taking the FCC Machine Learning Course.

I dont know how to correctly interpret the probabilities of the Model.predict function output. The CNN is meant to determine whether is an image of a cat or a dog. Some probabilities are negative and very low. I dont know how to interpret that data.  
Also mention the model achieves its goal with an aceptable margin.  
Here an example:

https://preview.redd.it/yyss5v202y0d1.png?width=759&format=png&auto=webp&s=b1141a6e7150fcfdf0b65dc2119de956a9fc8ec8

Here the code.

Link of images:

    wget https://cdn.freecodecamp.org/project-data/cats-and-dogs/cats_and_dogs.zip

Code:

    # 3
    train_image_generator = ImageDataGenerator(rescale=1./255)
    validation_image_generator = ImageDataGenerator(rescale=1./255)
    test_image_generator = ImageDataGenerator(rescale=1./255)
    
    train_data_gen = train_image_generator.flow_from_directory(
        train_dir,
        target_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size = batch_size,
        class_mode = 'binary')
    val_data_gen = validation_image_generator.flow_from_directory(
        directory = validation_dir,
        target_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size = batch_size,
        class_mode = 'binary')
    test_data_gen = test_image_generator.flow_from_directory(
        directory=test_dir,
        target_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size = batch_size,
        class_mode = 'binary',
        shuffle=False)

    # 4
    def plotImages(images_arr, probabilities = False):
        fig, axes = plt.subplots(len(images_arr), 1, figsize=(5,len(images_arr) * 3))
        if probabilities is False:
          for img, ax in zip( images_arr, axes):
              ax.imshow(img)
              ax.axis('off')
        else:
          for img, probability, ax in zip( images_arr, probabilities, axes):
              ax.imshow(img)
              ax.axis('off')
              if probability > 0.5:
                  ax.set_title(""%.2f"" % (probability*100) + ""% dog"")
              else:
                  ax.set_title(""%.2f"" % ((1-probability)*100) + ""% cat"")
        plt.show()
    
    sample_training_images, _ = next(train_data_gen)
    plotImages(sample_training_images[:5])
    
    
    

    # 5
    train_image_generator = train_image_generator = ImageDataGenerator(
        rotation_range = 360,
        horizontal_flip = True,
        vertical_flip = True,
        zoom_range = 0.2,
        shear_range = 60,
        rescale=1./255)
    
    

    # 6
    train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                         directory=train_dir,
                                                         target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                         class_mode='binary')
    
    augmented_images = [train_data_gen[0][0][0] for i in range(5)]
    
    plotImages(augmented_images)

    # 7
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dense(2))
    
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    model.summary()

    # 8
    history = model.fit(x = train_data_gen, 
                        epochs = epochs,
                        validation_data = val_data_gen)
    acc = history.history['accuracy']
    print(acc)

    # 9
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    
    epochs_range = range(epochs)
    print(epochs_range)
    print(acc)
    
    plt.figure(figsize=(8, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')
    
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.show()

    #10
    probabilities = model.predict(test_data_gen)
    print(probabilities)
    probabilities = np.argmax(probabilities, axis = 1)
    sample_test_images, _ = next(test_data_gen)
    plotImages(sample_test_images, probabilities=probabilities)

    # 11
    answers =  [1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
                1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
                1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
                1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
                0, 0, 0, 0, 0, 0]
    
    correct = 0
    
    for probability, answer in zip(probabilities, answers):
      if round(probability) == answer:
        correct +=1
    
    percentage_identified = (correct / len(answers)) * 100
    
    passed_challenge = percentage_identified >= 63
    
    print(f""Your model correctly identified {round(percentage_identified, 2)}% of the images of cats and dogs."")
    
    if passed_challenge:
      print(""You passed the challenge!"")
    else:
      print(""You haven't passed yet. Your model should identify at least 63% of the images. Keep trying. You will get it!"")","Since OP asked about the interpretation I would also add the following: Be careful when using these ""probabilities"". I prefer the term ""confidence"". The softmax output has all the properties of a probability distribution, but it is dangerous to treat these values as a probability of the prediction being correct. 

For a well-calibrated model the confidence value and probability of the prediction being correct should be the same, but that does not need to be the case.

In case OP needs this (or is otherwise interested), you can read more about model calibration here: [https://scikit-learn.org/stable/modules/calibration.html](https://scikit-learn.org/stable/modules/calibration.html)",misc
[D] Correct interpretation of Model.predict output.,"Currently taking the FCC Machine Learning Course.

I dont know how to correctly interpret the probabilities of the Model.predict function output. The CNN is meant to determine whether is an image of a cat or a dog. Some probabilities are negative and very low. I dont know how to interpret that data.  
Also mention the model achieves its goal with an aceptable margin.  
Here an example:

https://preview.redd.it/yyss5v202y0d1.png?width=759&format=png&auto=webp&s=b1141a6e7150fcfdf0b65dc2119de956a9fc8ec8

Here the code.

Link of images:

    wget https://cdn.freecodecamp.org/project-data/cats-and-dogs/cats_and_dogs.zip

Code:

    # 3
    train_image_generator = ImageDataGenerator(rescale=1./255)
    validation_image_generator = ImageDataGenerator(rescale=1./255)
    test_image_generator = ImageDataGenerator(rescale=1./255)
    
    train_data_gen = train_image_generator.flow_from_directory(
        train_dir,
        target_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size = batch_size,
        class_mode = 'binary')
    val_data_gen = validation_image_generator.flow_from_directory(
        directory = validation_dir,
        target_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size = batch_size,
        class_mode = 'binary')
    test_data_gen = test_image_generator.flow_from_directory(
        directory=test_dir,
        target_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size = batch_size,
        class_mode = 'binary',
        shuffle=False)

    # 4
    def plotImages(images_arr, probabilities = False):
        fig, axes = plt.subplots(len(images_arr), 1, figsize=(5,len(images_arr) * 3))
        if probabilities is False:
          for img, ax in zip( images_arr, axes):
              ax.imshow(img)
              ax.axis('off')
        else:
          for img, probability, ax in zip( images_arr, probabilities, axes):
              ax.imshow(img)
              ax.axis('off')
              if probability > 0.5:
                  ax.set_title(""%.2f"" % (probability*100) + ""% dog"")
              else:
                  ax.set_title(""%.2f"" % ((1-probability)*100) + ""% cat"")
        plt.show()
    
    sample_training_images, _ = next(train_data_gen)
    plotImages(sample_training_images[:5])
    
    
    

    # 5
    train_image_generator = train_image_generator = ImageDataGenerator(
        rotation_range = 360,
        horizontal_flip = True,
        vertical_flip = True,
        zoom_range = 0.2,
        shear_range = 60,
        rescale=1./255)
    
    

    # 6
    train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                         directory=train_dir,
                                                         target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                         class_mode='binary')
    
    augmented_images = [train_data_gen[0][0][0] for i in range(5)]
    
    plotImages(augmented_images)

    # 7
    model = Sequential()
    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D((2, 2)))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(Flatten())
    model.add(Dense(64, activation='relu'))
    model.add(Dense(2))
    
    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy'])
    model.summary()

    # 8
    history = model.fit(x = train_data_gen, 
                        epochs = epochs,
                        validation_data = val_data_gen)
    acc = history.history['accuracy']
    print(acc)

    # 9
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    
    epochs_range = range(epochs)
    print(epochs_range)
    print(acc)
    
    plt.figure(figsize=(8, 8))
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.legend(loc='lower right')
    plt.title('Training and Validation Accuracy')
    
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.legend(loc='upper right')
    plt.title('Training and Validation Loss')
    plt.show()

    #10
    probabilities = model.predict(test_data_gen)
    print(probabilities)
    probabilities = np.argmax(probabilities, axis = 1)
    sample_test_images, _ = next(test_data_gen)
    plotImages(sample_test_images, probabilities=probabilities)

    # 11
    answers =  [1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,
                1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,
                1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,
                1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,
                0, 0, 0, 0, 0, 0]
    
    correct = 0
    
    for probability, answer in zip(probabilities, answers):
      if round(probability) == answer:
        correct +=1
    
    percentage_identified = (correct / len(answers)) * 100
    
    passed_challenge = percentage_identified >= 63
    
    print(f""Your model correctly identified {round(percentage_identified, 2)}% of the images of cats and dogs."")
    
    if passed_challenge:
      print(""You passed the challenge!"")
    else:
      print(""You haven't passed yet. Your model should identify at least 63% of the images. Keep trying. You will get it!"")","For clarification argmax can be applied without softmax. But if OP wants probabilities then yes, a softmax is necessary on the output. 

Also it is unnecessary to have 2 logits in the output layer for binary classification. Can get away with using 1 and instead of softmax use sigmoid",misc
"[D] Friday's Oxen.AI Water Cooler call: High-performance audio processing, Python vs Rust","At this Friday's [Oxen.AI](http://Oxen.AI) Water Cooler,

*  the ""Show & Tell  /  Where are you stuck?  /  What is your project?"" segment topic will be:

**High-performance audio processing**

[Oxen.ai](http://Oxen.ai) discord member Shalini Ananda, PhD, [https://www.linkedin.com/in/shalinianandaphd/](https://www.linkedin.com/in/shalinianandaphd/)  
will discuss her experimentation with Python vs Rust with audio workloads.   Preview here:  
[https://discord.com/channels/1104137825638682806/1145920256301338685/1240029110726561823](https://discord.com/channels/1104137825638682806/1145920256301338685/1240029110726561823)

* Greg Schoeninger,  CEO of Oxen.ai , u/FallMindless3563 , will share highlights from this week's SW2 Conference and his session ""Better Data, Better AI""

To join the Ai Water Cooler call or Paper Club Zoom call at Friday 10:00 AM Pacific time, click hard on the 'subscribe' button:

[https://lu.ma/oxen](https://lu.ma/oxen)",It's rust,misc
"[D] Friday's Oxen.AI Water Cooler call: High-performance audio processing, Python vs Rust","At this Friday's [Oxen.AI](http://Oxen.AI) Water Cooler,

*  the ""Show & Tell  /  Where are you stuck?  /  What is your project?"" segment topic will be:

**High-performance audio processing**

[Oxen.ai](http://Oxen.ai) discord member Shalini Ananda, PhD, [https://www.linkedin.com/in/shalinianandaphd/](https://www.linkedin.com/in/shalinianandaphd/)  
will discuss her experimentation with Python vs Rust with audio workloads.   Preview here:  
[https://discord.com/channels/1104137825638682806/1145920256301338685/1240029110726561823](https://discord.com/channels/1104137825638682806/1145920256301338685/1240029110726561823)

* Greg Schoeninger,  CEO of Oxen.ai , u/FallMindless3563 , will share highlights from this week's SW2 Conference and his session ""Better Data, Better AI""

To join the Ai Water Cooler call or Paper Club Zoom call at Friday 10:00 AM Pacific time, click hard on the 'subscribe' button:

[https://lu.ma/oxen](https://lu.ma/oxen)",🦀 Rustaceans Unite,misc
[R] Fully neuromorphic vision and control for autonomous drone flight,"Arxiv: https://arxiv.org/abs/2303.08778 (15 Mar 2023)  
https://www.science.org/doi/10.1126/scirobotics.adi0591 (15 May 2024)

Also they uploaded a number of videos a few hours ago:

[Supplementary Video 1](https://www.youtube.com/watch?v=NQUv7l56r1o)  
[Supplementary Video 2](https://www.youtube.com/watch?v=0xQU7WMR1Ys)  
[Supplementary Video 3](https://www.youtube.com/watch?v=UfKr1N8mu4c)  
[Supplementary Video 4](https://www.youtube.com/watch?v=hp8Rudld3sI)

Abstract:

> Biological sensing and processing is asynchronous and sparse, leading to low-latency and energy-efficient perception and action. In robotics, neuromorphic hardware for event-based vision and spiking neural networks promises to exhibit similar characteristics. However, robotic implementations have been limited to basic tasks with low-dimensional sensory inputs and motor actions because of the restricted network size in current embedded neuromorphic processors and the difficulties of training spiking neural networks. Here, we present a fully neuromorphic vision-to-control pipeline for controlling a flying drone. Specifically, we trained a spiking neural network that accepts raw event-based camera data and outputs low-level control actions for performing autonomous vision-based flight. The vision part of the network, consisting of five layers and 28,800 neurons, maps incoming raw events to ego-motion estimates and was trained with self-supervised learning on real event data. The control part consists of a single decoding layer and was learned with an evolutionary algorithm in a drone simulator. Robotic experiments show a successful sim-to-real transfer of the fully learned neuromorphic pipeline. The drone could accurately control its ego-motion, allowing for hovering, landing, and maneuvering sideways—even while yawing at the same time. The neuromorphic pipeline runs on board on Intel’s Loihi neuromorphic processor with an execution frequency of 200 hertz, consuming 0.94 watt of idle power and a mere additional 7 to 12 milliwatts when running the network. These results illustrate the potential of neuromorphic sensing and processing for enabling insect-sized intelligent robots.

They have some other cool papers:

[Lightweight Event-based Optical Flow Estimation via Iterative Deblurring](https://arxiv.org/abs/2211.13726) and [Video](https://www.youtube.com/watch?v=1qA1hONS4Sw)","This is one of my favorite areas of research to follow. I wrote a post describing the [future of cameras](https://reddit.com/r/Futurology/comments/1as4bl9/the_future_of_cameras/) and gave a brief overview of this direction of research. If hardware manufacturers like Canon, Samsung, Sony, etc see the potential of this we could see a huge growth in this area over the next 20 years.

For reference, we're just seeing basic event cameras [being integrated into phones](https://venturebeat.com/ai/prophesee-teams-with-qualcomm-on-faster-event-based-smartphone-cameras/).

The big picture is with mixed reality where such ASIC processing allows for energy efficient SLAM, eye tracking, face tracking, hand tracking, pose tracking, and structured scanning. One of the goals is for MR devices that last for a full day and this kind of research could be invaluable.

I've been trying to push vision researchers in this direction for years to drive up demand for such event cameras. If these kind of topics sound interesting I'd recommend reading more into it. (UZH has a lot of event camera research for reference).",misc
[R] Fully neuromorphic vision and control for autonomous drone flight,"Arxiv: https://arxiv.org/abs/2303.08778 (15 Mar 2023)  
https://www.science.org/doi/10.1126/scirobotics.adi0591 (15 May 2024)

Also they uploaded a number of videos a few hours ago:

[Supplementary Video 1](https://www.youtube.com/watch?v=NQUv7l56r1o)  
[Supplementary Video 2](https://www.youtube.com/watch?v=0xQU7WMR1Ys)  
[Supplementary Video 3](https://www.youtube.com/watch?v=UfKr1N8mu4c)  
[Supplementary Video 4](https://www.youtube.com/watch?v=hp8Rudld3sI)

Abstract:

> Biological sensing and processing is asynchronous and sparse, leading to low-latency and energy-efficient perception and action. In robotics, neuromorphic hardware for event-based vision and spiking neural networks promises to exhibit similar characteristics. However, robotic implementations have been limited to basic tasks with low-dimensional sensory inputs and motor actions because of the restricted network size in current embedded neuromorphic processors and the difficulties of training spiking neural networks. Here, we present a fully neuromorphic vision-to-control pipeline for controlling a flying drone. Specifically, we trained a spiking neural network that accepts raw event-based camera data and outputs low-level control actions for performing autonomous vision-based flight. The vision part of the network, consisting of five layers and 28,800 neurons, maps incoming raw events to ego-motion estimates and was trained with self-supervised learning on real event data. The control part consists of a single decoding layer and was learned with an evolutionary algorithm in a drone simulator. Robotic experiments show a successful sim-to-real transfer of the fully learned neuromorphic pipeline. The drone could accurately control its ego-motion, allowing for hovering, landing, and maneuvering sideways—even while yawing at the same time. The neuromorphic pipeline runs on board on Intel’s Loihi neuromorphic processor with an execution frequency of 200 hertz, consuming 0.94 watt of idle power and a mere additional 7 to 12 milliwatts when running the network. These results illustrate the potential of neuromorphic sensing and processing for enabling insect-sized intelligent robots.

They have some other cool papers:

[Lightweight Event-based Optical Flow Estimation via Iterative Deblurring](https://arxiv.org/abs/2211.13726) and [Video](https://www.youtube.com/watch?v=1qA1hONS4Sw)",Unbelievably cool. This is the future of vision,misc
[R] Fully neuromorphic vision and control for autonomous drone flight,"Arxiv: https://arxiv.org/abs/2303.08778 (15 Mar 2023)  
https://www.science.org/doi/10.1126/scirobotics.adi0591 (15 May 2024)

Also they uploaded a number of videos a few hours ago:

[Supplementary Video 1](https://www.youtube.com/watch?v=NQUv7l56r1o)  
[Supplementary Video 2](https://www.youtube.com/watch?v=0xQU7WMR1Ys)  
[Supplementary Video 3](https://www.youtube.com/watch?v=UfKr1N8mu4c)  
[Supplementary Video 4](https://www.youtube.com/watch?v=hp8Rudld3sI)

Abstract:

> Biological sensing and processing is asynchronous and sparse, leading to low-latency and energy-efficient perception and action. In robotics, neuromorphic hardware for event-based vision and spiking neural networks promises to exhibit similar characteristics. However, robotic implementations have been limited to basic tasks with low-dimensional sensory inputs and motor actions because of the restricted network size in current embedded neuromorphic processors and the difficulties of training spiking neural networks. Here, we present a fully neuromorphic vision-to-control pipeline for controlling a flying drone. Specifically, we trained a spiking neural network that accepts raw event-based camera data and outputs low-level control actions for performing autonomous vision-based flight. The vision part of the network, consisting of five layers and 28,800 neurons, maps incoming raw events to ego-motion estimates and was trained with self-supervised learning on real event data. The control part consists of a single decoding layer and was learned with an evolutionary algorithm in a drone simulator. Robotic experiments show a successful sim-to-real transfer of the fully learned neuromorphic pipeline. The drone could accurately control its ego-motion, allowing for hovering, landing, and maneuvering sideways—even while yawing at the same time. The neuromorphic pipeline runs on board on Intel’s Loihi neuromorphic processor with an execution frequency of 200 hertz, consuming 0.94 watt of idle power and a mere additional 7 to 12 milliwatts when running the network. These results illustrate the potential of neuromorphic sensing and processing for enabling insect-sized intelligent robots.

They have some other cool papers:

[Lightweight Event-based Optical Flow Estimation via Iterative Deblurring](https://arxiv.org/abs/2211.13726) and [Video](https://www.youtube.com/watch?v=1qA1hONS4Sw)","Hi, check out Opteran - I think you might like them :)",misc
[D] What’s the best cloud compute service for hobby projects?,"Hi everyone!

I’m a research engineer working mainly on Computer Vision applications. I want to start experimenting with models or tasks I’m not an expert in as a side project, but I don’t have a GPU on my personal laptop, and I’d like to perform some small-to-medium training experiments at least. Just to give you an idea of the models I want to train:

- NeRFs and Gaussian Splats
- Diffusion models
- Some small transformer models (Think Llama-3 8b and less).

Considering the scale of the projects I have in mind, anything above an A100 is probably an overkill.

Until a few weeks ago I was using colab pro, but I didn’t really like the fact that I had to store stuff on my google drive and I’d like to have something where I can at least access the terminal and not being limited just to jupyter notebooks.

In your opinion, what’s a good cloud provider at a good cost for these sort of projects?","Lambda labs or run pod.

Not sure why vast ai gets mentioned so much, it seems way worse than those two options to me",general_qa
[D] What’s the best cloud compute service for hobby projects?,"Hi everyone!

I’m a research engineer working mainly on Computer Vision applications. I want to start experimenting with models or tasks I’m not an expert in as a side project, but I don’t have a GPU on my personal laptop, and I’d like to perform some small-to-medium training experiments at least. Just to give you an idea of the models I want to train:

- NeRFs and Gaussian Splats
- Diffusion models
- Some small transformer models (Think Llama-3 8b and less).

Considering the scale of the projects I have in mind, anything above an A100 is probably an overkill.

Until a few weeks ago I was using colab pro, but I didn’t really like the fact that I had to store stuff on my google drive and I’d like to have something where I can at least access the terminal and not being limited just to jupyter notebooks.

In your opinion, what’s a good cloud provider at a good cost for these sort of projects?",Tensordock is a good choice to get cheap GPUs but sometimes availability is an issue,general_qa
[D] What’s the best cloud compute service for hobby projects?,"Hi everyone!

I’m a research engineer working mainly on Computer Vision applications. I want to start experimenting with models or tasks I’m not an expert in as a side project, but I don’t have a GPU on my personal laptop, and I’d like to perform some small-to-medium training experiments at least. Just to give you an idea of the models I want to train:

- NeRFs and Gaussian Splats
- Diffusion models
- Some small transformer models (Think Llama-3 8b and less).

Considering the scale of the projects I have in mind, anything above an A100 is probably an overkill.

Until a few weeks ago I was using colab pro, but I didn’t really like the fact that I had to store stuff on my google drive and I’d like to have something where I can at least access the terminal and not being limited just to jupyter notebooks.

In your opinion, what’s a good cloud provider at a good cost for these sort of projects?",vast ai is great as you pay for the exact time you use and it is very simple to spin up a machine compared to other services.,general_qa
[D] What’s the best cloud compute service for hobby projects?,"Hi everyone!

I’m a research engineer working mainly on Computer Vision applications. I want to start experimenting with models or tasks I’m not an expert in as a side project, but I don’t have a GPU on my personal laptop, and I’d like to perform some small-to-medium training experiments at least. Just to give you an idea of the models I want to train:

- NeRFs and Gaussian Splats
- Diffusion models
- Some small transformer models (Think Llama-3 8b and less).

Considering the scale of the projects I have in mind, anything above an A100 is probably an overkill.

Until a few weeks ago I was using colab pro, but I didn’t really like the fact that I had to store stuff on my google drive and I’d like to have something where I can at least access the terminal and not being limited just to jupyter notebooks.

In your opinion, what’s a good cloud provider at a good cost for these sort of projects?",modal.com is great. Free credits every month and easy to apply in code.,general_qa
[D] What’s the best cloud compute service for hobby projects?,"Hi everyone!

I’m a research engineer working mainly on Computer Vision applications. I want to start experimenting with models or tasks I’m not an expert in as a side project, but I don’t have a GPU on my personal laptop, and I’d like to perform some small-to-medium training experiments at least. Just to give you an idea of the models I want to train:

- NeRFs and Gaussian Splats
- Diffusion models
- Some small transformer models (Think Llama-3 8b and less).

Considering the scale of the projects I have in mind, anything above an A100 is probably an overkill.

Until a few weeks ago I was using colab pro, but I didn’t really like the fact that I had to store stuff on my google drive and I’d like to have something where I can at least access the terminal and not being limited just to jupyter notebooks.

In your opinion, what’s a good cloud provider at a good cost for these sort of projects?","For an 8B parameter model you'd need at least 16GB of VRAM, maybe less for if you use float16 dtype or quantized models. At minimum a T4 GPU is going to be your best best, and most cloud providers outside of google colab supply them.

If you're looking for terminal access and a straightforward filesystem, any service with a Jupyter Lab console is in your wheel house.

American Data Science at [https://amdatascience.com](https://amdatascience.com) let's you start up CPU notebook lab servers for free, and then you can transition to a T4 or more at original cloud pricing. Credits are charged per minute, but let me know if you want some free credits to try out your hobby projects.",general_qa
[D] What’s the best cloud compute service for hobby projects?,"Hi everyone!

I’m a research engineer working mainly on Computer Vision applications. I want to start experimenting with models or tasks I’m not an expert in as a side project, but I don’t have a GPU on my personal laptop, and I’d like to perform some small-to-medium training experiments at least. Just to give you an idea of the models I want to train:

- NeRFs and Gaussian Splats
- Diffusion models
- Some small transformer models (Think Llama-3 8b and less).

Considering the scale of the projects I have in mind, anything above an A100 is probably an overkill.

Until a few weeks ago I was using colab pro, but I didn’t really like the fact that I had to store stuff on my google drive and I’d like to have something where I can at least access the terminal and not being limited just to jupyter notebooks.

In your opinion, what’s a good cloud provider at a good cost for these sort of projects?","[https://modal.com/docs/guide/gpu](https://modal.com/docs/guide/gpu) has got you covered. Here's what it looks like to spin up an A100 from your laptop and run a simple matmul:

    import modal
    
    app = modal.App(image=modal.Image.debian_slim().pip_install(""torch""))
    
    @app.function(gpu=""A100"")
    def f_gpu_a100(n: int = 5000):
        import torch
    
        M = torch.rand((n, n)).to(torch.device(""cuda""))
        N = torch.linalg.inv(M)
        I = torch.eye(n).to(torch.device(""cuda""))
        R = torch.matmul(M, N) - I
        mean = torch.mean(torch.abs(R))
        print(f""Matrix mean is {mean}."")  # Should be basically zero.
    
    
    if __name__ == ""__main__"":
        with app.run():
            f_gpu_a100.remote()

This python script completes in around 10s!

Someone suggested Lambda Labs, but I think they end up being to low level and cumbersome because they focus on delivering VMs and not much else. I've used Lambda Labs a lot in the past couple months as part of an investigation into whether Modal could run on Lambda Labs VMs.

Run Pod was also suggested. They're a competitor, but I've heard good things. If you try both would be interested to hear a comparison.

*Disclaimer:* I work at Modal :)",general_qa
[D] What’s the best cloud compute service for hobby projects?,"Hi everyone!

I’m a research engineer working mainly on Computer Vision applications. I want to start experimenting with models or tasks I’m not an expert in as a side project, but I don’t have a GPU on my personal laptop, and I’d like to perform some small-to-medium training experiments at least. Just to give you an idea of the models I want to train:

- NeRFs and Gaussian Splats
- Diffusion models
- Some small transformer models (Think Llama-3 8b and less).

Considering the scale of the projects I have in mind, anything above an A100 is probably an overkill.

Until a few weeks ago I was using colab pro, but I didn’t really like the fact that I had to store stuff on my google drive and I’d like to have something where I can at least access the terminal and not being limited just to jupyter notebooks.

In your opinion, what’s a good cloud provider at a good cost for these sort of projects?","100÷ agree with this answer. Lambda labs is generally better in terms of price and quality (I had weird instances on runpod). However runpod GPUs are almost always available.

Consider using skypilot to easily switch between both.",general_qa
[D] What’s the best cloud compute service for hobby projects?,"Hi everyone!

I’m a research engineer working mainly on Computer Vision applications. I want to start experimenting with models or tasks I’m not an expert in as a side project, but I don’t have a GPU on my personal laptop, and I’d like to perform some small-to-medium training experiments at least. Just to give you an idea of the models I want to train:

- NeRFs and Gaussian Splats
- Diffusion models
- Some small transformer models (Think Llama-3 8b and less).

Considering the scale of the projects I have in mind, anything above an A100 is probably an overkill.

Until a few weeks ago I was using colab pro, but I didn’t really like the fact that I had to store stuff on my google drive and I’d like to have something where I can at least access the terminal and not being limited just to jupyter notebooks.

In your opinion, what’s a good cloud provider at a good cost for these sort of projects?","100÷ agree with this answer. Lambda labs is generally better in terms of price and quality (I had weird instances on runpod). However runpod GPUs are almost always available.

Consider using skypilot to easily switch between both.",general_qa
[D] What’s the best cloud compute service for hobby projects?,"Hi everyone!

I’m a research engineer working mainly on Computer Vision applications. I want to start experimenting with models or tasks I’m not an expert in as a side project, but I don’t have a GPU on my personal laptop, and I’d like to perform some small-to-medium training experiments at least. Just to give you an idea of the models I want to train:

- NeRFs and Gaussian Splats
- Diffusion models
- Some small transformer models (Think Llama-3 8b and less).

Considering the scale of the projects I have in mind, anything above an A100 is probably an overkill.

Until a few weeks ago I was using colab pro, but I didn’t really like the fact that I had to store stuff on my google drive and I’d like to have something where I can at least access the terminal and not being limited just to jupyter notebooks.

In your opinion, what’s a good cloud provider at a good cost for these sort of projects?",Availability was tight maybe from january to march but there's plenty now,general_qa
[P] jaxsplat: 3D Gaussian Splatting for JAX,"I created jaxsplat which provides CUDA-accelerated 3D Gaussian Splatting for JAX.
The original INRIA code and gsplat's implementation contain dynamically shaped arrays unsuitable for usage with JAX.
Instead, I modified gsplat's CUDA implementation to expose custom XLA CUDA calls while not leaking any dynamic shapes into JAX-side code.

Take a look if you're interested in exploring 3D Gaussian Splatting with JAX:

GitHub: https://github.com/yklcs/jaxsplat

Docs: https://jaxsplat.readthedocs.io","nice.  .. good to include nice example of fitting splats to 2D image.

Whats the feasibility of running this on ROCm / AMD GPUs ?
My understanding is JAX has a code-gen backend to target various architectures ..
[ or am I mistaken .. that may have been Juilia language ? ]",misc
[P] jaxsplat: 3D Gaussian Splatting for JAX,"I created jaxsplat which provides CUDA-accelerated 3D Gaussian Splatting for JAX.
The original INRIA code and gsplat's implementation contain dynamically shaped arrays unsuitable for usage with JAX.
Instead, I modified gsplat's CUDA implementation to expose custom XLA CUDA calls while not leaking any dynamic shapes into JAX-side code.

Take a look if you're interested in exploring 3D Gaussian Splatting with JAX:

GitHub: https://github.com/yklcs/jaxsplat

Docs: https://jaxsplat.readthedocs.io",Nice work! What's the difference in rendering time compared with the official implementation?,misc
[P] jaxsplat: 3D Gaussian Splatting for JAX,"I created jaxsplat which provides CUDA-accelerated 3D Gaussian Splatting for JAX.
The original INRIA code and gsplat's implementation contain dynamically shaped arrays unsuitable for usage with JAX.
Instead, I modified gsplat's CUDA implementation to expose custom XLA CUDA calls while not leaking any dynamic shapes into JAX-side code.

Take a look if you're interested in exploring 3D Gaussian Splatting with JAX:

GitHub: https://github.com/yklcs/jaxsplat

Docs: https://jaxsplat.readthedocs.io",Jax won't help you in running this on Rocm/AMD GPUs since this is just wrapping the CUDA kernels and hooking them up with Jax.,misc
[P] jaxsplat: 3D Gaussian Splatting for JAX,"I created jaxsplat which provides CUDA-accelerated 3D Gaussian Splatting for JAX.
The original INRIA code and gsplat's implementation contain dynamically shaped arrays unsuitable for usage with JAX.
Instead, I modified gsplat's CUDA implementation to expose custom XLA CUDA calls while not leaking any dynamic shapes into JAX-side code.

Take a look if you're interested in exploring 3D Gaussian Splatting with JAX:

GitHub: https://github.com/yklcs/jaxsplat

Docs: https://jaxsplat.readthedocs.io","Good question, I threw together a quick benchmark. On my machine, I'm getting:

- jaxsplat: 28.8737ms
- gsplat: 33.3950ms
- diff-gaussian-rasterization: 18.1051ms

This is a benchmark of just the rendering inner loop without any JIT involved.
As all microbenchmarks go, this is not representative of actual performance.",misc
[P] jaxsplat: 3D Gaussian Splatting for JAX,"I created jaxsplat which provides CUDA-accelerated 3D Gaussian Splatting for JAX.
The original INRIA code and gsplat's implementation contain dynamically shaped arrays unsuitable for usage with JAX.
Instead, I modified gsplat's CUDA implementation to expose custom XLA CUDA calls while not leaking any dynamic shapes into JAX-side code.

Take a look if you're interested in exploring 3D Gaussian Splatting with JAX:

GitHub: https://github.com/yklcs/jaxsplat

Docs: https://jaxsplat.readthedocs.io","k, thx.",misc
[D] Apriori Algorithm,Do anyone still use Apriori in production use cases? There must be better algorithms available.,Fpgrowth is what you want to use instead.,misc
[D] Apriori Algorithm,Do anyone still use Apriori in production use cases? There must be better algorithms available.,"There are more efficient algorithms, but performance is such a non issue for most cases (~1M or less transactions) that you can use apriori without problems",misc
[D] Computer Vision Tooling - Multistage data processing,"At my line of work, I have to take a picture, detect/segment tens up to hundreds of points of interest and summarize it's sizes. 

I have ML model that is mostly precise but makes few stupid mistakes so it's not perfectly reliable (as expected) and occasionaly needs manual intervention that corrects it's output.

Currently, I use 

1) CVAT to upload an image, run prediction and correct/approve the results. Then I download the image and apply

2) Python script to run postprocessing

This workflow is good for few projects and few relatively-savy users but as time passes projects pile up and team grows. At the moment, there are several different tasks, each needs a bit different posprocessing and more and more people working with it.

**Do you know any software that can help me to implement this workflow without manually showeling data from CVAT to scripts?** 

I looked around if it's possible to extend CVAT but it's meant as annotation tool not a link in a production chain so I didn't found anything (appart plugging my own models into it). As an alternative I was thinking about writing my own solution. I would be able to write a backend but I cannot write the frontend part. I don't know javascript and searching through github for any decent frontend supporting tools (like brushes for segmentation) and label handling(fix mislabeled stuff etc)   led nowhere so I gave up thinking about it.","Something like parchyderm + labelstudio?

Never used it yet, but I've had similar questions and that seemed the best answer to me.

Pachyderm manages the data processing, versionning and lineage.

Labelstudio is a super versatile data annotation toolbox.

And there is some integration between them.

CVAT is great but from little experience I had with it, it does not do well with complex pipelines, annotations type and data versionning/lineage issues. However its interface is better from what I remember.",misc
[D] Computer Vision Tooling - Multistage data processing,"At my line of work, I have to take a picture, detect/segment tens up to hundreds of points of interest and summarize it's sizes. 

I have ML model that is mostly precise but makes few stupid mistakes so it's not perfectly reliable (as expected) and occasionaly needs manual intervention that corrects it's output.

Currently, I use 

1) CVAT to upload an image, run prediction and correct/approve the results. Then I download the image and apply

2) Python script to run postprocessing

This workflow is good for few projects and few relatively-savy users but as time passes projects pile up and team grows. At the moment, there are several different tasks, each needs a bit different posprocessing and more and more people working with it.

**Do you know any software that can help me to implement this workflow without manually showeling data from CVAT to scripts?** 

I looked around if it's possible to extend CVAT but it's meant as annotation tool not a link in a production chain so I didn't found anything (appart plugging my own models into it). As an alternative I was thinking about writing my own solution. I would be able to write a backend but I cannot write the frontend part. I don't know javascript and searching through github for any decent frontend supporting tools (like brushes for segmentation) and label handling(fix mislabeled stuff etc)   led nowhere so I gave up thinking about it.","Thanks, I’ll give it a look. Label studio is great but its GUI is laggy when you label with brushes so I had to find replacement.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"There will be 5,000 ""KANs for X"" papers that are posted to arXiv within the next 2-3 months. I do not think that everyone changing the input dimension and output dimension of the layers in the repo by Ziming Liu and plugging in their data warrants 5,000 new papers.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"Also interesting to note:  
- [https://github.com/ZiyaoLi/fast-kan](https://github.com/ZiyaoLi/fast-kan)  
- [https://github.com/KindXiaoming/pykan/issues/162](https://github.com/KindXiaoming/pykan/issues/162)

It seems like KANs can be, in fact, reduced to simplified RBF networks. Which is quite nice, since there is a lot of research on RBF NNs, they are faster, and simpler.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"Already tired of these KAN papers everywhere. Their performance seems consistently worse on real applications, are pretty much equivalent to traditional networks, and slower to train. Not to mention fully connected layers are far from the actually important layers in real life nets. They only serve to add nonlinearities and contain a negligible fraction of network parameters. Kans won't replace attention, convolutions, etc.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,Mmm yes this is good but where is the code,misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"It is interesting that this kind of replacement from linear to activation learning basis made significance in perception of graph based learning. But another side, the question that ""is this really worth it?"" actually counts also. So just to say that it is also important to inspect any clear reason why this approach has to be meaningful.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"I retired from ML a decade ago and thought this might be a good paper to catch up with what's new in the field. If I understand correctly it seems to be making the claim that piece-wise fitted curves (""spline"") make better fits than linear fits.

I don't think that's true. If you make the linear fits small enough they can fit anything the splines can do. And probably faster too. Lines are simpler and so will not bog down the computer calculating with curves. 

One of my favorite tools was Ross Quinlan's Cubist (aka M5). It (along with its discrete cousin C5.0) solved a lot of problems for me, quickly. I loved the fact that it spit out rules which essentially explained how each model worked.

Or am I missing what Kolmogorov-Arnold is actually doing here?",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,I read lot about Kan. Not so much about xLSTM although Hofreiter claims that it beats GPTs big times. Why is that?,misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"My two cents is that there might be; there should be. These people are helping the community by experimentation and then a writeup on the possibilities and the limitations of this new method, so that everyone else does \_not\_ have to. We have to hold a high bar for research but we should not discourage the explorations. If we find that something being presented is trivial, we should guide the author by pointing out what's missing and what would make their work remarkable. We should help people by giving constructive feedback rather than being dismissive of their work. We want to welcome all researchers, for we are all here to learn from each other but \_respectfully\_.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,I beg to differ. We need as much of those as we can and then to compile the results so we don't repeat experiments.,misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"The real innovation isn't going to happen until researchers determine how KAN interacts with contemporary naming practices for models. KANba? KANsformer? We don't know yet how those names for models will perform.

(EDIT: I am legitimately surprised they didn't call this [KANvolution](https://www.reddit.com/r/programming/comments/1crv9td/the_first_convolutionalkans/)! C'mon guys!)",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,I don’t begrudge anyone trying to explore some interesting thread.,misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,I think on arxiv is fine tbh,misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"Perhaps. But we have many more infinitely trivial LLM papers claiming SOTA on this or that benchmark, so I welcome a different approach on datasets that aren’t natural language.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"Every experiment must be recorded into the archive. Now wouldn't it be great if we had some sort of a tool to sort and index and organize all of that data and reason about it to draw conclusions and meta-analyzes in a way that's fast and efficient, almost as to be semi-automated. Some sort of... AI. Hmmm",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"Let people experiment, ffs. The original diffusion paper was published in 2015! It takes time to make things work. 

If you look at biological neurons, synapses are actually highly nonlinear. It's at least worth exploring through that lens.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"Reproduced on a bunch of PDE benchmarks, their performance is terrible to say the least.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,But surely KAN gives some insights in accommodating activation function. Why not use linear layer without activation function if the differences has only to be approximated? What can be the difference? I'm looking for the intuition for the answer myself.,misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,I can’t tell if you’re serious or not. There have been less than a single handful of papers.,misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"I guess they used the original pykan 

https://github.com/mintisan/awesome-kan?tab=readme-ov-file#papers",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"You’re not wrong. But it’s really motivated afaict by doing interpretable symbolic regression (going back to John Koza and the genetic algorithm jazz of yore, even) rather than fast fits. The paper to read is the original, which is totally transparent about when KANs are interesting, and when not https://arxiv.org/abs/2404.19756",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"I think thorough exploration of new ideas is a positive thing, but I also think there's a difference between thoroughness and volume.

Machine learning research publications already have a concerningly high frequency of careless comparisons using questionable benchmarks. A hasty rush to churn through iterations on a trendy new idea isn't going to improve that situation.

Like, consider the claim made in this paper's abstract:
> We demonstrate that KANs outperforms conventional Multi-Layer Perceptrons (MLPs) in a real-world satellite traffic forecasting task, providing more accurate results with considerably fewer number of learnable parameters.

That sounds like it would be quite interesting if it were true. And maybe it is true! But given that this is a publication that was put together pretty quickly, and that its topic is a supposedly brand new idea whose hype has thus far exceeded its proven merits, should I really be giving a lot of credence to its conclusions?

The whole point of reading research papers is that I want to benefit from other people having done difficult work to come to sound conclusions, and this is undermined by a crush of slapdash reports that I can't trust the veracity of without essentially redoing most of the work myself.

Edit: and to be clear, it's entirely possible that this is a solid publication with sound conclusions, but I don't know that and it doesn't seem like a great use of time to try to separate the good bits of an incipient KAM-o-mania from the unreliable bits.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"Wow, a not cynical take here on beginners and explorers. That's new",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"I'm kind of on the fence. The same thing basically happened when Transformers came out. Writing up using a Transformer to do some NLP task on some benchmark and showing it outperformed the previous SOTA was a enough to get a paper somewhere. The problem was that a bunch of sloppy papers came out at the time, as everyone knew doing so would get a paper publications, so there was a rush to get papers out.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"Completely agreed. But I don't think these results should be accepted as papers in top-tier conferences. It dilutes research and leads to thousands of publications with little or no citations. Also significantly raises the barrier for entry into research (PhDs, research scientists) since a lot of people could potentially write papers like these but don't have the required compute to run experiments.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"Findings derivative of KAN would be better as technical reports than a standalone paper. Unless there's actual novelty, I wouldn't let a paper that just swaps the MLPs in existing architectures for KAN to pass peer review, regardless of better performance in existing benchmarks.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,Which PDE benchmarks? How many parameters?,misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,">You're not wrong.

Perhaps not, but after reading Liu's paper and listening to [his video](https://www.youtube.com/watch?v=AUDHb-tnlB0) I was stunned how little I knew about this topic. It is amazing what happens when you replace MLP linear weights with spline functions along the edges of the network,  allowing them to be  learnable activation functions. 

I've got a lot to learn. (I just turned 80, so this is how I try to keep my brain nimble)",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"*You* don't need to churn through all the publications personally validating them. ""It's too much for me to go through"" is not a valid reason to be dismissive. 

Ultimately, the whole field will benefit from whatever insights are, or indeed are not, gleamed from any ""gold rush"" of this kind. If you care about being first to benefit from those, then sure, you will have to put in the work yourself. But if you can't be bothered, you can just kick back and wait for other people to work out which insights are useful and which not so much, and just copy whatever ends up becoming consensus.

Realistically, the volume of ML papers is already such that literally no human being would ever be able to keep up with merely reading them, nevermind trying to reproduce conclusions or work out for yourself how valid the results are or if there are any large issues with the paper or whatever. I think everybody understands the negatives of that, but also that it is objectively a lot better than artificially forbidding 99.9% of would-be research from being published just to keep the output down to manageable levels. 

You can always hierarchically work through the output, with experts of extremely specific specialty fields pointing out the most important papers there, filtering them up to experts in slightly less narrow fields who again filter them up etc, and anybody is free to dive as deeply or shallowly into any direction as they want to. And sure, in practice, it doesn't work out that cleanly. A lot of great research remains buried in obscurity because it was published by unknowns and lacked anything trendy to get eyeballs on it. And a lot of relatively low-effort research gets way more attention than it ""deserves"", too. But I don't think it's productive, or indeed likely to result in anything positive at all, to ""attack"" those producing what one deems ""low quality research"". Instead, surely it's better to focus that energy on improving the systems to sort out all the papers, and getting buried gems the visibility they deserve.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"Passing peer review and going to a conference, and putting something neat on preprint, are not the same thing",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"Fails on Navier Stokes completely , bad accuracy on Poisson.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"Didn't reply to the parameters question, I think the most I tried was 3000 parameters for Poisson on an A100, in addition to being slow, it had something like 49% relative L2 error. It's a very bad sign that it fails on Poisson. The poisson equation they put in the paper converges to machine precision the minute they stopped learning for activation functions and and they report machine precision in the paper. That's cheating in my book.",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"It is indeed wonderful stuff! Good to keep the old brain turning too! 

By the way, slightly off-topic here but if you’re interested in a parallel track (this is trad. MLP’s) for symbolic regression applied to scientific problems, I thoroughly recommend checking out Miles Cranmer’s group

https://youtu.be/fk2r8y5TfNY",misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,No one is forbidding anyone from doing anything. Professional researchers who feel concerned that their work might be criticized or ignored due to people suspecting that it consists of little more than insubstantial trend chasing know what they need to do in order to avoid that outcome. It's part of the job.,misc
[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis,https://arxiv.org/pdf/2405.08790,"I like Cranmer's idea of creating *foundation models* for science using neural nets (MLPs) replacing *language*. These models would be massive, like LLM's, trained on all of science, generating pre-trained models which effectively are the *priors* for further research and discovery. 

I occurred to me that these foundation models could be created using rule-generating multivariate tools (e.g. Cubist and C5.0) where the output rules (equivalent to decision trees) could be viewed as priors, in the same sense that Cranmer proposes, equivalent in the sense that they are point-wise regressions over high-dimensional spaces. (I view classification as a kind of regression. So this would be a mix of numeric and non-numeric data, comprising a language for representation of knowledge)

The point of these models is to *learn* new knowledge. Learning to me includes an exercise in *analyzing*, effectively, subsets of the data you have examined and recorded in your research, to determine selectively if you should pay more attention to it (tag it) or ignore it.  The attention mechanism in LLM's is critical for their success and could be also applied to this multivariate analysis. 

It should be obvious that optimal learning occurs somewhere between rejecting every datum you examine and accepting everything. So restraints must be applied optimally to maximize the effectiveness of learning. I call this *learning by constraint relaxation*. I believe *attention focus* achieves this goal (even at the biological level :)

Thanks for the Cranmer link.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!",Can you point to your github repo if possible?,misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","> It seems that model is overfitting when it comes to reconstruction loss

Why do you say that?

---

I'm really curious about that sharp inflection point that appears in all three graphs around epoch 8.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","Are you doing any regularization on the decoder side? Someone correct me if I'm wrong but I think the issue is the decoder. Since your KL loss is not over fitting, I think your encoder is 'correctly' projecting the data into your latent space. 

I would try adding dropout in the decoder or maybe even reducing its complexity. You could also try getting more data or augmenting what you have.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","If you want perfect reconstructions… well use a regular AE. VAEs generally tend to minimize the KL divergence loss pretty well and early (vanishing KL term) compared to the reconstruction loss term. You will never get a perfect reconstruction as a result of the reparameterization trick, hence the blurriness in VAE images for example. You could look at beta-VAE and play with different betas but even there it’s a trade off between minimizing reconstruction loss vs approximating your posterior and prior.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","Another thing to try: every few epochs, freeze the decoder parameters, and train the encoder to convergence: [https://arxiv.org/abs/1901.05534](https://arxiv.org/abs/1901.05534)",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!",How many input features do you have ? And what is the size of the latent  vector?it is important to Normalize inputs. How many data do you have for training ? What is the batch size ?to really understand the problem you must check mean and sigma values,misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","try different ways of disentanglement

Like, Factor VAE, Beta TC VAE, etc..

I literally have nausea even thinking of reading and understanding those loss functions in all those VAE methods...",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!",I don't have a repo setup for this yet as I'm just testing stuff out on a SLURM cluster. Is there anything in particular you need more info on?,misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","Would the gap between the validation and training loss not necessarily mean overfitting in this case?   
I believe the inflection point could be due to the annealing, although I could be wrong. I would need to investigate that further.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","No I don't do any regularization on the decoder, I found that when I added dropout (p=0.1) the model performed a little worse, although that was for both the encoder and decoder. I will definitely try with just the decoder, thank you for the suggestion.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","I find them cool, hehe.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","You should generally be more concerned with rates of change (direction of slope) than actual loss values when comparing training curves. Those two curves are changing together and in the same direction, which is what you want. Overfitting would be if the training loss was continuing to decrease while validation loss was *increasing*, indicating that your training procedure is improving relative to something specific to the training dataset at the cost of generalization performance. This isn't what we're seeing here: validation loss goes down and stays down. It would be nice if it went down further, but it's not going back up again so we're happy.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","Late to the party but you should absolutely not add dropout to a VAE ! 
This messes with the probabilistic nature of the model and makes things worse indeed. 
There are papers who propose what they call « variational dropout » though but it’s a bit more tedious to implement. 
In my case, i could improve on the overfitting by using data augmentation and some regularization on the decoder. 
Maybe increasing the value of your Beta parameter could help a little too !
Hope this helped !",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","I know you're not alone in saying this, but this just doesn't make sense to me. If the validation loss is worse than training loss, the model is overfitting, end of story. A little overfitting may not be a problem, and it might be difficult to get a better validation loss by regularizing more, but it's overfitting nonetheless.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","It's entirely possible for the validation loss to be higher than the training loss, but also for the confidence interval of the validation loss to overlap with the confidence interval of the training loss. The validation set is a discrete sample of data: if you repeated the same fitting procedure with a different sample as your validation set, maybe it'd be higher and maybe it'd be lower. This sampling error is part of why we don't care about the gap, only the direction of change.

Another reason we don't care about that gap is we don't actually know if these losses are being calculated on the same scale. Depending on what the training objective is, it's entirely possible that the loss scales inversely with the number of observations used to compute it, in which case we would *expect* the validation loss to be higher than the training loss under learning dynamics demonstrating good generalizability.

Outliers also have higher leverage when data is small. If your validation set contains outliers, they will disproportionately impact the point estimate of the validation loss. Importantly though, they will not impact the general trend of the training dynamics.

TLDR: confidently asserting that ""if the validation loss is worse than training loss, the model is overfitting"" doesn't make it true. It's definitely not, and it's trivial to construct cases that demonstrate this.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!",">If the validation loss is worse than training loss, the model is overfitting, end of story. 

This is a common misunderstanding. Overfitting is diagnosed by observing a worsening of validation performance **only**. Training performance is well known to be optimistically biased, and is completely useless for determining underfit / optimal fit / overfit conditions.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","Ah ok, all of that is fine. I meant actual validation performance, not estimates. Still doesn't make sense to rely on a lack of trend though - the model could actually have worse validation performance without that performance getting worse.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","> I meant actual validation performance, not estimates

Every statistic is an estimate. It's not clear to me what you mean by ""actual validation performance, not estimates"", especially in the context of this discussion and in response to my comment.

> Still doesn't make sense to rely on a lack of trend though

I've already made what I consider to be a pretty strong case for why trends are literally all we care about here. If you think I'm wrong, how about trying to make a case for your position.",misc
Tips for improving my VAE [Project],"Hi everyone,

I'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?

Also my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.

https://preview.redd.it/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557

For further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.

I appreciate any suggestions!","I simply mean the expected value over the data distribution, ie the thing we actually care about, and refer to when talking about overfitting.  If a model fits noise at some point during training, it doesn't matter whether it fit less noise earlier, or more later - it's overfitting. Surely we can agree on that?",misc
Nanogpt alternative [D],"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model

Also, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? 

Thanks",I would glance over the tinystories paper and find their dataset and models on Huggingface and use the transformers library to copy their architecture. They trained a series of very small language models.,misc
Nanogpt alternative [D],"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model

Also, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? 

Thanks","What's wrong with nanogpt? 

>Doesn't have to be an instruction model

That's something you'd decide by your choice of training data.",misc
Nanogpt alternative [D],"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model

Also, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? 

Thanks",Great thanks,misc
Nanogpt alternative [D],"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model

Also, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? 

Thanks","Nothings wrong
I just want to try out a different llm",misc
Nanogpt alternative [D],"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model

Also, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? 

Thanks","[Nanogpt](https://github.com/karpathy/nanoGPT) is not an LLM. It is a program for training LLMs. 

You would create a different LLM by training on different data.",misc
Nanogpt alternative [D],"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model

Also, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? 

Thanks",Ah ok.so do you know of a program that trains llms?,misc
Nanogpt alternative [D],"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model

Also, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? 

Thanks",There's still nothing wrong with nanogpt.,misc
Nanogpt alternative [D],"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model

Also, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? 

Thanks","God did you even read the nanogpt Readme?

Ever heard of a dataset? Ever tried to look at more than buzzfeed.

You could be training an llm in 10 mins with nanogpt.

This is even worse than the usual aibro who shows up in random discords asking about chatgpt",misc
Nanogpt alternative [D],"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model

Also, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? 

Thanks",Alright. Do different llms have different input data format requirements?,misc
Nanogpt alternative [D],"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model

Also, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? 

Thanks","That's determined by the dataset. NanoGPT can train on whatever dataset you want, long as it can be tokenised.",misc
[D] Confusion: What does the y-axis of calibration curves represent?,"I was going through the paper [On Calibration of Modern Neural Networks](https://arxiv.org/pdf/1706.04599), and saw that the authors used the following definition for the ""fraction of positives"" which shows up on the y-axis of the calibration curve. 

https://preview.redd.it/pq9eqj16bq0d1.png?width=944&format=png&auto=webp&s=be71a70ff0e6ba77b672ca9b4315c6e7ba3d1011

From my understanding, the above equation is calculating the average accuracy in the bin m.

However, my original understanding about the ""fraction of positives"" was that it was the proportion of actual positive outcomes within the bin m, which intuitively makes more sense in the context of calibration curves. I have also seen this interpretation of calibration curves.

Can you fill in the hole in my knowledge?","What's the difference between ""fraction of positives"" vs ""accuracy"" in your understanding? Accuracy means ""fraction of _correct answers_"" in this case, is that what you mean or does ""positive"" means ""y-hat == 1"" to you?  
  
I'd guess the paper's definition makes most sense given the multi class models that are analysing - the result is not just 0/1 outcomes, there are potentially hundreds of classes being assigned as the outcome.",general_qa
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Taking my chances with meta-review of 3.,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"I got accepted to Findings with a meta-review of 4. Seeing that some of you got rejected with the same score, I am now actually quite happy. It was very competitive this year because many people who had a meta-review of 4 in the previous two cycles waited until ACL 2024 to commit. This caused an inflation of 4's, so seemingly not everyone could get accepted.",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Any idea around what time they will come?,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,When will the decisions come!?,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"Waiting for decisions with meta-review 3. I think, out of a total 3k papers with meta 3 & 4, only around 1.8k will be accepted in both main and findings including long and short.",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,when do you think the results will be released?,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"Some of the results came out!  
I can see the decision in the acl venue.  
Expected Main but Findings :(",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,I got meta review 4 but rejected.... It is because the paper was short paper. I am disappointed because i think it can go findings,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Got my decision by email. Main conference accept :),misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Main conference accept it is :),misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"Meta review of 4 got rejected. Disappointed to not even get findings. Reviews were 4, 3.5 and 2.5, with the 2.5 being quite ridiculous (what else is new). Resubmitting to EMNLP it is.",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Has anyone gotten a response?,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Will the results be out today? 16th.,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,[deleted],misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"Any chance with meta-review of 2? Two reviewers didn't respond, and meta-reviewer stated something that is outright wrong which was already clarified by us.

One major reason pointed out by one metareviewer was not using LLMs despite us beating sota with foundational architectural improvements!",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"With meta-review 3, the paper is rejected.",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"The results are in...do you guys see any ""final review""?",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,[deleted],misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"Findings it is, meta review of 4.
Not sure how this stacks against the main conference.",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Does anyone know how to edit the submission for the revision phase?,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Rejected. Is there anyone with an accept on 3?,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Same. Had a meta-review of 4 with Main being the suggested venue. A lot of competition this year.,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"If you submitted through ARR I believe you can resubmit it to EMNLP without any changes, keeping the previews reviews and meta-review. I would a assume a 4 is worth keeping unless the paper is exceptional (and even then it can be kind of random).",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,That sucks! Can you see any final reviews about your paper?,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"Congrats, same here. I had a question, do they reveal about the best and outstanding papers now or later?",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,congrats!,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,still waiting...,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,it is due 15th AOE,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Did it get in with 2?,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"Same here, what was the recommendation from meta reviewer?",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,No i got a reject but no reviews,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Findings is considered the same as ACL main and there no difference in terms of publication. You may not have the ability to present at the main but it’s an ACL publication. Congrats!,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Did you receive a separate notification for findings ?,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,No I didn't get the why this was reject. only I can see is just reject sign. :(. I hope i can develop this on the next cycle(EMNLP),misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Congrats and I'm assuming you had best paper nom so double congrats!! I think the process usually is that announce all the best paper nominations with the accepted papers and then at the conference they recognize the best paper.,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"ahh I see, so the results would hopefully be out in 5-6 hours",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Conferences can be very random...I'm sure you have done solid work if you got a meta review of 4. Try EMNLP or TACL,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"I see, yes the meta reviewer gave the nomination, although I am not the first author. Thanks for the info!",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,last year main conf acceptances were out around 11pm AOE but there were people getting notifications even past midnight AOE,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Thank you for saying so :).,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"Conference organizers usually let authors know couple weeks (although sometimes is just days!) in advance, so you can make sure to attend session where awards are given.",misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,Oh is it? thanks for sharing.,misc
[D] ACL 2024 Decisions,Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? 🇹🇭🐘,"Of course, all the best mate!",misc
[D] Neurips checklist Questions,"I had a doubt about the Neurips 2024 checklist questions. 

The instructions relating to checklist says: ""All submissions must be in PDF format, and in a single PDF file include, in this order, (1) the submitted paper; (2) optional technical appendices that support the paper with additional proofs, derivations, or results; (3) the NeurIPS paper checklist. ""

It is also mentioned that ""The checklist is included in the LateX style file or the NeurIPS 2024 template on Overleaf.""

However, when I'm trying to open the link ""NeurIPS 2024 template on Overleaf"", it is showing as ""Project Not Found"" in the overleaf website. 

So, how are folks accessing and adding the checklist that needs to added at the end of the paper? 

Also, while submitting the abstract in openreview there was no checklist questions. I could submit the abstract and still can view and edit the submission. Hopefully the abstract submission has gone through correctly.",Download the zip for the style file directly from neurips website. The .tex inside has the checklist.,misc
[D] Neurips checklist Questions,"I had a doubt about the Neurips 2024 checklist questions. 

The instructions relating to checklist says: ""All submissions must be in PDF format, and in a single PDF file include, in this order, (1) the submitted paper; (2) optional technical appendices that support the paper with additional proofs, derivations, or results; (3) the NeurIPS paper checklist. ""

It is also mentioned that ""The checklist is included in the LateX style file or the NeurIPS 2024 template on Overleaf.""

However, when I'm trying to open the link ""NeurIPS 2024 template on Overleaf"", it is showing as ""Project Not Found"" in the overleaf website. 

So, how are folks accessing and adding the checklist that needs to added at the end of the paper? 

Also, while submitting the abstract in openreview there was no checklist questions. I could submit the abstract and still can view and edit the submission. Hopefully the abstract submission has gone through correctly.",Thanks.,misc
[D] Neurips checklist Questions,"I had a doubt about the Neurips 2024 checklist questions. 

The instructions relating to checklist says: ""All submissions must be in PDF format, and in a single PDF file include, in this order, (1) the submitted paper; (2) optional technical appendices that support the paper with additional proofs, derivations, or results; (3) the NeurIPS paper checklist. ""

It is also mentioned that ""The checklist is included in the LateX style file or the NeurIPS 2024 template on Overleaf.""

However, when I'm trying to open the link ""NeurIPS 2024 template on Overleaf"", it is showing as ""Project Not Found"" in the overleaf website. 

So, how are folks accessing and adding the checklist that needs to added at the end of the paper? 

Also, while submitting the abstract in openreview there was no checklist questions. I could submit the abstract and still can view and edit the submission. Hopefully the abstract submission has gone through correctly.","Why does the [FAQ](https://neurips.cc/Conferences/2024/PaperInformation/NeurIPS-FAQ#) say: ""Do you need to include a paper checklist? No, there was no checklist in the PDF this year, so this is not needed.""",misc
[D] Neurips checklist Questions,"I had a doubt about the Neurips 2024 checklist questions. 

The instructions relating to checklist says: ""All submissions must be in PDF format, and in a single PDF file include, in this order, (1) the submitted paper; (2) optional technical appendices that support the paper with additional proofs, derivations, or results; (3) the NeurIPS paper checklist. ""

It is also mentioned that ""The checklist is included in the LateX style file or the NeurIPS 2024 template on Overleaf.""

However, when I'm trying to open the link ""NeurIPS 2024 template on Overleaf"", it is showing as ""Project Not Found"" in the overleaf website. 

So, how are folks accessing and adding the checklist that needs to added at the end of the paper? 

Also, while submitting the abstract in openreview there was no checklist questions. I could submit the abstract and still can view and edit the submission. Hopefully the abstract submission has gone through correctly.",Probably reused last year’s FAQ and didn’t update accordingly. Last year the checklist wasn’t needed I think. I think the checklist is needed this year since the style file has it.,misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","This isn't a new idea and I'm not sure that it makes sense to write an entire paper in order to coin a neologism.

E.g. it's weird that they don't cite the one of the original papers on score matching diffusion, which gives a proof that the latent representation developed by such models is a function of *only the data*, and not the architecture or the model itself:
> Unlike most current invertible models, our encoding is uniquely
identifiable, meaning that with sufficient training data, model capacity, and optimization accuracy,
the encoding for an input is uniquely determined by the data distribution (Roeder et al., 2020)

[Score-Based Generative Modeling through Stochastic Differential Equations](https://arxiv.org/abs/2011.13456)

The paper cited in the quote above also seems relevant and is not cited in the platonic representations paper:
> In Section 2, we describe a general discriminative model family, defined by its canonical
mathematical form, which generalizes many supervised, self-supervised, and contrastive
learning frameworks. In Section 3, we prove that learned representations in this family have an asymptotic property desirable for representation learning: equality up to a linear transformation. In Section 4, we show that this family includes a number of highly performant models, state-of-the-art at publication for their problem domains, including CPC [Oord et al., 2018], BERT [Devlin et al., 2018], and GPT-2 and GPT-3 

[On Linear Identifiability of Learned Representations](https://arxiv.org/abs/2007.00810)",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.",I have a very simple heuristic for detecting quality work. It's when the top-rated insulting comments on this sub can't agree on whether a paper is *obviously wrong* or just a *trivial fact* that's been know for years.,misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","This entire research tract is nonsense. The reason for convergence of models has nothing to do with them learning the *""Platonic Nature of Reality""* or some such high-falooting poppycock.    

Convergence of multimodal models occurs because they are training on data scraped from the entire internet. The larger they become the more their datasets overlap.   Someone should have cued this authors in before they spent hours of their life preparing this paper.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.",If this is true does this dispel the notion of the need for multiple modalities for improving reasoning?,misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.",Phillip Iosla is the last author? Not surprised lol,misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.",This was accepted to ICML? Crazy.,misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.",cool,misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","It makes sense. Our brains have similar representations of reality, which allows us to interact with the world and each other. However, this representation can break down in cases of mental illness (such as schizophrenia, dementia, or severe depression). While I haven't read the paper, it seems that larger and more advanced AI models can better represent the reality they were trained on. Currently, the human brain remains the pinnacle of this kind of representation capability.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","I wouldn’t say that the identifiability of diffusion modes is that relevant here.  In diffusion models, you, the person training you model, effectively make an arbitrary choice of how to go from your data to a fixed prior (see the recent line of work on bridge matching for details).  Diffusion models identifiability just says that what you learn will be what you chose, which does not directly say anything about representations of data.

Quick edit: also the “identifiability” from that quote isn’t the same as identifiability from ICA, which is much more relevant for representation learning",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","Sometimes, papers contain some aspects that are trivial facts and also some aspects that are obviously wrong.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","(replying to my own post).   A relatively simple experiment could falsify the authors' hypothesis.  

You get multimodal data from two wildly different environments.   That is, 

+ video+audio of a busy metropolitan city,  

+ video+audio of deep rainforest.  


You train two different predictive foundation models on  each of these data sets , separately.

They WILL have different latent representations and different kernels. I guarantee it.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","Under finite data, no",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","My understanding is their argument depends on using multiple modalities, so I wouldn't say so at all",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","I don’t buy that argument anyway, it seems very handwavey and there’s no clear reason you shouldn’t be able to do reasoning on a single modality.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","Does he have a reputation for this? I've definitely tagged him in the past as someone who uses mathematical formalism in a pretty fake way.

edit: someone elsewhere in these comments somehow misread this as “uses too much math”! They blocked me so I have to also point out here that this post is, if anything, an insult and not a brag.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.",Yep: https://icml.cc/virtual/2024/poster/34734,misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.",We have those similar representations because our brains constantly associate those different sensory impressions with each other and use all of them for learning. Thus it makes sense to have a „combined latent representation“ - a world model so to say. But you can’t take only the eyes paired with a brain and then expect it to learn the same. This is fundamentally flawed.,misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","If we say that identifiability is the idea that any two models fitting a data distribution  will produce latent representations that are a function only of the distribution, and not the specific method of modeling it, then the score matching thing is an example of a specific case of this: it is notable that *any* score model will learn the same latent representation, irrespective of the model. It's definitely relevant, which is why that paper cites the other paper i mention which describes a more general case. 


Its especially relevant in that it's a provable and concrete implementation of the idea, and not just a conjecture coupled with a neologism.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","Fascinating. And in another comment you were bragging about how you've insultingly ""tagged"" one of the authors for using too much math.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","Aren't the authors claiming that given the same dataset (where size n is large) different models with different modalities will converge to similar latent representations and similar kernels?

Also in practice large datasets that scrape the internet will inevitably overlap.

Another interesting experiment would be to have:

* audio of a busy metropolitan city
* video of a busy metropolitan city
* audio + video of a bmc
* audio + video + language of bmc
* ... and other combinations

Hypothesis is that if they perform \~100 on COMPETENCE, their latent representations will be aligned.

Also, have you tried running your experiment? Would appreciate it if you could share the UMAP plot.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","I agree with your definition of identifiability, but there’s a subtle difference between “representation” and “encoding” from your quote.  In that paper, encoding refers to the point you get by moving data to the base space of the probability flow ode associated with the diffusion model.   This encoding isn’t a representation at all - it’s got the same dimensionality as your data and depends on how you, the user, chose to learn your diffusion model.  For example, if you chose to learn the reverse sde of the ornstein uhlenbeck process, you will get different encodings than if you learned the Schrödinger bridge between your data and prior.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","What is the meaning of this ""~100 on COMPETENCE"" ?",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","Why do we hypothesise that a busy metropolitan city has an underlying distribution that is the same for all data modalities? And how can we even do that when different modalities leave out all manner of information about it? 2d vision does not tell us that there’s a hot dog man around the corner, audio does. And why would a network find the same latent representations for cars and the noise they make?! Where would it get that association. Sorry but this stops making sense after 5 seconds of thinking.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","Is there any substantive difference between an encoding and a representation? I think the only real difference between what a score model produces, and what most self-supervised models are designed to do, is that the score model gives a lossless encoding whereas other models produce lossy encodings. A lossless encoding is, if anything, a better representation of the data than a lossy one, at least in the colloquial sense of the word ""representation"".


And yeah the proof in that paper only pertains to diffusion score models, so it stands to reason that is would not extend to different methods of creating flows.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","I'm referring to figure 2 of the paper. On second look, it does seem vague what they actually measure with competence and what exactly they are testing for ""competence"". But the theory goes that the vision models that DO perorm well on this metric converge on representation. Sorry if that wasn't clear. I'm also reading this paper as it's not mine.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","if you try all the combinations of modalities like mentioned, the ones that perform well on a metric that (which I believe is COMPETENCE, which, like I said above, is not that clear what exactly they're measuring. I'm presuming it's an MMLU like task-set). I don't think that statement necessarily implies that the hypothesis space of 2d vision WILL overlap with that of audio. But that is precisely the point. The larger the models get, the more training data they will be fed, increasing the probability of the overlap (look to figure 5 and 6 if you would like to understand it visually). My speculation is that more modality doesn't necessarily lead to better general performance, but if two models with different combination of modalities do perform well, they are aligned.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","The probability flow ODE **is** a normalizing flow, so the encodings represent points in the base space of a normalizing flow.   It is counter intuitive, but the encodings of a flow are almost useless for representation learning even though there is a 1-1 mapping from data to encoding because there are an infinite number of possible flows you can learn from your data to prior.  Diffusion models correspond to a specific choice but there is absolutely nothing about this choice that has to do with how useful the encodings will be however you measure usefulness.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","So, in your mind, the distinction between a representation and an encoding is that a representation has an obvious or built-in means of use or interpretation?


I don't think that's an impractical way of thinking about the matter, but I do think it's kind of arbitrary. By that way of thinking you could take any representation and any model that uses it, apply any arbitrary invertible transformation to both the representation and the input of the model using it, and thus get something that would be an ""encoding"" to someone who does not have the transformed model but, at the same time, would be a ""representation"" to someone who does have the transformed model. I think the fact that this distinction is relative means that it does not capture any fundamental truths about what is going on. 


And even so, diffusion model encodings/representations/etc are not inherently useless - the original denoising diffusion model shows that you can use them for latent space interpolation, and that they form a kind of progressive encoding as a function of time, which strongly suggests that meaningful semantic information is being captured. Other papers have shown that semantic information is indeed captured by flow models. 


The fact that you also need the drift model in order to interpret the flow's encoding/representation/etc in a useful way does not seem like an important distinction to me, in light of the above-described relativity of the thing.",misc
[R] The Platonic Representation Hypothesis,"arxiv: [https://arxiv.org/pdf/2405.07987](https://arxiv.org/pdf/2405.07987)  
Project page: [https://phillipi.github.io/prh/](https://phillipi.github.io/prh/)  
github: [https://github.com/minyoungg/platonic-rep/](https://github.com/minyoungg/platonic-rep/)

Interesting positional paper on the convergence of self-supervised, multi-modal representation learning.","I still disagree with the idea that the encodings of flows are useful without any assumptions on what kind of mapping your flow can learn.  Also if you look at how people do interpolations with flows, they don’t use linear interpolations specifically because most of the encoding space has low probability mass (see realnvp).  There’s a bunch of other ways to see that these encodings aren’t good representations but at the end of the day I just wanted to point out that the paper OP posted shouldn’t be trivially dismissed for the reasons you’ve mentioned.",misc
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",The easiest method is multiple sampling at inference with drop out enabled,general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","You should look at this recent (2024) paper where we benchmark various Bayesian neural network methods specifically for uncertainty quantification in regression tasks:  
Paper: [https://doi.org/10.1016/j.neucom.2023.127183](https://doi.org/10.1016/j.neucom.2023.127183)  
Preprint: [http://profs.polymtl.ca/jagoulet/Site/Papers/Deka\_TAGIV\_2024\_preprint.pdf](http://profs.polymtl.ca/jagoulet/Site/Papers/Deka_TAGIV_2024_preprint.pdf)",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",I liked [https://arxiv.org/abs/2402.19460](https://arxiv.org/abs/2402.19460),general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","Look into conformal prediction. There's a variety of methods under this umbrella, including density-based.",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","If memory usage is not a problem Deep Ensembles are hard to beat. 
Otherwise, you can simply regularize the NN with spectral normalization and train a density estimator on the latent space (e.g. GMM).
The likelihood of the embedding then serves as an uncertainty estimate.
See for example https://openaccess.thecvf.com/content/CVPR2023/papers/Mukhoti_Deep_Deterministic_Uncertainty_A_New_Simple_Baseline_CVPR_2023_paper.pdf",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","I would say deep ensembles most likely, especially if you factor in implementation complexity. This has driven the Bayesian neural network community a bit mad. 

I would also recommend https://arxiv.org/abs/2110.13572 as a possible fancier alternative",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",Wouldn't it depend on the type of network?,general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",There just was an ICLR oral that might be interesting for you https://arxiv.org/abs/2401.08501,general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","I think feature based methos are more popular. People try to make modifications to model during training like including stuff like spectral normalisation and stuff but at the end its just using some feature based method. Take up any feature based method and you myt get good estimates. Like Virtual logit matching, gaussian modelling etc",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",We’re experimenting with this technique in a physics code: https://arxiv.org/pdf/2207.07235,general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",!remindme 2 hour,general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",Yes but I've stumbled upon concerns about the quality of uncertainty estimates with Monte Carlo Dropout. Do you know if this concerns are of relevance in practice?,general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","Thank you, I will have a look at this. Do you plan to share a corresponding git repo with the publication?",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",I was gonna post this one too,general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","Conformal prediction has some nice properties but assumes training and test data are exchangeable, which will not be true in a situation where we have distribution shift, i.e. the new data is not from the same distribution as the training data, which it sounds like is one thing that OP would like to detect. There have been a number of papers that have tried to develop methods to overcome this limitation but to my knowledge they have only been able to do so for certain cases or given certain assumptions, for example if we know how the distribution of the data has changed or if the shift in distribution meets certain criteria. I am not sure how well these kinds of assumptions fare on real-world data, it probably depends...",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",Might be but i guess that there are methods out there that work model-agnostic / orthogonal to the neural network architecture you choose.,general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",Do you have a specific feature-based method in mind that works well?,general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","I will be messaging you in 2 hours on [**2024-05-15 13:54:46 UTC**](http://www.wolframalpha.com/input/?i=2024-05-15%2013:54:46%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/MachineLearning/comments/1csh3tv/discussion_what_are_sota_uncertainty/l456km3/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F1csh3tv%2Fdiscussion_what_are_sota_uncertainty%2Fl456km3%2F%5D%0A%0ARemindMe%21%202024-05-15%2013%3A54%3A46%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201csh3tv)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",Only one way to find out if the method works for your problem setting... (hint: Nike's eternally famous tagline),general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","I've not heard of these, post a link and I'll have a look. I guess it's reliant on the volume and quality of the data and the degree of drop out applied to some extent. In practice however I've found it works very well.",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","Here is the repo for pyTAGI library: [https://github.com/lhnguyen102/cuTAGI](https://github.com/lhnguyen102/cuTAGI)  
Repo for reproducing the paper results: [https://github.com/lhnguyen102/cuTAGI/tree/UCI\_Benchmark\_Baseline/benchmarks](https://github.com/lhnguyen102/cuTAGI/tree/UCI_Benchmark_Baseline/benchmarks)",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","Most methods for quantifying uncertainty have this problem, no? If I fit (i.e. train) a linear model with homoscedastic errors, when I see new observations, I do not start accounting for heteroscedasticity. All methods would have to make some assumptions...",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",I found Virtual Logit matching to perform better compared to others. But this is more of a feature + logit based method. But i think u can use it without using the logits and still get good results.,general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",That repo/code should be illegal lmfao.,general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",Furthermore there are methods like locally weighted conformal bands that get you pretty far when you don't have simple error structure -- without complete domain shift. See this blog tutorial (caveat emptor: in R) [https://cdsamii.github.io/cds-demos/conformal/conformal-tutorial.html](https://cdsamii.github.io/cds-demos/conformal/conformal-tutorial.html),general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","I'm not sure I agree that all methods for quantifying uncertainty have this problem, depending on what you're trying to do. The problem is not that we are making assumptions -- I agree we need to make some assumption -- the problem is that the uncertainty assigned by conformal prediction may not reliably increase for new datapoints distant from the training set.

We'd really like the behavior that we can get from a Gaussian process with a stationary kernel and appropriate hyperparameter settings, like this (to pick a somewhat random example): [https://www.researchgate.net/profile/Florent-Leclercq/publication/327613136/figure/fig1/AS:749406701776896@1555683889137/Illustration-of-Gaussian-process-regression-in-one-dimension-for-the-target-test.png](https://www.researchgate.net/profile/Florent-Leclercq/publication/327613136/figure/fig1/AS:749406701776896@1555683889137/Illustration-of-Gaussian-process-regression-in-one-dimension-for-the-target-test.png) Notice that as we move away from the data we've already seen, our uncertainty increases. Linear regression will also do this (Bayesian linear regression is of course just a GP with a linear kernel). Conformal prediction is not guaranteed to do this.

This picture is of course a little simplistic because in 1d it is easy to say ""this datapoint is distant from the training set"", but not so easy when dealing with say images where ""distant from the training set"" may be harder to quantify. Of course, if we think of the neural net as mapping from an input (say an image) to a feature vector where the last layer uses the feature vector to make a prediction, we could say ""distant in the feature space that the NN maps the input into"", but ""distant"" in this space may not necessarily correspond to ""distant"" in the input space, so this doesn't necessarily simplify the problem as much as we might like.

We could of course use some other method to try to detect when data is out of distribution or OOD and if it *is* OOD notify the user rather than trying to estimate our uncertainty using conformal prediction, which may be misleading. OP seems to want to use an uncertainty quantitation method for which uncertainty is guaranteed to be high for OOD data however.  
  
There are a variety of methods proposed in the literature, I'm not familiar enough with all of them to be able to say for sure which is ""the best"" -- might need to do some careful benchmarking. One example is the SNGP method from this paper [https://arxiv.org/abs/2006.10108](https://arxiv.org/abs/2006.10108) which in fact just replaces the last layer of the neural net with a random Fourier features approximated GP, and uses spectral normalization on layer weights to try to ensure the mapping represented by the neural net is distance preserving.",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)","Not necessarily. Conformal predictions tend to rely stronger on those assumptions since they yield stronger guarantees. Bayesian methods, for example, don't have coverage guarantees but also don't make assumptions about where your features come from.",general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",Why?,general_qa
[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?,"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.

EDIT: I am mainly working with regression problems.

Thanks in advance! :)",Certainly agree with your assessment. This is more about your statement seems to imply that only conformal methods suffer from domain / distributional shift issues. Most off-the-shelf vanilla methods have this problem and assessing the severity on your setting is an empirical exercise. You rightly note that there are some flavors of conformal prediction try to deal with this problem but discount them. Similarly you bring up using GPs; however applying GPs to quantify uncertainty in NNs is going to be non-trivial and your admonition about not being sure about how it will work still applies. I spent a lot of time on a project to quantify uncertainty in NNs and there are no panaceas.,general_qa
[D] Running large models on Mac for prototype/finetune,"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?",You could still wait for the Asahi Linux to finish and use a Linux distribution instead of MacOs,misc
[D] Running large models on Mac for prototype/finetune,"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?","For a lot of data scientists not having to use tools like ray or Kubernetes is actually a benefit as it's just another thing that can get in the way and make it harder to debug and fix problems.

Personally as a data scientist I'd rather just get a bare metal ok prem development machine for experimentation when working in a smaller team. If I'm moving fast it's easier to strip everything off the machine including any bit of IT management stuff, unnecessary packages, frameworks, orchestration stuff, etc. makes debugging stuff far easier and there's nothing using extra CPU processes so I can fully utilize the machine.",misc
[D] Running large models on Mac for prototype/finetune,"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?",Have you looked at MLX: https://github.com/ml-explore/mlx  for fine tuning on the Mac? They have some interesting use cases in that repository. Also this is the guy to follow if you are trying to work with LLMs on Apple Silicon https://x.com/awnihannun?t=HRsKaY40T71P5YiFGZl_Tg&s=09. He works for Apple Machine Learning Research and they have been putting out interesting papers lately on locally run models. Check out openELM https://machinelearning.apple.com/research/openelm.,misc
[D] Running large models on Mac for prototype/finetune,"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?","Honestly, you are probably going to be better off buying a server full of Gaming GPUs than using a Mac Book pro with 32Gb of unified memory. Between loading data and checkpointing and actual training it's really not a lot of memory between a bunch of torch distributed processes. The mps backend for Torch is just not very mainstream, and has quite a lot of rough edges that you just don't get when using Nvidia gpus.",misc
[D] Running large models on Mac for prototype/finetune,"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?","They cannot run stock container workloads, but they can run containers. However for anything cluster or hosted, I’m pretty sure it would be better and cheaper to just cluster some used 3090s or similar (and cheaper)",misc
[D] Running large models on Mac for prototype/finetune,"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?","With no Apple involvement, I wonder if Asahi Linux will get to that point.",misc
[D] Running large models on Mac for prototype/finetune,"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?","We looked at MLX and have been discussing whether MLX is Apple's direction for replacing Pytorch on Apple Silicon. However, MLX is much lower level than Pytorch, and it doesn't seem realistic that data scientists would adopt MLX over Pytorch to experiment on Mac machines. Moreover, experimenting/prototyping with MLX on Apple Silicon and then deploying them with Pytorch on Linux machines with CUDA/Nvidia will not work. This makes us think data scientists would want to work with Pytorch on Mac hardware.  The wild card is how much Apple is committed to supporting the Metal Backend for Pytorch (MPS).",misc
[D] Running large models on Mac for prototype/finetune,"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?","There is also MLX, which Apple seems to be working on for their hardware specifically for ML, and it seems to be an alternative to Pytorch. Have they stopped putting effort into the Apple silicon backend in Pytorch?",misc
[D] Running large models on Mac for prototype/finetune,"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?",Same. But that is still mostly for local work I guess. This whole clustering things op wants to do seems not thought through,misc
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","It depends on so many things. Would a course on this be useful for you? 


I work directly with open source models, as well as train and finetune them. Most open source models are a good placeholder if you need to build something around it, and then later finetune for your usecase. 


If I were you I would focus AI OPs, AI needs infrastructure and engineers know how to build that best. You need near to zero knowledge about ML. Just know how to use docker and kubernetes.",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","Finetune them for tasks such as information extraction, summarization. Finetuning on gpt 4 generated data is magic",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","Yes. I use the LLAVA model a lot and my team has extremely positive feedback.


The ground team takes photographs of assessments and sites and situations. We use them for myriad purposes so we need to describe them, get estimates, advise on risk profiles using said images. This is a manual process and needs to be done by competent people when the availability is scarce.


So I have developed a workflow and a set of prompt engineered questions that give appropriate enough answers that are then improved upon by said competent people.


This has cut down many hours of dry work and the team is talking about getting a bigger machine just for LLMs",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","We don't. After months of testing, no LLM can help us with enough tasks to the point it would make sense to maintain and make a solid pipeline (we have tried both open source and proprietary LLMs)",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","A lot of companies including mine are scared about potential copyright lawsuits in future so they are taking baby steps, mostly it's for internal tool deployment, data analysis etc.

There is a lot to gain of course, even something as simple as a semantic search on an existing database can generate good results.",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","We tried so much, no way to get it as cheap as OpenAI for large scale inference.",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","We train on internal engineering and product documents to automate Q&A


It mostly just lies to us, but a few times it answers the question semicorrectly",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",Damn nobody answered the question. Can you try in /r/LocalLLaMA? I'm curious to know the answer as well,general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","Embeddings, fine tuning for rag, synthetic data generation, evals.",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",It would be nice to have an industry survey paper with insights into the pitfalls discovered,general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",cool topic,general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",Oper ai,general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","Sure, would appreciate a course you found useful. 

My context is that I used to fine tune models and deploy them back when RoBERTa-large etc was considered a standard model size. With the rise of closed LLMs its way quicker to use prompting to ship stuff quickly. 

I wanted to know how people are leveraging open source in this world of openai APIs, and what benefits they give apart from the privacy, compliance aspects. 

Will definitely look more into AI ops!",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","How's the cost, latency, and quantitative evaluation versus just using GPT 3.5 & 4?

Generally we've found performance is greater than 3.5 for more cost, but worse performance than GPT 4 turbo however cheaper.

But we're currently experimenting on a fine-tuning dataset that might beat GPT4 turbo's performance as well as being cheaper. It's all in the specific use case and dataset ime.

What've you found?",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",Can you tell me why fine tuning yourself is useful for this task? Are the LLM APIs just not good enough quality for your use case? Where do they fall short?,general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","I tried LLAVA for a commercial use case, I didn’t find it worked well and went down a CNN route which worked.  I’d be interested in how youve utilised it? If you’re able to share.",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",Damn that sounds so cool,general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","What was the main reason it didn't hold?  
Random answers?",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",What kind of set up did you try? We're looking into open source for a variety of reasons including costs.,general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",What? It’s the opposite,general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",Also curious about this course!,general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","I’ve got similar results as well. I’m not sure what model to fine tune though, for some more complex tasks the BERT-style models don’t work well enough. I’m not sure how far down I have to go to get good fine tuning results - a <5B LLM fine tune? A LoRA fine tune of a larger model? Where to stop? How do you make decisions here about how much time/resources to put into these efforts?",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",We used completely synthetic dataset generated by gpt-4 for an NLP task trained on a smaller and faster model. It really works well; although I advise involving humans to review the datasets.,general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","Depends what you are using it for. 

Also the model does not read stuff. It will give out annotations/translation to standard publicly available images.

Anything you put like a paragraph you typed or an invoice to convert to a table, it is a major flop. Show it a picture of a tree and it will give you a lot more than you asked for and It even goes into further detail and can chat about it.

I am not aware of the CNN route. Care to share the setup and process please?",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","For the most part:

- Without finetuning or RAG, answers were always so generic that were basically useless

- With finetuning/RAG, we just did not have many use cases where accuracy below 99.99% is acceptable. If a human has to manually verify everything, it just doesn't pay out

- Most of our business relies heavily on pure math/logic and less ""fuzzy"" knowledge (I work at pharma company, where half the battle is drug discovery and supply chain)",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!",Do you have an example of open source being cheaper? Could you help me with the approximate price and performance differences between a closed LLM and your OSS approach?,general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","Usually mistral 7b or llama 3 8b give amazing results. They manage to surpass gpt 3.5 after finetuning. I do want to experiment with phi 3 tho. While finetuning, make sure you save checkpoints, look at the training loss graphs and pick the appropriate checkpoint. Usually 2 epochs works for me but not always, depends on the training data tbh. Time and resources again depend on the amount of data. Bulk of time goes in preparing the data, the actual training you can just let it happen. For my firm, we use aws g4dnxlarge. For freelancing, any 24 gb runpod gpu.",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","So I was using it for reading information from solar meters. I was hoping llava could do some OCR or at least read the serial number. 

Instead we used an RTMDET model for detect the serial number and LCD display then used another specially trained model to read LCD digits and used Doctr OCR to read the serial number. 

I’m interested that you used it to assess risk profiles and estimates, what kind of use case was that (obfuscate the industry if you like) 

We have a client that does power network installation and they’ve been chatting to us about some AI help for exactly that kind of use case",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","Let's assume 25M input, 5M output consumption



OpenAI

GPT-4-turbo: 25 \* 10 + 5 \* 30 = $400

GPT-4o: 25 \* 5 + 5 \* 15 = $200

GPT-3.5: 25 \* 0.5 + 5 \* 1.5 = $20

  
Claude

Opus: 25 \* 15 + 5 \* 75 = $750

Sonnet: 25 \* 3 + 5 \* 15 = $150

Haiku: 25 \* 0.25 + 5 \* 1.25 = $12.5

  
FireworksAI (https://fireworks.ai/pricing) & TogetherAI (https://www.together.ai/pricing) have similar pricing:

Large model (> 16B): 25 \* 0.9 + 5 \* 0.9 = $27

Small model (< 16B): 25 \* 0.2 + 5 \* 0.2 = $6

  


You can even finetune the smaller models with synthetic data generated from GPT4 and use that to get similar performance in some tasks, while being much cheaper and much faster. Obv the smaller models won't work for a lot of use cases but if you're doing extraction/summarization/classification/structured output/text-to-sql etc. then a fine-tuned OSS model goes a looong way",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","Thanks, this is very helpful!",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","I work in the sustainability industry. Waste, installations, surveys, etc.",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","Thanks, this is promising and exactly the kind of information I was looking for. Will try this out asap!",general_qa
"[D] Those in the industry, how are you using open source LLMs?","Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: 

- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of
- Do you see any performance/cost gains using fine tuned open source?
- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?
- Anything else that might be relevant to using your own LLMs

I'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","Apologies, I was talking about hosting it on my own, scalable cluster. If I sent my data to another corporation, I do not care if the model is open or not.",general_qa
[R] LLM4ED: Large Language Models for Automatic Equation Discovery,"**Paper**: [https://arxiv.org/abs/2405.07761](https://arxiv.org/abs/2405.07761)

**Abstract**:

>Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms. In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations. In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively. The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance. The second strategy is to instruct LLMs to perform evolutionary operators for global search. Experiments are extensively conducted on both partial differential equations and ordinary differential equations. Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems. Further comparisons are made with state-of-the-art models, demonstrating good stability and usability. Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.",!RemindMe 2 hour,misc
[R] LLM4ED: Large Language Models for Automatic Equation Discovery,"**Paper**: [https://arxiv.org/abs/2405.07761](https://arxiv.org/abs/2405.07761)

**Abstract**:

>Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms. In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations. In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively. The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance. The second strategy is to instruct LLMs to perform evolutionary operators for global search. Experiments are extensively conducted on both partial differential equations and ordinary differential equations. Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems. Further comparisons are made with state-of-the-art models, demonstrating good stability and usability. Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.","I will be messaging you in 2 hours on [**2024-05-15 13:53:15 UTC**](http://www.wolframalpha.com/input/?i=2024-05-15%2013:53:15%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/MachineLearning/comments/1csgx30/r_llm4ed_large_language_models_for_automatic/l456dw1/?context=3)

[**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F1csgx30%2Fr_llm4ed_large_language_models_for_automatic%2Fl456dw1%2F%5D%0A%0ARemindMe%21%202024-05-15%2013%3A53%3A15%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201csgx30)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",misc
[P] Multivariate Traffic Flow Predictions,"Hi, i just started a new project, i have this traffic datasets with 5 Minutes time intervals , and flow and speed, now i tried using prediction models like Lstm and gru successfully with flow or speed but when i try to use the model on both speed and flow it doesn’t work. Any inputs where can i learn this stuff ?","What exactly are you trying to do with the data?

What kind of model is it? Forecasting, classification, clustering?",misc
[P] Multivariate Traffic Flow Predictions,"Hi, i just started a new project, i have this traffic datasets with 5 Minutes time intervals , and flow and speed, now i tried using prediction models like Lstm and gru successfully with flow or speed but when i try to use the model on both speed and flow it doesn’t work. Any inputs where can i learn this stuff ?",It is well overlooked that physical variables has its own unique property in any network processing. It will be useful to find why you have to use LSTM and GRU network in the first place. Just time-variant variables don't guarantee the adequacy of the LSTM or other related models. Hope you find the answer well.,misc
[P] Multivariate Traffic Flow Predictions,"Hi, i just started a new project, i have this traffic datasets with 5 Minutes time intervals , and flow and speed, now i tried using prediction models like Lstm and gru successfully with flow or speed but when i try to use the model on both speed and flow it doesn’t work. Any inputs where can i learn this stuff ?","Forecasting, I’m trying to implement models like svr, lstm, gru, Xgboost, my problem is using 2 variables instead of one.",misc
[P] Multivariate Traffic Flow Predictions,"Hi, i just started a new project, i have this traffic datasets with 5 Minutes time intervals , and flow and speed, now i tried using prediction models like Lstm and gru successfully with flow or speed but when i try to use the model on both speed and flow it doesn’t work. Any inputs where can i learn this stuff ?",Traffic flow predictions with KANs https://www.reddit.com/r/MachineLearning/s/w1cxgKH7zT,misc
[P] Multivariate Traffic Flow Predictions,"Hi, i just started a new project, i have this traffic datasets with 5 Minutes time intervals , and flow and speed, now i tried using prediction models like Lstm and gru successfully with flow or speed but when i try to use the model on both speed and flow it doesn’t work. Any inputs where can i learn this stuff ?",Thank youu,misc
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","I wonder if it's something closer to the original DALL-E where the image was decomposed into image tokens with a discrete variational autoencoder, then a pretty standard decoder-only transformer was trained on sequences of some text tokens then some image tokens. The embeddings of the image tokens and text tokens could share the same latent space, so that model was ""natively"" multimodal.

I'm sure there is some additional sophistication, but I wouldn't be surprised if the overarching technique was the same. For audio, I imagine you could train something similar to the image VAE that decomposes some audio signal into a sequence of discrete values.

Edit: [here's an example of a VQ-VAE for audio](https://arxiv.org/abs/2207.09983)",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","So start by thinking of architecture which is not natively multimodal.

If we had a vision-to-text module take a picture convert it to text  and stream to GPT-4, in a certain sense it's multimodal but in a certain sense, not natively. It lacks the association layers that create the merged embedding of the two primary streams, vision and text.

  
I could be wrong, but as a former computational neuroscientist, that's where my headspace goes when I think about ""natively"" multimodal.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",You train an audio encoder that looks like WavLM or something that outputs discrete tokens. You train an audio decoder that goes from discrete tokens to wavform. You then train the entire network with mixed input of bpe + audio discrete tokens with next token prediction. The next token might be either audio discrete token or bpe as well.,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",Isn't Gemini natively multi-modal too?,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",Watch [How AI 'Understands' Images (CLIP) - Computerphile](https://www.youtube.com/watch?v=KcSXcpluDe4) and include other mediums in your thoughts.,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","My guess would be something process which type of inputs you send in, sends it to the correct embedding configuration, then routes to the appropriate modality experts. They have some mechanism to communicate like a MOE to align outputs and speed up generation time.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",I naively assume there’s some cross-attention?,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","I guess they used something like SORA’s spacetime patches and had three channels. We see multiple demonstrations of video and audio working at the same time, so in terms of tokens it seems like these tokens should be in parallel or interlaced. But of course for the three different modalities, they may need to be mapped onto the same latent space if they are interlaced (or maybe the tokens just consist of all three components [text|audio|image] if they are in parallel).",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","The concept of multi-modality reasoning within the single neural net hurts my head. It was very apparent that both OpenAI and Microsoft were approaching 'multi-modality' through a system of models within their releases... I never stopped to consider what true multi-modality would look like, or how it would process.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",They use VQVAE. It puts it into tokens then they reserve a space for their embedding to register the tokens.Which means they trained it on these tokens instead of using something like a text captioning model.,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","After talking to it a bit this morning, it still can't ""hear"" what you say... it can tell if you're shouting, whispering, your tone, I think speed of speech, background noise... but it can't tell you if you have an accent, or if you're pronouncing something unusually. The brains underneath seem to be just a standard transformer llm, only now the words you speak seem to be getting tagged with metadata supplied by parallel models (e.g. tone of voice, timestamps etc). So seems like a collection of models pre-processing audio into tokens for a transformer. The voice itself sounds just as good as last iteration so it may well still be LLM text out -> TTS, but probably the LLM output is also now giving ""tagged text"" output in order to inform the TTS the mood a statement should have (rather than the TTS independently guessing the mood from the text, which it seems to have been doing before).

I think this strategy would let them take a text only base model like they've been doing, and fine tune with metadata tagged input supplied by the audio frontend. Presumably that's wildly more efficient and easier to train than just dumping raw audio into a neural net.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","pre-training the whole model on webpages with text and images/videos, would've been my guess.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","I guess they're doing something along the lines of LanguageBind: https://arxiv.org/abs/2310.01852

Use modality specific encoders with some contrastive losses to learn multimodal relationships. Then fine tune for your task. LanguageBind pairs each modality with language, so you can contrast pairs that don't correspond.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","My guess is you just have embedding for the input modes generating tokens in the same space. The thing is, a transformer architecture only knows tokens anyhow and in principle you could just send them and have the model learn when different tokens have the same meaning. It would probably not be done naively as I'm suggesting here but with some secret sauce that relates tokens already on the embedding level, so that the token sequence for ""hello"" is easy to relate for text and audio.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",Does this mean that there's an inductive bias where each exemplar of video/audio + text only happens within that time context or is it continually training in streams of some sort?,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","I think it's not as multimodal as they make it out to be. It still doesn't produce an image of a nerd without the glasses, hinting that it's prompting a Dall-e like model to generate the image. Some pieces like speech-to-speech might be purely ""native"" though.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","Yes, I think that's exactly it: when they say they train a single GPT model end-to-end on all modalities simultaneously, I think they mean exactly that, and it makes sense if this is what ""Gobi"" has been all along. 'Just' train a encoder tokenizer for each modality, maybe define some of the extra 100k BPEs as modality-specific delimiters similar to delimiting prompts/end-of-text tokens - and then it's just 'tokenize all the things' as long interleaved sequences like iGPT/DALL-E 1, Gato, CM3, or Gemini; and train normally at scale. Then every kind of paired data just falls out naturally, all of the few-shot or zero-shot, all of the editing, and so on, and you just keep adding in whatever new modality or metadata you need to.

This also could potentially get you the low-latency they are showing off: you aren't running a diffusion model for iterations over the entire output before you can ship it off to the waiting user, you are spitting out a few tokens encoding the final modality (skipping all of the older multi-stage pipelines), which can start serially going through the upscaler/decoder's single forward pass and stream out to the user immediately.

(It also means that it's easy to provide new ways of formatting or reprocessing data cleanly. Just define it as a new 'modality'. For example, you could keep BPEs at runtime, with the context window benefits, but you could then also provide a 'character/byte-tokenized modality' which is the same text, just using only the byte-level BPEs; and then train on both forms of text occasionally, like a translation task. This would hopefully fix most or all of the BPE pathologies, from spelling to glitch or 'undertrained tokens', and would stop people on Twitter from endlessly mocking your latest model by asking it ""how many 'r' letters are there in the word 'strawberry'"" and GPT-4o embarrassingly answering '2' *still*.)

As opposed to GPT-4-V which seemed to be something like a separate VAE trained standalone and then tacked onto GPT-4 via cross-attention or something.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","From where do you think they might have acquired such enormous interleaved data of audio, text and images to learn the complex interdependence and correlation between tone, pitch of the audio and images and text
Also while training using next token prediction how did they create batches like <audio><image><image><audio><image>.. or
<audio><image><audio><image>..",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",Don’t tokens have to be small? How can it fit an entire concept like “building” into one token,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",I wonder how the 2D nature of the images is accounted for in such a tokenization?,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","Not sure about its architecture, but at the I/O keynote yesterday they said several times that they designed it to be multi-modal from the start, so perhaps it is.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","so if we want to represent more than 2 mediums in the same vector space, do we need to find training examples that contain all of the mediums together?  for example, do we need to find an image with a text label and an audio clip if we want to represent images, text, and audio in the same space?  or do we find image-text pairs and image-audio pairs and text-audio pairs and then somehow combine them all together?",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","I don't think I would consider the model natively multimodal unless there is a multimodal embedding somewhere along the way. If they embed inputs separately and then learn a projection to put those embeddings into the same space then maybe, but what you described to me means the exact opposite of being 'natively' multimodal in my mind.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",GPT-4o isn't fully released yet. You were talking to Whisper speech to text and the voice was the original text to speech,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",How does it know to output e.g. only text tokens?,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","Makes sense, if the basic concept is just ""tokenize everything, throw it together, apply GPT training recipe"", then doesn't seem particularly groundbreaking (tho I'm sure many sophisticated things layered on to make it work)

Doing token-by-token predict->decode->send for something non-discrete like audio and having it be seamless is pretty slick",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","I personally liked VAR because it doesn’t tokenize image in an interleaved manner. I think interleaved token representation is a hack because images tokenized that way doesn’t have strict one way causality.

https://github.com/FoundationVision/VAR",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","See [this tweet from Greg Brockman](https://twitter.com/gdb/status/1790869434174746805) for what might be a hint of the GPT-4o architecture.

cc u/iplaybass445.

cc u/Flowwwww.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","Would be my guess as well, just tokenize all inputs.
I wonder how the rest of the model looks. I could imagine a MoE model that learns to just route the inputs such that different modalities always get routed to different experts.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","The nice thing about the autoregressive approach is that you largely don't have to. Even if you have zero metadata or parallel data, just a giant pile of unlabeled audio you've tokenized into sequences, your LLM is still able to do a huge amount of unsupervised learning on it - just like text. Web scrapes don't come with much useful metadata, you just train the LLM on the text. So what little metadata or parallel data you have will go a long way, as it is simply 'finetuning' the translation task. It's closer to prompt engineering than supervised learning: ""a sexy voice like Scarlett Johansson's in _Lost in Translation_ or _Her_ saying 'Hi'"".

Then you can grab your metadata/parallel data anywhere you can find it. For example, use Whisper-generated transcripts of audio, and once your new model is better than Whisper at speech-to-text, switch over; then to learn text-to-speech, simply swap the order of tokens from speech-then-text to text-then-speech.

That's why the sequence approach is so beautiful: it's crazy flexible, all by simply thinking a little bit about how to reorganize your data.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","They probably put massive amounts of engineering effort into gathering those datasets. Synthetic data probably plays some role too; I’ve heard speculation that Sora used unreal engine renders as training data for example.

The tokenization model components themselves would be totally self supervised and don’t need anything but the raw audio/image, no associated text required. Once you have that, you just need paired examples of modality 1/modality 2 rather than any specific annotations on timbre or pitch. I could see adding in additional information tokens for timing & tone to the text sequence to make training easier, but I don’t think it’s a hard requirement.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","Tbh I’m not sure, but it seems like they must have had some learnings from the Sora “4-d patches” tokenizing",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","So in Dall-E 1 image tokens aren’t concepts, they are patches of “a blob of colors that look like this”, typically 16x16 pixels in size. The vae then is responsible for taking real images and reducing them to those image patches, as well ad reconstructing a realistic image from those patches",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",damn good question,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","Ah could be, tho I think I got the new model at least once. I said some Spanish and asked it how I sounded, it said I spoke clearly but watch my ""R""s when I say ""Tampico"" and ""familia"" xD. When I laughed and pointed out there are no Rs in those words it sounded disappointed and said ""Oh, I'm sorry about that. I misunderstood you"". With the gpt4 model it tends to flat out say it can't hear my speech, it can only read my words.

But yeah I'll check in periodically and do the accent test if I get a model that can sing to me.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","In this naive approach it kinda doesn't. It outputs t1 t2 t3 v1 v2 t4 t5, where the t tokens are text and the v tokens are inline graphics, just as it is trained that text sometimes contains graphics. In a real approach you would probably do something. The kinda baseline idea I can think of is to take the highest valued token of the desired type instead of just the highest valued token period.",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",The amazing thing about these LLM architectures is their relative simplicity.,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?",This is why it's all about scaling your hardware.,general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","Is that why nVidia has entered the chat, or do they use something else? If so, what?",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","It's nvidia,[ they even mentioned them during the new model launch](https://www.businessinsider.com/openai-thanks-jensen-huang-nvidia-at-gpt4o-launch-ai-king-2024-5)",general_qa
"[D] GPT-4o ""natively"" multi-modal, what does this actually mean?","What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?

E.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system ""self-select"" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","They entered the chat because other hardware makers are coming hard.  Everyone else wants to hedge against Nvidia being their only hardware… they want to hedge against other companies changing hardware.  Also, vertical integration. If companies can pay them what they charge there is a lot of money in it.",general_qa
Does every TTS tool need reference voice along with model to run ? [D],"Does every TTS tool need reference voice along with model to run ?

does every text to speech tool need reference voice along with model to work  
even if you have model for that voice u still  need the reference voice of it   or just the model is enough ?","If you do not have a reference voice, then just a random voice will be used for the prediction. Check fish speech.",misc
Does every TTS tool need reference voice along with model to run ? [D],"Does every TTS tool need reference voice along with model to run ?

does every text to speech tool need reference voice along with model to work  
even if you have model for that voice u still  need the reference voice of it   or just the model is enough ?","so it doesn't matter the reference voice ?
the final result will be passed on the model voice ?",misc
Does every TTS tool need reference voice along with model to run ? [D],"Does every TTS tool need reference voice along with model to run ?

does every text to speech tool need reference voice along with model to work  
even if you have model for that voice u still  need the reference voice of it   or just the model is enough ?","The data the model was trained on will of course affect the output. For example, if you train on audio books, the generated audio will also sound more like an audio book. But what I am saying is that if you do not use a reference voice during the prediction phase, at least the speaker will be random (sometimes woman, sometimes man etc). The other option is to provide a reference voice such as a famous actor. Then the generated audio will sound more like this. This is my experience at least with Fish speech. I do not know about other TTS.",misc
Does every TTS tool need reference voice along with model to run ? [D],"Does every TTS tool need reference voice along with model to run ?

does every text to speech tool need reference voice along with model to work  
even if you have model for that voice u still  need the reference voice of it   or just the model is enough ?","thank bro  , you explained it to me better than most youtube video i saw",misc
[D] Audio Tokenizers,"The recent GPT-4O model got me thinking whether they actually tokenized the audio and trained their GPT on text + audio tokens. Are there any successful audio tokenizers that seem to work well with auto regressive models? People have used VQ-VAE\[1\] for learning discrete representation of audio samples but the encoder and decoder of such VQ-VAE uses covnets applied over Mel-Spectrogram which I think in practice cannot enable audio streaming (As it applied 1d and 2d covnets over the entire audio signal and also doing this makes the representations non casual)

\[1\] - [https://arxiv.org/pdf/1711.00937](https://arxiv.org/pdf/1711.00937)

  
Edit:

A more general question I have is that is this method of tokenizing audio even feasible(will it even work?) or it's better to incrementally sample from the audio and proj each sample to an embedding and then pre train the GPT on those embeddings instead of the embeddings learned from tokens?",Maybe take a look at [BEATs: Audio Pre-Training with Acoustic Tokenizers](https://arxiv.org/abs/2212.09058) . But this still applies to the audio spectrogram.,misc
[D] Audio Tokenizers,"The recent GPT-4O model got me thinking whether they actually tokenized the audio and trained their GPT on text + audio tokens. Are there any successful audio tokenizers that seem to work well with auto regressive models? People have used VQ-VAE\[1\] for learning discrete representation of audio samples but the encoder and decoder of such VQ-VAE uses covnets applied over Mel-Spectrogram which I think in practice cannot enable audio streaming (As it applied 1d and 2d covnets over the entire audio signal and also doing this makes the representations non casual)

\[1\] - [https://arxiv.org/pdf/1711.00937](https://arxiv.org/pdf/1711.00937)

  
Edit:

A more general question I have is that is this method of tokenizing audio even feasible(will it even work?) or it's better to incrementally sample from the audio and proj each sample to an embedding and then pre train the GPT on those embeddings instead of the embeddings learned from tokens?","You can use Hubert model and kmeans model trained on the outputs from a layer to tokenize speech.
See VoxtLM, Spirit-LM both are multimodal and were trained on discretized speech and text tokens. 

Speech vocab in this case is the number of kmeans centroids and each frame is encoded by hubert and finally represented by the “code” of its nearest centroid.",misc
[D] Audio Tokenizers,"The recent GPT-4O model got me thinking whether they actually tokenized the audio and trained their GPT on text + audio tokens. Are there any successful audio tokenizers that seem to work well with auto regressive models? People have used VQ-VAE\[1\] for learning discrete representation of audio samples but the encoder and decoder of such VQ-VAE uses covnets applied over Mel-Spectrogram which I think in practice cannot enable audio streaming (As it applied 1d and 2d covnets over the entire audio signal and also doing this makes the representations non casual)

\[1\] - [https://arxiv.org/pdf/1711.00937](https://arxiv.org/pdf/1711.00937)

  
Edit:

A more general question I have is that is this method of tokenizing audio even feasible(will it even work?) or it's better to incrementally sample from the audio and proj each sample to an embedding and then pre train the GPT on those embeddings instead of the embeddings learned from tokens?","> which I think in practice cannot enable audio streaming (As it applied 1d and 2d covnets over the entire audio signal and also doing this makes the representations non casual)


This assumption doesn’t hold, convolutions have a limited window",misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Yo can i join,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Can you send the link here pls,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!","Hello OP, I am not a complete beginner, as I already have some knowledge of ML, and even practiced, but I want to enhance my knowledge coz it’s not sufficient, pls consider joining me :)",misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Can I still join,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Send link. Would love to join,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",I'm not a beginner and not a pro in ML. But I do not  want to forget about the Algos. Can i join 👉🏻👈🏻,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Sure would love to join,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Im interested!,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Hey i am beginner too and interested to join,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",I am interested too!,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Beginner and interested,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",interested as well,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!","Absolutely, I’d like to join too. Could you send the link?",misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",I am also interested,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Please DM me I will send the link,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Sure. Please DM,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Sure. Please DM me,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Please DM me,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Sure.Please DM me,misc
[D] We are forming a study group!,"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. 

Our aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.

So if anybody interested you can approach us.

This group is not only for beginners it's also for pro as well as they can mentor others.

DM me if u want to join!",Dm'ed,misc
[R]  Matryoshka representation learning (MRL) for CLIP (& SigLip),"MRL \[1\] for CLIP allows smaller dimension embeddings to be used without loss in fidelity. Training is modified to optimize for truncated embeddings (multiple target dimensions at once) across both vision and text encoders.  

  
Key findings:

* Reducing embeddings size by 4x retains \~95 performance
* Projection layers for sub-embeddings did not help performance
* Works in and out (zero-shot) of domain on multi-modal retrieval
* Using too many sub-embeddings degrades performance (i.e. {512, 256, 128} vs {512, 256, 128, 64, 32, 16, 8}
* The number of sub-embeddings impacts convergence (same as above)
* Works with rank-tuning methods like GCL
* Relative importance (weighting, wi) of sub-dimensions matters (e.g. w1\*L\_512 + w2\*L\_256 + w3\*L\_128)
* MRL trained models can improve if the original sized embedding is used. i.e. performance is improved even if the smaller embeddings are not used.

Article:  
[https://www.marqo.ai/blog/matryoshka-representation-learning-with-clip-for-multimodal-retrieval-and-ranking](https://www.marqo.ai/blog/matryoshka-representation-learning-with-clip-for-multimodal-retrieval-and-ranking)

\[1\] MRL [https://arxiv.org/abs/2205.13147](https://arxiv.org/abs/2205.13147)",Nice to know that it works with GCL,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","if it's classification, deberta is fine enough, using LLMs is suboptimal for classification (with a moderate number of classes)  
[https://huggingface.co/microsoft/deberta-base](https://huggingface.co/microsoft/deberta-base)  
[https://huggingface.co/sileod/deberta-v3-base-tasksource-nli](https://huggingface.co/sileod/deberta-v3-base-tasksource-nli)  
I'd say that using encoders is required and using LLM is optional in such experiments",misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",Use Deberta. Better than BERT and Roberta.,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",I am working on a tesk where bert is better than roberta and deberta. But you should probably do comparison with most recent encoder only architecture to be rigorous.,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","If you want the encoder only framework,.most NLP focuses on T5 instead of BERT.",misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",You can check NAACL 2024 and if anyone has published papers looking at BERT and It's variants. I published a paper there that explores adversarial training in language models and only evaluated on BERT and RoBERTa. In my reviews no one complained about not using LLMs. You can check out the paper if you're curious https://arxiv.org/abs/2403.18423.,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",If you know LLMs would perform better why would you go with BERT (which is technically an LLM)?,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","Hello, I'm Dref360 co-maintainer of Baal, a Bayesian Active Learning library. 

While in-context learning is getting very strong on few-shot learning benchmarks, specialized models are still better on most complex tasks. In consequence, research in active learning is incredibly important for companies. 

The well-known OATML lab is publishing heavily in the domain and Baal (built at ElementAI/ServiceNow) is quite active as well.

Feel free to visit our library on Github, we support Huggingface and Pytorch Lightning. https://github.com/baal-org/baal",misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",Thank you this is extremely helpful! Do you know any recent papers or references that have shown this about classification tasks?,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","> encoder only

T5 is technically an encoder-decoder though?

I guess something like DeBERTa-v3 should be used if you want to stay encoder only.",misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","T5 is so bad for so many use cases. I cant understand why it keeps being pushed, probably the Google factor. For Sequence 2 Sequence, okay maybe. But OP asked for classification, just use Deberta.",misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",Because BERT has less than a bajillion parameters,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",I don't think people think of BERT as a LLM these days.,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","I’d be studying reliability of classification model embeddings (as one part of the work) and studying other properties of model outputs in an active learning setting. 

Not sure if this paradigm is relevant anymore if in context learning has taken over",misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","Is bert a ""large"" model ?",misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","[https://arxiv.org/pdf/2302.10198](https://arxiv.org/pdf/2302.10198)  
and this one where deberta outperforms GPT-4 [https://arxiv.org/pdf/2402.07470](https://arxiv.org/pdf/2402.07470)",misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",People like to use the embedding weights only is what I mean.,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",What's the upside of decoder only? You don't get info about the labels on training?,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",I don't think it should have ever been considered an LLM. It's not large and not a language model.,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",I know that even non LLM based methods beat Deberta also for many of our internal datasets. You gotta see what works best,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",Better for generative models. Classifications are better done with encoders.,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",Can you explain why?,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.",Why would you use causal masking for a classification task?,misc
[D] Is BERT still relevant in 2024 for an EMNLP submission?,"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being ""out of date""?

My idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","Encoder only models pay attention to all the tokens, which are condensed into the CLS token for prediction. If we want to train a model for generative tasks (next word prediction), we need to use masked attention (causal self-attention), which is exactly what decoder only models like GPT3 do.",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)",There seems to be a bit too much hype around this paper despite not having been tested much. Any more promising results after further testing trying to scale it up?,misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)",The examples in the paper are extremely unconvincing and the supposed applications to knot theory and physics are garbage. (Ask any knot theorist if the KAN-found formulas are interesting.) Why are so many people talking about this paper? Baffling to me,misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","Solid video, OP",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)",Thank you! Very good explaination,misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","[FastKAN is a very fast implementation of Kolmogorov-Arnold Network (KAN). Uses Gaussian Radial Basis Functions to approximate the B-spline basis, which is the bottleneck of KAN.](https://github.com/ZiyaoLi/fast-kan)",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","I think you just described this whole field. The new thing gets latched onto a hundred projects, derivative papers, and ever increasing optimizations later to release an archivx paper claiming to be sota a 1/100 is actually useful. and become mainstream",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)",Everything I've seen makes it seem like a pointless development. It's just an MLP alternative that only outperforms handicapped MLPs without modern setups and activation functions. To that end it's useless in frontier models.,misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","Here’s a good list of work being done on KANs: 

[Awesome-KAN](https://github.com/mintisan/awesome-kan)",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","KANs open a very interesting game for those who care for interpretability. Yes, the Statistics community have played with GAMs, Splines and what not. But the authors of  this framework  were able to effectively communicate the message.

If you only care about classifying images of dogs vs cats or producing a 2 min video clip about a dancing pandas additionally to being immersed in a culture of ResNet + 48 GPUs + 2 weeks training, I understand many won't see the potential value KAN brings. 

However, for many other (scientific) fields, read: Aerospace Engineering, Medical Imaging, Physics, Neuroscience, where simple MLP won't make it to production because it's blackbox, this paradigm is a  game changer. Suddenly you can send a mini-AI model in outer space to measure gravitational waves (instead of using some analytical formula that is known to produce errors),  because said mini-AI model possess units that are white box. Before I ship my probe to space I can devise some tests (""adversarial attacks"" as you call it) to see how it behaves in the first place.

Again, it may not live to what we're imagining it to be. But I see a great potential.",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","I think being able to do symbolic regression reliably with backprop rather than genetic algorithm is in itself a great achievement, but maybe the paper wouldn't sell if framed that way...",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","This case is different tho. I’ve never seen so much attention and praises that a paper is the next breakthrough, while all they tested on are toy examples even smaller than mnist, which is what I did in my first ever ML project. This hype seems a bit absurd.",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","> Suddenly you can send a mini-AI model in outer space to measure gravitational waves (instead of using some analytical formula that is known to produce errors)

What are you talking about??",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)",Oh for sure but give it a month and we will find something new.,misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)",I came up with a fictitious scientific application.  I don't want to come on Reddit and write exactly what I do in real applications.,misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","Fair enough, but your fictitious example is very confusing and vague. I’m yet to see anyone clearly describe the supposed great promise of KANs, it doesn’t seem like it should be so difficult.",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","Vague? It's fairly straightforward and close to cases I saw in real life.

Think of an embedding system for high end applications: Aerospace, nuclear power plants, telecommunications, etc. Many of the systems have hard coded functions for a few reasons 1. Power consumption. 2. Interpretability. Even if we had the capacity to store a MLP model in there, well, we don't actually trust how the thing will behave when the system operates in a weird regime, in case of space, that could be some sort of radiation.

Now, my point about KANs was, what if by definition one is capable to - from start - define units that when put together form a close, tight, function, that I can safe derive limits of safety operation. That would be different than retrieving the learnt matrices and plugging in my system.",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","In this regard, where’s the key difference with all the interpolation and regression methods that predated neural networks?",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","The same reason why neural networks gained popularity in the first place - a bit of representation power, sure, but mostly  modularity (blocks), automatic differentiation and  computing power.

Interpolations are used sometimes. It's not something I often saw being used. It was mostly ""curve fitting"". The point here is KAN seem to provide another alternative symbolic regression, yet respecting the modularity aspect of NNs.",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","I'm afraid I don't see the big difference. Looking at the symbolic formulas in the KAN paper, I don't see any difference from what you can get with classical regression - which can itself have a modular form, easy differentiation, computational tractability, and so on. The thing is that the KAN expression types are totally arbitrary, not of any clear interest. (I'm looking mainly at B-F in Table 5 since I happen to know about knot theory.)",misc
[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video),"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties/results of KANs like continual learning, sparsification, symbolic regression etc.

Link here: [https://youtu.be/7zpz\_AlFW2w](https://youtu.be/7zpz_AlFW2w)","Sure. Do you happen to know an easy, classical regression package where one can easily perform symbolic regression in a modular fashion building on automatic differentiation? 

The key point is not only interpolation for the sake of interpolation. The key point is that I want to interpolate conditioned on the dictionary of terms I'm interested. Oh, and if possible, the term has to be an arbitrary function, model, or script. 

If by all means there's a package doing the same out there, they are very welcome. The statisticians pushed back saying Generative Additive Models are out there since ever. Sure, how practical is the infrastructure around it? 

So far, I have yet to see a valid criticism on KANs.",misc
[R] Classification tasks with llms,"Hey! as part of my thesis I have seen good results on using AutoModelForSequenceClassification for large texts with llm's (mistral) instead of bert but I have some questions I am trying to figure out:

1. is there a point for having a coherent, understandable prompt with task definition and targets while using AutoModelForSequenceClassification?
2. I have seen an improvement for the model using cot (chain of thought) prompting, when I asked him to: ""write out your reasoning step-by-step to be sure you get the right answers!"", is there any explanation why it does help? we are using a classification head instead of the decoder one so it's not auto regressive.","Using a prompt might shift the model representations closer to the problem space and make easier to tune  
But that could disappear with many examples

See related thread about classification in general [https://www.reddit.com/r/MachineLearning/comments/1crxhfp/comment/l41tj0o/](https://www.reddit.com/r/MachineLearning/comments/1crxhfp/comment/l41tj0o/)",misc
[D] Any reason not to submit to NeurIPS?,"As we all know, abstracts are due tomorrow. I'm on the fence on being able to finish a strong submission in a week. I know that I can always withdraw if reviews are bad (or if I don't feel like I have a strong submission in a week when it's due), but I'm worried that there might be a trace of the submission left online which future reviewers would be able to google. Can anyone confirm that this is only the case if you don't withdraw and instead submit a rebuttal that results in a rejection? If you withdraw from openreview, is any trace of it left online? Do you have to do some trick where you edit and scrub your submission before withdrawing? I know submission results are stochastic, so I'd like to know when, if ever, submitting is a strategic blunder.",You can submit an abstract and withdraw before the actual submission deadline if the paper is not of high enough quality.,misc
[D] Any reason not to submit to NeurIPS?,"As we all know, abstracts are due tomorrow. I'm on the fence on being able to finish a strong submission in a week. I know that I can always withdraw if reviews are bad (or if I don't feel like I have a strong submission in a week when it's due), but I'm worried that there might be a trace of the submission left online which future reviewers would be able to google. Can anyone confirm that this is only the case if you don't withdraw and instead submit a rebuttal that results in a rejection? If you withdraw from openreview, is any trace of it left online? Do you have to do some trick where you edit and scrub your submission before withdrawing? I know submission results are stochastic, so I'd like to know when, if ever, submitting is a strategic blunder.","Is there any trace of the withdraw, either at that stage or after the first round of reviews?",misc
[D] Any reason not to submit to NeurIPS?,"As we all know, abstracts are due tomorrow. I'm on the fence on being able to finish a strong submission in a week. I know that I can always withdraw if reviews are bad (or if I don't feel like I have a strong submission in a week when it's due), but I'm worried that there might be a trace of the submission left online which future reviewers would be able to google. Can anyone confirm that this is only the case if you don't withdraw and instead submit a rebuttal that results in a rejection? If you withdraw from openreview, is any trace of it left online? Do you have to do some trick where you edit and scrub your submission before withdrawing? I know submission results are stochastic, so I'd like to know when, if ever, submitting is a strategic blunder.",Nope. NeurIPS doesn't publish reviews for rejected papers. ICLR does tho so keep that in mind if submitting in the fall instead.,misc
[D] Any reason not to submit to NeurIPS?,"As we all know, abstracts are due tomorrow. I'm on the fence on being able to finish a strong submission in a week. I know that I can always withdraw if reviews are bad (or if I don't feel like I have a strong submission in a week when it's due), but I'm worried that there might be a trace of the submission left online which future reviewers would be able to google. Can anyone confirm that this is only the case if you don't withdraw and instead submit a rebuttal that results in a rejection? If you withdraw from openreview, is any trace of it left online? Do you have to do some trick where you edit and scrub your submission before withdrawing? I know submission results are stochastic, so I'd like to know when, if ever, submitting is a strategic blunder.","It seems like your worries arise from the assumption that reviewers will tend to not act in good faith. If you believe this then why ever submit?

The answer is no though. They cant view past submissions nor other reviews until they submit their own review. Additionally, the authors are hidden from them even after they submit their review.",misc
[D] Any reason not to submit to NeurIPS?,"As we all know, abstracts are due tomorrow. I'm on the fence on being able to finish a strong submission in a week. I know that I can always withdraw if reviews are bad (or if I don't feel like I have a strong submission in a week when it's due), but I'm worried that there might be a trace of the submission left online which future reviewers would be able to google. Can anyone confirm that this is only the case if you don't withdraw and instead submit a rebuttal that results in a rejection? If you withdraw from openreview, is any trace of it left online? Do you have to do some trick where you edit and scrub your submission before withdrawing? I know submission results are stochastic, so I'd like to know when, if ever, submitting is a strategic blunder.","My worry is that if the submission is rejected/withdrawn, subsequent reviewers will google the title or key words to see if the work was previously rejected/withdrawn. Based on what I've found in this sub, this is common.",misc
[D] Any reason not to submit to NeurIPS?,"As we all know, abstracts are due tomorrow. I'm on the fence on being able to finish a strong submission in a week. I know that I can always withdraw if reviews are bad (or if I don't feel like I have a strong submission in a week when it's due), but I'm worried that there might be a trace of the submission left online which future reviewers would be able to google. Can anyone confirm that this is only the case if you don't withdraw and instead submit a rebuttal that results in a rejection? If you withdraw from openreview, is any trace of it left online? Do you have to do some trick where you edit and scrub your submission before withdrawing? I know submission results are stochastic, so I'd like to know when, if ever, submitting is a strategic blunder.","I saw that someone withdrew their submission from ICLR 2024 got into CVPR 2024. They changed their title a bit.

openreview.net/forum?id=DIHBt1OUli (Withdrawn ICLR submission)

It appears in the CVPR 2024 Accepted Papers list (no link available)

I don't think it should be an issue.",misc
[D] Any reason not to submit to NeurIPS?,"As we all know, abstracts are due tomorrow. I'm on the fence on being able to finish a strong submission in a week. I know that I can always withdraw if reviews are bad (or if I don't feel like I have a strong submission in a week when it's due), but I'm worried that there might be a trace of the submission left online which future reviewers would be able to google. Can anyone confirm that this is only the case if you don't withdraw and instead submit a rebuttal that results in a rejection? If you withdraw from openreview, is any trace of it left online? Do you have to do some trick where you edit and scrub your submission before withdrawing? I know submission results are stochastic, so I'd like to know when, if ever, submitting is a strategic blunder.","Yes it happens, this is a high volume field and not all reviewers really put in the effort, or follow the rules. However, I think it is a bit of an irrational fear. Plus, if you do end up rejected/withdrawn and come back with a significantly improved version of your paper it should hopefully sway all but the absolute laziest of reviewers, even if they somehow saw your last submission's reviews.

On the other hand I think it is fine to hold off if you think the chance of acceptance is very low, or if you think there is a decent chance of it being accepted and you would like to improve it for personal satisfaction.",misc
[D] Any reason not to submit to NeurIPS?,"As we all know, abstracts are due tomorrow. I'm on the fence on being able to finish a strong submission in a week. I know that I can always withdraw if reviews are bad (or if I don't feel like I have a strong submission in a week when it's due), but I'm worried that there might be a trace of the submission left online which future reviewers would be able to google. Can anyone confirm that this is only the case if you don't withdraw and instead submit a rebuttal that results in a rejection? If you withdraw from openreview, is any trace of it left online? Do you have to do some trick where you edit and scrub your submission before withdrawing? I know submission results are stochastic, so I'd like to know when, if ever, submitting is a strategic blunder.","If Neurips don't publish reviews for rejected/withdrawn papers, how would subsequent reviewers find anything on google?",misc
[D] Any reason not to submit to NeurIPS?,"As we all know, abstracts are due tomorrow. I'm on the fence on being able to finish a strong submission in a week. I know that I can always withdraw if reviews are bad (or if I don't feel like I have a strong submission in a week when it's due), but I'm worried that there might be a trace of the submission left online which future reviewers would be able to google. Can anyone confirm that this is only the case if you don't withdraw and instead submit a rebuttal that results in a rejection? If you withdraw from openreview, is any trace of it left online? Do you have to do some trick where you edit and scrub your submission before withdrawing? I know submission results are stochastic, so I'd like to know when, if ever, submitting is a strategic blunder.",It sounds like that was OP's reason for asking in the first place.,misc
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","I'm a mathematician but you should probably take a introductory course to some proof-heavy subject. Anything would be good, analysis would be the best I think.

You need exposure to proof techniques and you need experience at attempting proofs. After you have a little bit of maturity and confidence, you should be able to work proofs for any subject that you're trying to study. Remember that the authors might also have skipped steps (or even got some of them wrong!) and try to reach out when you're confident that there's something missing.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","I used to work for a scientific publisher, so I read lots of papers that I had no background for.

To answer your question very generally, when I read a paper in an unfamiliar domain, I follow a familiar pattern:

1. The first read, I just let it wash over me and don't worry to much about anything. I will generally highlight terms I don't understand, and note any important citations that are new to me. But other that the first read is just to get a feel for things.
2. Step two is to look up any unfamiliar terms, and read at least the abstract for any citations that seem really central to the work. (this step might be interwoven with the first read)
3. After looking up unfamiliar terms and reading abstracts for the citations, I then read it a second time, trying to put the pieces together. I'll write any questions I have in the margin, highlighting the parts that made them.
4. If I can't answer my questions by the end of the paper, I'll start googling, now that I know what terms to search for because I highlighted either the lingo or the important authors and papers.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","Towards combatting imposter syndrome, know that a large percentage of ML proofs are basically puffed up nonsense.  Meaning, technically correct but written in far more formal language than necessary to say something pretty simple.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.",You might be surprised at how little even a lot of profs follow some of that stuff.,general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","I’d say that most of the proofs in, say, most neurips papers are either illegible, incorrect, or pointless. This is even the case in very high profile papers, thinking for instance about the Adam optimizer paper or the original GAN paper. (Speaking as a mathematician.) I don’t think you need to worry about it too much.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","Learn more math.  Or don’t.  These days, you can be an AI expert after building your first API framework.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","Do you have a good background in CS? No offense but a proper CS background should definitely equip you with being and to read proofs, especially those in the typical ML papers which aren't usually complex or long.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","Bank is a boring business, the same as Java.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","GPT it, or contact the authors immediately",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.",Claude,general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","If possible, can you suggest some general resources? I struggle with understanding proofs establishing bounds/rate of convergence etc. It’s just like you said - I need some exposure about proving techniques. I’m trying to read papers and learn unknown terms/concepts which I come across but it’s a rather scattered process.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","""Most ML proofs are puffed up nonsense"" is a popular opinion around these parts, but I honestly don't think it's the case.  The vast majority of papers I've read from NeurIPS/ICML/ICLR/etc that include proofs have excellent high level descriptions of why those proofs are important.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.",Lol no. Once the supply and demand of ML expertise gets back into balance you'll have a tough time getting a job at a top company or industry lab if you don't understand the math behind ML. Better study up before it's too late.,general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","Well, you also don't need a lot of math to be an ML engineer.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.",Umm… no.  Domestically educated undergrads have barely enough math to identify the branch of math they’re being shown.,general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","My favourite undergraduate book was Spivak's Calculus. I think it's very easy to fall in love with Analysis in that book.

That being said, don't be like me, try not to waste one day of your life in one problem. Look it up and move on.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.",Check out Terence Tao’s analysis,general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","YMMV, but the practical issue is that:

* For much of the advances of the last decade, you could strip every single proof from every single paper, and the field (in terms of where interesting SOTA is happening) wouldn't have evolved very differently.

Minor exceptions for certain things like diffusion, papers that are specifically theoretical, some attention formulations, etc.

* And, ML proofs (or, to be even more jaded, many of the attempts to make things look math-y that really aren't) are very rarely actually ""useful"".  

""Useful"" is in the eye of the beholder, but what I mean here is that they rarely lead to new insights (again, setting aside papers that are very explicitly focused on the theoretical, rather than ""I did a thing and bolted a proof [or something that looks like a proof] on afterwards"").  

""Wow that sounds like a bold and refutable claim.""

Maybe--but the reality is that you very rarely see follow-on papers which build on proofs.  In other fields where proofs are deeply important (pure math, applied math, physics, etc.), you see people build on prior theoretical work.  This (setting aside some of the examples above) rarely happens in ML.

(Maaaybe all of the ""proofs"" we see will be appreciated for their genius in the decades to become, and lots of value is being created for future readers.  But this seems...doubtful.)

In practice, this means that most of the ""formal"" language we see in the papers are really for communication, not actual proofs.  In which case, throwing down random set notation is frequently just noise.

Turning this around the other way--

The best and most impactful papers over the last decade typically *did* use formal language, but *sparingly*, i.e., when it is actually helpful to crisply communicate key ideas.  

There are too many papers which go the opposite direction--they've got an interesting insight or two and then decide how much of their communication they can move into faux-rigorous math language.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","In the end real AI won't require linear algebra proofs. It's all just people bandwagoning on discrete math because it gets you into conferences. 
Real revolutionary ideas won't be so complex that you need 12 lemmas and a new theorem to write q paper on it.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","You assume that college is the only source of math for the curious undergrad (who, in this scenario, is ambitious enough to fill in gaps of their knowledge).",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.",Not at a top program and especially not if you don't try to avoid every math or CS theory course.,general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","Spivak's Calculus is amazing, so I second this.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.",Thank you! Will check it out for sure :),general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.",Edit: ignore this,general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","100% this. Proofs the way they are written in the big ML conferences are mostly to embellish and provide a false sense of correctness/mastery. 

Far too often we see a 3 page proof, just to go over the experimental session and see experiments without replications, without confidence intervals, without reproducible code. 

Sure, in theory you just proved to me these embeddings are bounded by some epsilon in RKHS. I followed it, I'm also a mathematician. But then I ran a test with an heuristic MSE and your method isn't statistically different than another method that is simpler and easier to understand. 

Now what? Now the paper is getting dust and it's so arcane to understand and honest future researchers will waste their time and discover it the worst way a. Because some people wanted to be fancy and publish at ""top conferences"" b. But weren't rigorous all the way down in communicating results and experimentation.

Very frustrating....",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","Using a multi-layer neural network, to approximate arbitrary functions, is not a useful idea then I guess...",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","Yeah, it’s a safe assumption on average.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","Yes, this, which flags the other issue I didn't think to touch upon, which is that most ""proofs"" are just backwards rationalizations.

Proofs (broadly defined) are particularly interesting when either 1) we write a proof and it then informs the tests we run or at least 2) it helps explain something that happened, and then informs additional takeaways/experiments.  

You rarely get (1) or (2)...even if we include follow up papers.

(To be clear, when (1) or (2) happens, that often *is* very intriguing!  But it is far from the norm.)

And ""the math formalizes what happens""...

9/10 times, just give us the code, because the ""formalized"" explanation is so frequently missing key details.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.",Also this doesn't require linear algebra proofs. Just linear algebra.,general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","Yeah, that's an idea from 5 decades ago lmao

Come up with something new, not tiny iterations on a very straightforward curve-fitting algorithm.

Finding these mathematical bounds on performance is pretty useless. The resulting ""groundbreaking"" networks are 0.1% better on benchmarks. The underlying paradigm of backprop is flawed. Animals do not use backprop and vastly outperform neural networks. Real time learning, inventiveness, spatial reasoning, reliability, etc.",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","It was proved in 91 (iirc) actually. So more like 3 decades ago. And we use that proof to develop further proofs, such as that Transformers are Turing complete (2018 iirc). Understanding how and why things work is generally useful...",general_qa
"[D] How do you get better at reading proof in the ML papers, with background in CS only?","Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!

Edit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","Yes that theorem was from the 90s, but NNs were invented before that.

A real AI will not be built from systems that are easily understood with proofs. It will be invented without rigorous mathematical proofs, but still built with math, based on new ideas, not feedforward neural networks.

No one actually needs to know for sure whether transformers are provably turing complete. We just need to build them and test them, and know that they suck compared to what nature has built. Transformers are a faulty and lazy architecture, and will be superseded by something much better.",general_qa
[D] Research on diffusion models with focus on efficiency and reducing computation cost,"I need help with the specific keywords,  notable research or surveys  on the mentioned topic.  I have only tried with keywords such as: ""edge device"", ""edge computing"", ""diffusion models"",  on google scholar and the results return are less than satisfactory.  Can someone give me some suggestion on where to look at ?","check out the second half of this excellent blog  [What are Diffusion Models? by Lilian Weng](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)

Most of the seminal works in diffusion models recently have been about efficiency and reducing compute",misc
[D] Research on diffusion models with focus on efficiency and reducing computation cost,"I need help with the specific keywords,  notable research or surveys  on the mentioned topic.  I have only tried with keywords such as: ""edge device"", ""edge computing"", ""diffusion models"",  on google scholar and the results return are less than satisfactory.  Can someone give me some suggestion on where to look at ?",We have some extensive MLSys optimizations in CommonCanvas to achieve this: [https://arxiv.org/abs/2310.16825](https://arxiv.org/abs/2310.16825),misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.",Because you have residual connection from the beginning of the FFN.,misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","There's an activation function in between those 2 linear layers making the whole ffn non linear.

The way it works in theory is that the attention layer should give info to the embedding about previous embeddings and the ffn should give info that were stored in the model",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","Since then all of the layers at the start of the next block would use more parameters. d\_feedforward is typically 4x the parameter count of d\_model, so each Q, K, and V matrix would be using 4x the parameters. Overall, this is more than the parameter penalty than just having a single d\_feedforward\*d\_model layer and then three d\_model\*d\_model parameter layers (the Q, K, V matrices).",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","If you use only a single linear transformation however spare then you have no privilaged bias\[1\], the resulting circuit between the last linear layer and the QKV projection is rotationally invariant (if you also include the residual connections and layer norm then they do not have a privilaged bias either)\[2\]. Thus any neuron between the two layers can learn any feature which is not very appealing in terms of interpretability. Having a priviliged bias means features represented by each individual neuron are significant and information is not stored at the population level  
  
Also if you use a 1d convolution, that is exactly the same as a linear projection  
You can't use a 2d convolution in casual transformer blocks  
  
\[1\] - [https://transformer-circuits.pub/2022/toy\_model/index.html](https://transformer-circuits.pub/2022/toy_model/index.html)  
\[2\] - [https://arxiv.org/pdf/2307.12941](https://arxiv.org/pdf/2307.12941)",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","If you are talking about replacing the linear layer with a 1d conv, I actually did this experiment with a small ViT. You get a huge reduction on parameter. But your accuracy also drops. On cifar 10 - the full fledged vit has 3 million params and reaches an accuracy of 0.89 whereas replacing the last linear layer with longconvs results about 0.3 mil params but fizzles out at around 0.72. (please mind that the implementation was pretty raw). 
My intuition from reading some part of Hyena filters and a paper called ""Pay attention to MLPs"" is that the MLP layers actually do a lot of heavy lifting in transformers. Just replacing them with sparse layers might affect accuracy.",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","Looks like I hallucinate things;

I thought about permutations and things. The first linear layer can make all sorts of permutations while the value embeddings also can makes all sort of permutations.

Here is a test of approximation-ability; The MSE does not reduce to machine precision, only to 0.002.

[https://drive.google.com/file/d/153UGUR8Mn\_rrtCqoMDi74EFCPzk6R\_TN/view?usp=sharing](https://drive.google.com/file/d/153UGUR8Mn_rrtCqoMDi74EFCPzk6R_TN/view?usp=sharing)

The implement is not optimal; just a proof of concept.",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","I also wondered by there is always this ""expand and contract"" pattern for the MLP at the end of transformers, where we double the hidden dim and the halve it.",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","It packs more parameters per block for a given embedding size. Which means it can approximate a more complex linear function than single FF layer. 

Increasing embedding size penalize attn/context cost and increasing block count gets way too deep.",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.",I did propose that we can replace it with a super sparse operation at the end.,misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","I was intending to replace that particular linear layer with a super sparse linear layer; or something like a resemble convolution with stride, example:

linear = nn.Linear(4, 1)

input = torch.randn(b, n\_s, n\_d)

input = input.view(b, n\_s, n\_d // 4, 4)

input = linear(input).squeeze(-1)

So no extra parameter.",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","Yeah I made a mistake but you kinda misinteprets my idea of convolution; I also made a small experiment. The experiment shows 0.002 MSE for learning random MLP feature, which shows that I'm wrong. If I were right, it would reduce to machine precision.

[https://drive.google.com/file/d/153UGUR8Mn\_rrtCqoMDi74EFCPzk6R\_TN/view?usp=sharing](https://drive.google.com/file/d/153UGUR8Mn_rrtCqoMDi74EFCPzk6R_TN/view?usp=sharing)

My idea of convolution means (b, n\_s, n\_d) ->(b \* n\_s, 1, n\_d) -> Conv1D; Conv1D here takes (batch\_size, n\_features, n\_dimensions). I thought it can work because the first linear layer can learn all sort of permutations and the qkv linear layer of next layer can also learn to rearrange the dimensions on a more global level. The convolution also has bias.

I will read the work after I woke up. Thanks for sharing.",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","It's popularized by MobileNetV2 (Inverted Residual) to improve information flow; check Figure 1.

MobileNetV2: [1801.04381 (arxiv.org)](https://arxiv.org/pdf/1801.04381)",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","What does a ""more complex"" linear function mean?",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","There should be no more complex linear transformation than a single dense linear transformation. A chain of linear transformation of dimension (d0, d1,...,dn-1) should have the same representation power to one single dense linear transformation having dimension equal to the minimum of (d0,... ,dn-1)


XW1W2 = X(W1W2) = XW'",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","Can you specify what you mean by super sparse operation?

If you are talking about your code in your reply to ClearlyCylindrical, it does not preserve the useful characteristics of a residual connection. If you place your residual connection between the ""super sparse operation"" and your self-attention layer, then you are not creating a universal function approximator within the boundary of your skip connection. Ensembling many small MLPs is one of the desireable characteristics of skip connections.",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.",Its even *more* linear,misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.",If that were true then every llm is silly for not taking the chance on decreasing parameter count by 8 times.,misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.",It can't. But the linear layer transforming the value vector will fix that. I probably will make a code experimenting if there exists Xspr(W1)W2 = XW1W2 for all/most of XW1W2.,misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.",linearer,misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","No, it is true. You probably misunderstood their meaning.

There is no activation function at the end of the FFN, so they believe that the last layer of the FFN and the linearities in the attention module will collapse into a single linear transform.

They are not asking why we have a wide FFN to begin with.",misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.",How does it fix that?,misc
[D] The usefulness of the last linear layer of each transformer layer,"This is a pretty obvious.

I recently see that the last linear layer of transformer is kind of a waste of parameters.

A transformer model is a stack of many transformer layers.

These layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\_model \* d\_dim\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.

We all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.

So why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","I thought the first linear layer and the QKV linear layers of the next layer can learn to rewire things / any permutation.

I am wrong, these permutation learning is not sufficient.

[https://drive.google.com/file/d/153UGUR8Mn\_rrtCqoMDi74EFCPzk6R\_TN/view?usp=sharing](https://drive.google.com/file/d/153UGUR8Mn_rrtCqoMDi74EFCPzk6R_TN/view?usp=sharing)

The MSE Loss is around 0.002; in the notebook of the small experiment I share.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","it seems like you're using linear transformer and choosing the kernel as the taylor approx of softmax? If so, this paper has done this before (Building Block 2): [https://hazyresearch.stanford.edu/blog/2023-12-11-zoology2-based](https://hazyresearch.stanford.edu/blog/2023-12-11-zoology2-based)",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",I would be interested in understanding this but the notations used are not very readable at all...,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","When I embark on an idea like thus, after trying a proof of concept, I ask why this donesnt exist already. There are s lot of smart people out there thinking along similar lines. .",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","If I understand corectly, yoy want to approximate self attention using the taylor series, while using parallel scan techniques of ssms to speedup that process.

This might have some value, but I can see several things go wrong

1. The number of coeff you need to get good approximation might be way to large (which i think you said), that might be the real issue here. How many coefficent do you need as the number of tokens grow (if it grows at all)? The scaling for accurate representation might be just as bad as O(n^2)

2. The tylor seriea is famously a pretty bad approximate for functions, id look into other approximations that are cummomative.

3. It doesn't work until you train it, and issues with computing precision and gradient flow might still make this infisable.

I was just reading this on a bus, so maybe i completly missunderstood.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","Is this the kind of thing you are talking about?

    import numpy as np
    
    def parallel_scan(arr):
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        step = 1
        while step < n:
            for i in range(step, n, step * 2):
                arr[i] += arr[i - step]
            step *= 2
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k, n)  # k^n element-wise
        q_power = np.power(q, m)  # q^m element-wise
        partial_sum_kv = parallel_scan(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(q)
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                A_nm = 1.0  # Simplified coefficient for illustration
                basis_function = compute_taylor_basis_function(q, k, v, n, m)
                attention_numerator += A_nm * basis_function
                normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(v), n, m)
                attention_denominator += A_nm * normalization_basis_function
        
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",Structured State space model?,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",I didn't understand a thing of what you wrote but you should probably do some experiments even on small language models (less than 500M parameters) to show gains of performance and no or little loss of quality,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is before. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)!

    import numpy as np
    
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",am I the only one that thinks this is just gibberish and he's making it unnecessary complex just to sound smart?,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","This is LLM-generated drivel... Move along people, nothing to see here...",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","Update:

So I read the blog post and indeed it seems that they are doing the same thing that I am. They even give a formula for computing all of the second order terms! Thanks for sharing!

Previous Comment:

No this is not a linear transformer. It is a Taylor series expansion of a vanilla transformer with a single head. It computes softmax(QK\^T)V. I'm using the parallel scan algorithm to compute the Taylor series basis functions of the query and key components and then adding them up to give the equation above. Each Taylor series basis function takes log(N) time and N steps of computation. The big caveat is that the number of basis functions that you would have to calculate would make it so that the total amount of computation is bigger than N\^2. But I think that's just because the softmax is a hard activation to compute using scans, at least the way that I did it in the post. I'm betting there is a more efficient activation that can be used in place of the softmax.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",Seconded. OP please write this up in markdown. Could be a good idea.,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",Which part is the most confusing? Maybe I can rewrite that part.,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","This reminds me of a joke about an economist. An economist sees a $100 bill on the ground, and thinks to himself, ""that can't be a $100 bill because if it was, someone else would have picked it up"", and so keeps walking.

Jokes, aside, what I laid out could fail and it would be very interesting if it did. I don't think computing the softmax using taylor series basis functions is a practical or good way to compute an activation. Probably the number of terms you would need would negate the reduction in computation per term. There are other activation that can be computed with a single scan. If they also fail, then it whether or not something can be efficiently computed with a parallel scan or not would be predictive of its computational power, which I think would be very interesting. But if I had to bet, I doubt there is a deep relationship between what can be efficiently computed with a scan and computational power. I think probably a different activation that can be computed with a single or a few scans will probably do just as well as the softmax.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","I think you've got it! Thank you for taking the time to read!

But I don't understand your third point, can you explain a bit more?

About the number of coefficients, yes, it's impractical to compute the softmax activation using the algorithm that I outlined. But neural networks aren't too sensitive to the exact activation, so long they are nonlinear and make the NN a universal approximator. I'm betting that there is an activation that can be computed with just a few scans that can perform as well as the softmax.

About your second point, I think it's related to your first, that you might need a lot of coefficients, since taylor series are bad approximators... although when the inputs of a taylor series get larger than or smaller than certain values, the it can diverge by a lot. Is that what you meant? The good news is that you can generate sines and cosines and exponential functions with one scan, and they might serve as better basis functions for creating interesting activations.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","# I made a bunch of changes. The algorithm could be more efficient, for instance I did two loops over indices of the queries and keys tensors, but really you only need one because you can do k_power**n,  q_power[:,i]**m and compute basis functions in parallel. I added a comment starting with ""# change:"" to explain what changes I made. I have not ran the code so not sure if it is buggy.
    
    import numpy as np
    
    # change: implemented in log(n) steps and changed the name
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
    
        return arr
    
    # change: added inices i, j for the components of q and k. If v is the value vector, expand dims of the power for broadcasting, else v is the denominator, so don't expand dims.
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0]) # change: softmax normalization is per position
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # change: adding ij indices, and using the proper shape for the denominator
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1) # change: for broadcasting
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","I don't know if you did this, but I tried gpt-4 to get clean python code out that the OPs text, and it gave me something quite similar

Edit, here's gpt-4 writing the pytorch code for the OPs description

    import torch 
    def parallel_scan(x):
      # Perform parallel scan (prefix sum)
      n = x.size(0) step = 1
      while step < n: 
        for i in range(step, n): 
          x[i] += x[i - step] step *= 2
      return x
    
    def taylor_expansion_basis_function(q, k, v, n, m):
      # Compute the basis function for the numerator
      q_power = torch.pow(q[:, None, :], m) 
      # Broadcasting over sequence length
      k_power = torch.pow(k[:, :, None], n)
      # Broadcasting over embedding dimension
      k_v_product = k_power * v[:, None, :]
      result_numerator = q_power * parallel_scan(k_v_product.sum(dim=1))
      # Compute the basis function for the denominator
      k_power_summed = parallel_scan(k_power.sum(dim=1))
      result_denominator = q_power * k_power_summed
      return result_numerator, result_denominator
    
    def causal_self_attention(Q, K, V, A, num_terms):
      # Compute query, key, and value matrices
      q = Q
      k = K
      v = V 
      # Initialize numerator and denominator tensors
      num_basis_funcs = q.size(0)
      numerator = torch.zeros_like(v)
      denominator = torch.zeros((v.size(0), v.size(2)))
      # Compute Taylor expansion basis functions and accumulate their weighted sum
      for n in range(num_terms):
        for m in range(num_terms):
          result_num, result_den = taylor_expansion_basis_function(q, k, v, n, m)
          coeff = A[n, m]
          numerator += coeff * result_num.sum(dim=-1)
          denominator += coeff * result_den.sum(dim=-1)
      # Divide the numerator by the denominator to get the final attention result
      attention_output = numerator / denominator.unsqueeze(2)
      return attention_output
    
    # Example usage for a simplified case
    sequence_length = 5
    embedding_dim = 4
    num_terms = 3
    Q = torch.randn(sequence_length, embedding_dim)
    K = torch.randn(sequence_length, embedding_dim)
    V = torch.randn(sequence_length, embedding_dim)
    A = torch.ones(num_terms, num_terms)
    output = causal_self_attention(Q, K, V, A, num_terms) print(output)",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","Yes, this is like an SSM, but where you apply the identity matrix as the recurrent step, so that you are essentially just doing partial sums.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","This isn't a practical way to do transformers. It's more of a proof that it can be done, that transformers can be implemented as parallelizable RNNs--ones with associative recurrence equations. The number of RNNs that you would need would be huge to compute the softmax activation, so it's not practical. Neural networks aren't too sensitive to which activation you use. Yes, choosing a suboptimal activation means longer training times and perhaps worse metrics, but scale the model up and it makes up for it. The softmax activation isn't a practical activation to compute with RNNs. MAMBA uses a different activation, a different recurrence equation, and uses the parallel scan algorithm, and it seems to beat transformers, while having linear compute and logN time steps. The fact that transformers can be cast as parallelizable RNNs and that MAMBA exists and is made of parallizable RNNs hints to me that with a different activation transformers might be possible with linear compute.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",When I read the first paragraph about NlogN and them not being an academic that’s what I was expecting. But it kinda makes sense when I read it. Though it would need to be written more formally (and ideally in latex) to really get what’s going on. Using Taylor expansion definite make sense.,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","I mean, it is complicated, and I did write a quick post, which to be fair, is pretty bad. To make it clear I'd have to spend much more time. I'm going to wait form someone to go through the math themselves to validate the arguments in the post, and if that doesn't happen I'll have to take the time, which I was avoiding, to write it out in great detail. Sorry for the poor writing.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",An LLM writes much better than I do ;-) What part of the post do you think is wrong?,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","yes, this is what the paper does, you’re essentially kernelizing softmax(QK^T) using the taylor approximation. and linear transformer does the same linear scan at inference time.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",reddit does not allow markdown does it?,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",maybe put it as a gist on github with markdown (.md) extention where Latex can be rendered automatically by just writing $a+b=c$,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","I mean to ask, how does your method behaves as the number of tokens grow? Would you need to scale anything? I also thought you might want to try other approximations than the tylor series, they might need less co efficents, I have no offhand suggestions though.

About the third point, im just saying there might be hardware issues or unexpected ones. 

I found once a review of several methods to aproximate softmax, ill send you the paper if i find it.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",not only this makes no sense but this doesn't answer my question at all,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","> To make it clear I'd have to spend much more time.


Now you understand why people write papers ;)


(but cool idea! you definitely should try clarifying and testing it to show equivalence, practicality of the approach aside it will tell you if it's worth continuing with)",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","Is it even mathematically possible to compute full causal self-attention in less than O(N\^2) operations? By ""full"" I mean every token attends to every previous token. Linear Transformer obviously doesn't have full attention.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","Maybe I misunderstood. My understanding of linear attention, is that you compute the outer product \`values queries\^T\` for each position, take the partial sum, and dot it with the query matrix in the end, like \`partial\_sum(keys\^T values) queries\`. I suppose you could cast the algorithm in the post in a similar light by using outer products. Let \`o\` be the outer product of the last index of two tensors. The formula for all taylor basis functions for power n and m would be something like \`partial\_sum(values o queries\^n) o keys\^m\`. Is that what you meant?",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",Great idea!,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","I must have misunderstood. What was the question? I thought you were telling me to run some experiments. I was trying to explain that the construct in the post isn't meant to be a practical model, that running experiments on it isn't appropriate.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",not too sure but i’m pretty doubtful,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","The trick is that you don't need to keep each separate softmax attention score, you sum them up in the final step, each multiplied by their respective value vector. Because you only need the sum, you can accumulate parts of it, by starting at the left and summing as you move to the right, which is a partial sum. You do this for each basis function of the taylor series and then add all the basis functions together to retrieve the self-attention layer. Partial sums can be computed in O(logN) time and O(N) computation.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","Yeah, the paper https://arxiv.org/abs/2302.13214 argues that it can’t be done under some reasonable assumptions.

The PolySketchFormer paper takes a similar approach, but they swap out exponential kernel attention for polynomial attention (which has a finite basis expansion, unlike the softmax) so technically you can come up with a linear-time algorithm for it. In practice these basis expansions are so large that context lengths would have to very large for the linear factor to dominate (in their case they use a some polynomial-kernel-specific results to approximate the inner product via sketching—super cool paper!)",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","Or even rewrite clearly by hand and upload some photos, if the Latex doesn’t agree with you",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",then show us your results. Also you probably mean 1B parameters as in 1 billion rather than 1M as in million,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","Yea I can't see a way that would be possible, afaik by definition it is O(N^2) to do full causal attention",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",or ask an llm to convert to markdown + clean it up,misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)","Since you used the O notation, technically that statement is right, but whether you can do it faster than N² can not be argued from definition alone. E.g. naively, one would think matrix-matrix multiplication would take at least N³.",misc
"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","\*Update\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.

I think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.

tl;dr Use the parallel scan\[1\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m v(i)$$ and the normalization $\\sum\_{i=0}\^{j-1} k(i)\_a\^n q(j)\_b\^m$. k(i)\_a\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\_b\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.

# Background

I was inspired to think about this because I was implementing MAMBA\[2\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \[a\_1, a\_2, a\_3, a\_4, ...\]. You can compute all partial sums by first adding a\_i to a\_{i -1}, where a\_{-1} is zero, and generally a\_{-n} is defined to be zero. Then take the result, call it r = \[a\_1, a\_1+a\_2, a\_2 + a\_3, ...\], and compute r\_i + r\_{i-2}, which gives \[a\_1, a\_1+a\_2, a\_1+a\_2+a\_3, ...\]. The first 4 partial sums are already complete. The next step would be r\_i + r\_{i-2\*\*2}, and the next step, just increase the power of 2 until i-2\*\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.

# The Meat of It

In the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.

Let's assume we have a tensor x of shape (sequence\_length, embedding\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\[:,i\]\*\*n)\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\[y\[0,:\], y\[0,:\]+y\[1,:\], ...\]. Now multiply the result by q\[:,j\]\*\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v).

What is left is to find the Taylor series coefficients A\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\cdot k$ in place of $q\[:,j,:\] \\cdot k\[:,i,:\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\cdot k) = 1 + (q \\cdot k) + (q \\cdot k)\*\*2 / 2! + ... + (q \\cdot k)\*\*n / n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\cdot k)\*\*n /n! for every n. It can be done but I'm not going to do it. Just assume that A\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\sum\_{n, m} A\_{n, m} x\[:,j\]\*\*m \* ParallelPartialSum((x\[:,i\]\*\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:

$$  
(\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)\*v)) / (\\sum\_{n, m} A\_{n, m} q\[:,j\]\*\*m \* ParallelPartialSum((k\[:,i\]\*\*n)))  
$$

Where again, A\_{n, m} are the Taylor series coefficients for exp( q \\cdot k).

# Take-Aways

One big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.

Non-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.

\[1\] [https://en.wikipedia.org/wiki/Prefix\_sum](https://en.wikipedia.org/wiki/Prefix_sum)

\[2\] [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)

# Update

Actually there is a better algorithm for the parallel scan given in the wiki link above\[1\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.

# Update 2

@[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\_Mittesdine](https://www.reddit.com/user/Lajamerr_Mittesdine/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.

    import numpy as np
    
    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.
    def parallel_partial_sum(arr): 
        """"""Parallel scan (prefix sum) implementation.""""""
        n = len(arr)
        steps = np.ceil(np.log2(n))
        
        for i in range(steps):
            # check if this is the numerator or denominator
            if len(arr.shape)==2:            
                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)
            else:
                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)
    
        return arr
    
    def compute_taylor_basis_function(q, k, v, n, m, i, j):
        """"""Compute a Taylor basis function for given powers n and m.""""""
        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise
        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise
        if len(v.shape) == 2:
            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast
            q_power = np.expand_dims(q_power, axis=-1)
        partial_sum_kv = parallel_partial_sum(k_power * v)
        basis_function = q_power * partial_sum_kv
        return basis_function
    
    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):
        """"""Compute the causal self-attention using Taylor series approximation.""""""
        attention_numerator = np.zeros_like(v)
        attention_denominator = np.zeros_like(v[:,0])
    
        for n in range(max_n + 1):
            for m in range(max_m + 1):
                for j in range(q.shape[-1]):
                    for i in range(k.shape[-1]):
                        # note, either i or j loop can be removed because basis functions can be computed in parallel
                        A_nmij = 1.0  # Simplified coefficient for illustration
                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)
                        attention_numerator += A_nmij * basis_function
                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)
                        attention_denominator += A_nmij * normalization_basis_function
        
        attention_denominator = np.expand_dims(attention_denominator, axis=-1)
        attention = attention_numerator / attention_denominator
        return attention
    
    # Example usage
    sequence_length = 10
    embedding_dim = 4
    
    # Randomly initialize q, k, v tensors
    q = np.random.rand(sequence_length, embedding_dim)
    k = np.random.rand(sequence_length, embedding_dim)
    v = np.random.rand(sequence_length, embedding_dim)
    
    # Compute the causal self-attention
    attention_output = compute_causal_self_attention(q, k, v)
    
    print(""Causal Self-Attention Output:"")
    print(attention_output)",Good point,misc
[P] Tips on training a Transformer model,"Hello everyone,  
I am attempting to train a hybrid Resnet18 encoder-6 layer 520 dmodel Transformer decoder to do full page handwriting recognition and I am struggling to properly train it. The main issues/questions I am stuck at are:

* I have a dataset that has around 6000 samples of pages of handwriting distributed 80% training/20% validation, how would I know if this is enough to work with?
* I am currently using gradient accumulation since each sample in the dataset is quite large, as you would imagine. It proved to be beneficial, as the behaviour of the model during training is less fuzzy than the past training attempts using a fixed batch size of 2
* I am currently exploring learning rate schedulers and have used, for now, a reduce on plateau on validation loss with a learning rate starting at 1e-4 with 0.75 factor. What is the best way to approach and choose lr schedulers

Even though the place where I am at seems to suggest that I am heading in the right direction (WER is slowly decreasing across epochs), the training time is very slow (12hrs for around 3-5 epochs) on available Kaggle GPUs (P100). Any tips for going forward?","""how would I know if this is enough to work with?"" in my experience, you can get a feeling for the increase in performance by halving the training dataset size, since doubling gives you an almost linear increase in performance.  

For different learning rate schedulers it seems to be tricky to explore and to be problem specific. Usually Cosine annealing works in most of my problems. 

You could try synthetic data with smaller images to ""pretrain"" with a larger batch size, but there is no guarantee that it will also perform better on your actual data.",misc
[P] Tips on training a Transformer model,"Hello everyone,  
I am attempting to train a hybrid Resnet18 encoder-6 layer 520 dmodel Transformer decoder to do full page handwriting recognition and I am struggling to properly train it. The main issues/questions I am stuck at are:

* I have a dataset that has around 6000 samples of pages of handwriting distributed 80% training/20% validation, how would I know if this is enough to work with?
* I am currently using gradient accumulation since each sample in the dataset is quite large, as you would imagine. It proved to be beneficial, as the behaviour of the model during training is less fuzzy than the past training attempts using a fixed batch size of 2
* I am currently exploring learning rate schedulers and have used, for now, a reduce on plateau on validation loss with a learning rate starting at 1e-4 with 0.75 factor. What is the best way to approach and choose lr schedulers

Even though the place where I am at seems to suggest that I am heading in the right direction (WER is slowly decreasing across epochs), the training time is very slow (12hrs for around 3-5 epochs) on available Kaggle GPUs (P100). Any tips for going forward?",just my knee jerk feel: 6k for training is not nearly enough for a project like this. Also you gotta get off google colab and use serious cloud compute.,misc
[D] A possible solution for the surging submissions in AI/ML top confs,"Just heard that Nips has just received over 16k submissions, which is really concerning me. Such abonormal explosion in paper number is very likely to cause a largely degraded average quality of each paper, and demand more reviewers for paper review, which also could largely worsen the average review quality. Both factor would ultimately defame the conf and the paper accepted by the conf.

A straightfoward solution that came to me is to fine-tune an LLM with high-quality reviews on both good and bad papers,  giving it expertise to filter out low-quality papers, and implement a pre-review (some confs seem to arleady have that) to lower the demand for expert reviewers.

Cons are the training cost/ethics/security/accuracy, etc.

Any thoughts?",obviously one should get the LLM to also write the papers then,misc
[D] A possible solution for the surging submissions in AI/ML top confs,"Just heard that Nips has just received over 16k submissions, which is really concerning me. Such abonormal explosion in paper number is very likely to cause a largely degraded average quality of each paper, and demand more reviewers for paper review, which also could largely worsen the average review quality. Both factor would ultimately defame the conf and the paper accepted by the conf.

A straightfoward solution that came to me is to fine-tune an LLM with high-quality reviews on both good and bad papers,  giving it expertise to filter out low-quality papers, and implement a pre-review (some confs seem to arleady have that) to lower the demand for expert reviewers.

Cons are the training cost/ethics/security/accuracy, etc.

Any thoughts?","Yeah... 2004 google spamdexing all over again.

""SubmissionEngineLand.com says that in order to pass pre-review, 5.7% of our keywords in the paper need to be SoTA - the little o is important!""",misc
[D] A possible solution for the surging submissions in AI/ML top confs,"Just heard that Nips has just received over 16k submissions, which is really concerning me. Such abonormal explosion in paper number is very likely to cause a largely degraded average quality of each paper, and demand more reviewers for paper review, which also could largely worsen the average review quality. Both factor would ultimately defame the conf and the paper accepted by the conf.

A straightfoward solution that came to me is to fine-tune an LLM with high-quality reviews on both good and bad papers,  giving it expertise to filter out low-quality papers, and implement a pre-review (some confs seem to arleady have that) to lower the demand for expert reviewers.

Cons are the training cost/ethics/security/accuracy, etc.

Any thoughts?",">Just heard that Nips has just received over 16k submissions

Those are rooky numbers. The abstract submissions have already passed [21k](https://twitter.com/thegautamkamath/status/1790834956547027446).",misc
[D] A possible solution for the surging submissions in AI/ML top confs,"Just heard that Nips has just received over 16k submissions, which is really concerning me. Such abonormal explosion in paper number is very likely to cause a largely degraded average quality of each paper, and demand more reviewers for paper review, which also could largely worsen the average review quality. Both factor would ultimately defame the conf and the paper accepted by the conf.

A straightfoward solution that came to me is to fine-tune an LLM with high-quality reviews on both good and bad papers,  giving it expertise to filter out low-quality papers, and implement a pre-review (some confs seem to arleady have that) to lower the demand for expert reviewers.

Cons are the training cost/ethics/security/accuracy, etc.

Any thoughts?","Hey there, you're right that surge in AI/ML submissions presents a real need for robust computational resources. One approach you might consider is using a marketplace for computing power like [CudoCompute.com](https://www.cudocompute.com/?utm_source=reddit&utm_medium=organic&utm_campaign=community-engagement&utm_term=/r/machinelearning). We provide access to underutilized global resources, making it sustainable and cost-effective. This could be a game changer for handling extensive ML tasks and let you freely take on more AI experiments. Also, it's a solid alternative to the Big 3 (AWS, Azure, Google Cloud), especially for high-demand use cases. Might be worth a look. Keep up the great work!",misc
[D] A possible solution for the surging submissions in AI/ML top confs,"Just heard that Nips has just received over 16k submissions, which is really concerning me. Such abonormal explosion in paper number is very likely to cause a largely degraded average quality of each paper, and demand more reviewers for paper review, which also could largely worsen the average review quality. Both factor would ultimately defame the conf and the paper accepted by the conf.

A straightfoward solution that came to me is to fine-tune an LLM with high-quality reviews on both good and bad papers,  giving it expertise to filter out low-quality papers, and implement a pre-review (some confs seem to arleady have that) to lower the demand for expert reviewers.

Cons are the training cost/ethics/security/accuracy, etc.

Any thoughts?",That’s absurd. I can imagine upcoming posts from authors/reviewers complaining about the poor quality of reviews/papers.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","What technology do you think they are using to make it faster? Quantization, MoE, something else? Or just better infrastructure?",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Has anyone else done multimodal output with an LLM? Directly generating audio and images? I haven't seen one, but I bet there are some papers I've missed.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Do anyone have a clue why 4o achieves a super-fast inference? Is the model actually much smaller than GPT4 (or even 3.5, since its faster than 3.5)

I've looked into the openai releases, but they don't comment on the speed achievement.

Thought that to get better performance in LLMs, you have to scale the model, which is going to eatup resources.

For 4o, despite its accuracy, it seems that the model computation requirements are low, which allows to be used for free users too.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Supposedly it's available on the free version of Chatgpt, but I don't have access to it.  I'm using the web version, but apparently I'm one of the last few people in the world with a computer and everyone else uses their phone, so hard to find out whether others have access or not.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Anyone else here wonder how the heck they made the speech model to have emotions, change in tones, sing, understand like stuff like if you tell them to talk faster or slower? That part is the more crazy part to me.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",">this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)

How do you know?",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",So why would you pay now for GPT Plus?,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Gpt-4o is the Gemini that google promised, but better.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","The movie ""Her"" doesn't look too far away to happen IRL now.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","On first glance it looks like a faster, cheaper GT4-Turbo with a better wrapper/GUI that is more end-user friendly. Overall no big improvements in model performance.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",When they gonna develop an AI that knows when to shut up,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","> freely available on the web

Where?",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Hilarious and a good time to look at other competitors.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",crazy cool and definitely eliminating many startup ideas,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",These tools have really amazing GUIs but what else? The frontends always look amazing then the backends disappoint once you get past rudimentary examples.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I'm interested in this. The trend from GPT4 to GPT4-Turbo, to this seems like they're making the flagship models smaller. Maybe they've found a good path to distill the alignment into progressively smaller models.

If it was something like speculative decoding, quantization, or hardware improvements, you'd think that they'd go back and apply it to the older models to save on serving costs.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",One component would be the new tokenizer (more for languages other than English). Less tokens per string means faster generation.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","The CTO did say something along the lines of ""thank you to Nvidia for providing us with the gpus to make this possible"" so perhaps they are also using better faster gpus on top of other optimization technics",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Better data? It is their next-gen model, it has to have all their new tricks.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","All of them, I guess.

Batching also helps. Doesn't make it faster for the user, but makes it scalable and enables really high cumulative tok/s per GPU.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","They mentioned how multimodality was now being handled within the same model, right? So perhaps they also added their moderation models directly into the same architecture? I suppose that would speed things up, in any case it would take away one de-embedding and embedding step. Similar for the multimodelity, you're essentially removing the decoder and encoder steps between models.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",I think they are taking incremental improvements in inference speed and iteratively pruning while leveraging mixture of experts more heavily as time goes on.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",More compute,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Just better architecture, there is a ton of minor architecture breakthroughs and improvements they probably have in secret.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",The magic of removing the throttling delay :),misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Overtraining,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I’ve yet to see any papers in respect to models that work with text, audio, and images within a single end-to-end architecture. IF anyone has seen one, please share! 

It’s seems like it was the natural and obvious directions to go -- after LLMs, CLIP, Baklava, etc.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",[AudioPaLM](https://google-research.github.io/seanet/audiopalm/examples/) did text + audio to text + audio in one LLM,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Check out ImageBind. It's doing some multi-modal generation stuff,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Llava-interactive does this with images, however it can’t do it with audio too.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Don't know/won't know.  Since gpt4, OpenAI has stopped releasing technical details of any kind.  Supposedly for safety reasons, but they just don't want to lose their lead.  Which is fine. Companies having trade secrets is normal.  Except they have the holier than thou attitude which rubs people the wrong way.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Parameter count is not the only way to make models better, in the past 12 months alone a lot of advancements are being made even in open source that allow much better models while being trained with same parameter count, and closed source companies likely have internal advancements further on top of this that improves how much capabilities they can get while keeping parameter count the same.

The fact that this is a fully end to end multi-modal model likely also helps as this allows the model to understand information about the world from more than just text, this is all a single model trained seemingly on video, images, audio and text end to end all in the same network.

Even if you do decide to scale up compute, parameter count is far from the only method of doing so. There is ways of increasing the amount of compute that each parameter does during training by using extra forward passes per token, as well as increasing dataset size and other methods. And just because you scale training compute doesn’t mean it requires more compute at inference time either, methods like increasing training time or training dataset size for example are methods that keep the inference compute completely the same at the end while resulting in better models.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Faster inference and cheaper usage costs seems to indicate a smaller model (it might be smaller as in fewer transformers or something). If it got faster due to newer hardware, presumably the cost wouldn't go down due to the cost of the hardware, unless they're running this at a loss to capture the market / outcompete competitors.

IMO there's tons of areas for potential improvement in current ML techniques, especially if you included more human programming to do things we already know how to do efficiently, rather than trying to brute force it.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","It wouldn't surprise me if they went for a set of specialized models in a Mixture of Experts (MoE) setup. It makes sense, they had a lot of data when they trained GPT 3 and 4, but they've gained one very important dataset: how people interact with LLMs. That additional value could be utilized best, I believe, in a MoE architecture, because neural nets would be able find a setup that is most efficient at splitting up the different type of tasks LLMs are used for. It's also been a trend with open-source models lately.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","They probably used a smaller, more spare model and trained it for longer on a bigger dataset.

Don't forget that gpt-4 was trained in 2022 which means they trained it using A100 and V100.
Now they have a lot of H100 and a buch of AMD MI300 so they can scale even more.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",It was slow before because they used multiple models for speech to text and text to speech and thought inference .  For 4o they trained a single model to do all of it.  Less tokens because everything is “passed around” less.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",It's lighting fast. Slightly better at reasoning in general. But a much better coder than GPT-4Turbo.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",They said it will be rolled out over the next couple of weeks. I'm a paid subscriber and I have access to GPT-4o but not the multimodal part.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","You simply have the model create an understanding of audio through the same next token prediction process that we do with text, you simply take a chunk of audio, cut off the end, then have the model attempt to predict how the next segment of audio would sound like, then you adjust the weights of the model based on how close it was to the actual real ending of the audio, and you continue this auto-regressively for the next instance of audio and another etc, over time this process allows it to gain an understanding of both how to input and output audio and even do things like different types of voices, or even generate audio that’s not even voices at all such as generating music or coin effects for video games or signing, it can do all of this from essentially just being trained on next token prediction for audio, constantly predicting what the next instantaneous moment of audio should sound like.

As long as you include as many diverse source of audio as possible, you can have it gain an understanding of them by just predicting what the next instance of audio sounds like.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","emotions are encoded in labeling of training data, same for speed of speech. That's achievable already in some TTS models. They have advantage of scale and a lot of $$$ for the best training data and labeling.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","The same way they made GPT-4 able to do translation, summarization, sentiment analysis, base64 decoding, and a million other tasks: they didn't. They just trained it end-to-end on a dataset that has those things in it. Voilà!",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Usual text2audio models don't understand the context as well as chatgpt.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","All you really need is the audio samples to go with the text. All those audiobooks out there are filled with the data needed to decode emotional content, change tone, etc.

Speed change seems like it could be a fairly simple set of adjustable parameters that could be tuned through RLHF.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I think they bought in the speech generation tech. Probably from some firm which aims to supply Hollywood with actors who perform on demand, don't strike and can't feed the courts.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",[https://twitter.com/LiamFedus/status/1790064963966370209](https://twitter.com/LiamFedus/status/1790064963966370209),misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","It’s not available right now for free tier, might take them a few months. It is on the sub now",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",earlier access and 5 times higher rate limit.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I'm so salty that they made it worse over weekend recently! 

Past few weeks it was pretty fun, I could get it to predict what's in images or links despite it claiming to not being able to open images or access the internet 

Today it couldn't and I am disappointed",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","It's still unbeleivably far away, as this is a superficial model. Any real quality of life/work improvement is lacking. Anything annoying, cumbersome, and fiddly is still impossible for AI, and it is where it would have the greatest impact. Software is becoming more deficient in quality as the years go by, and options and settings are hidden behind layers of obfuscated panels/windows, and functionality is being removed. Integration of personal daily-use software and data is still unreachable with AI.

Ironically, the human job of writing the halmark cards in Her has been acheivable for years, but general maintenence and administrative work everyone needs to do on their phone and computer is not even close to being achieved by AI.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","OpenAI’s description of the model is:

>With GPT-4o, we trained a single new model end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network. Because GPT-4o is our first model combining all of these modalities, we are still just scratching the surface of exploring what the model can do and its limitations.

That doesn’t sound like an iterative update that tapes and glues together stuff in a nice wrapper / gui.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Huge ELO gain if you believe this [post](https://twitter.com/LiamFedus/status/1790064963966370209) has no issues.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","To me this reads they got a handle on managing infrastructure, optimizations and product roadmaps, for which I was afraid they were bogged down by.


The speed at which the assistant responds is truly impressive. And making it free for all signals they are pretty confident it holds up 


Now all is ready to focus on getting GPT5 dressed up.  Imagine theyd try to release that (which is likely much more resource hungry) on much less singing infrastructure. User experience matters hugely. Everyone would burn it down.


Yeah I'd focus putting the horse in front of the car first too.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","According to the blog post, they’ve made major improvements to audio and image modalities. It was trained end-to-end on all three types of data, instead of stapling an image encoder to an LLM like GPT-4V did.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Why do you think that? Have you seen any data supporting your claim? What an odd comment to see at the top of a MachineLearning post.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","The natural language rendition of a GPT prompt was super awkward, the AI was clearly not engaged in the conversation and was ready to blurt entire paragraphs of drivel unless interrupted.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",You can try [https://chat.lmsys.org/](https://chat.lmsys.org/) it has been there and still is. Now under the real name,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",More like when and what would be the usage limit. Sometime in the future,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",I see it in ChatGPT right now.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",it's already in chatgpt and openai api,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",And creating many more!,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","This is a single model that is able to understand image, video, audio and text all with a single neural network, this is a big advancement in the backend, not just a GUI connecting multiple seperate models.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I's cheaper now. It means you can spam it with requests and combine the results. It also has a larger contex window (very important, you don't need to finetune it, just provide context).

Soon will come the day when we can infere on our phones.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","> If it was something like speculative decoding, quantization, or hardware improvements, you'd think that they'd go back and apply it to the older models to save on serving costs.

Not if it would affect model outputs and they made a commitment to users (especially of API) that they would have a certain lifetime.

I’ve found it useful to go back to models in a specific release window to verify certain things.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",[deleted],misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",what makes you think gpt40 isnt just quantized gpt4?,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Speculative decoding would actually reduce the throughout since it requires more compute.
It only helps with reducing latency when you are memory bound.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","They did get the first production H200

https://venturebeat.com/ai/nvidia-ceo-jensen-huang-personally-delivers-first-dgx-h200-to-openai/",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Didn't they use those GPUs mainly for training? So this optimization wouldn't directly be reflected at inference?,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",batching doesn't make it faster since they've done it since day one,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Do you have any specific ones in mind?,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",The good old Perceiver IO,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",NextGPT was there,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","[https://codi-gen.github.io/](https://codi-gen.github.io/) is multimodal text/image/audio in and out, although I don't understand how it works even with the pictures.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",I think the GPT-4 paper made clear it was for both reasons.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Their name is oPeNaI and they claim to be a non-profit organization that wants to accelerate AI research and progress.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Doesn’t feel much better at code tbh,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","But I think they are not using TTS here...? They talk about multimodal tokens, but idk how do you make a probability distribution for every ""audio sample"" when you don't have a fixed vocabulary",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","That’s only the case for text to speech, for voice to voice models you don’t need any text labels at all with the voice, you just predict the next sequence of audio autoregressively in pretraining and you have tokens that represent highly detailed audio information instead of text tokens, and you just do next token audio prediction on any audio.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Isn't the model end-to-end multimodal though? Hence the astonishingly low latency for voice outputs. You can even hear some audible glitches/hallucinations in the audio output.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","it’s all one model, the GPT-4o model itself is what is generating the audio directly.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",That big of a jump? Pretty impressive,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","A friend of mine said it was available on iPhone already. He tried it out by talking to ChatGPT.  
EDIT: Ah yeah, it's only on the iPhone, but in the browser you still only have access to 3.5 I see",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I have it, but I mean, if ChatGPT4 is free it's kind of a waste. but it's not available. I was just curious if I should cancel my sub when my friend talked about the OpenAI video, since he said he could use it. At that moment I was saying ""okay than why am I paying?"" but it's clear now",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Hmm, are you so sure ? Talking about phones, if the deal between OpenAI and Apple goes through, I can imagine Apple giving the ability to developers to make tools, shortcuts and actions from their app directly accessible to an API that the model could use. 
The environment would be adapted for the model and I guess the model would also be finetuned to use the tools, docs provided by the developers but also the internal APIs of the iPhone. 
That doesn’t seem « unbelievably far away », at least for having access to the internal APIs of iOS. This opens up A LOT of use-cases, since we can do almost anything with a smartphone.
Being so assertive and confident about limitations in this time of rapid progress is not a good idea!",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","it’s a new tokenizer too, even if it’s a “gpt4” model it still has to be pretrained separately - so likely a fully new model with some architectural differences to accommodate new modalities",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I was not referencing architecture. There isn't much benefit to having a single network process multimodal data vs separate ones joined at a common head if it does not provide benefits in tasks that require multimodal inputs and outputs. With all the production of the release they are yet to show benefit on anything audiovisual other than Audio ASR. I'm firmly in the ""wait for more info"" camp. Again, there is a reason this is GPT-4x and not GPT-5. They know it doesn't warrant v5 yet.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",How is that elo measured?,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I don't know if I trust that though, can't people specifically compare it with others and just rate it higher due to bias? Or once they see that the output came from that model, just rerun the pairing with a new prompt and rank it higher too? I would wonder if its rating slowly goes down over time",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Not on Twitter so did not see that. I guess they are highlighting the UX/UI components on the main page. The ELO gain is impressive if as you said no issues. But overall across all performance metrics, nothing to brag about it seems. This is the reason they are not calling this GPT-5.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Even with multimodal end-to-end training with text/audio/image/video instead of encoded multimodal input to LLM like GPT4V, where are the gains?

  
[https://github.com/openai/simple-evals?tab=readme-ov-file#benchmark-results](https://github.com/openai/simple-evals?tab=readme-ov-file#benchmark-results)

I am seeing marginal gains in MMLU, GPQA, Math Human Eval vs Claude-3 or GPT-4 Turbo and underperformance in MGSM and DROP.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","[https://github.com/openai/simple-evals?tab=readme-ov-file#benchmark-results](https://github.com/openai/simple-evals?tab=readme-ov-file#benchmark-results)

Or you can read the last comment above",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I don't, so it's not freely available for everyone as OpenAI seem to be falsely claiming.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",I have an account on chatgpt and have no access to it.  Still 3.5 or can switch to the pay model for GPT 4.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Not for everyone, only some people have access through ChatGPT.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","the trouble is that the scientific  leaps are amazing, the branding an UI is nice, but the real world application in many cases is not good enough. Good enough in terms of: scalability, cost, reliability of output, interoperability with internal software.

I'm fully aware that this is where we're heading. But as OP mentioned, it currently disappoints once you go beyond primitive tasks. The issue being that consultancies and OpenAI oversell and overpromise currently achievable productivity and teansformative gains of AI.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","That's a good point. Decoding schemes and hardware optimization should give identical outputs, or at least within a reasonable margin of error. Maybe they don't even want to mess with that.

Quantization would degrade quality, but I wouldn't be surprised if all of the models were already quantized. Seems like an easy lever to pull to reduce serving costs at minimal quality expense, especially at 8 bit.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","They don't make any such guarantees. They have a beta feature where they allow you to set a PRNG seed parameter for deterministic completions, but they say that you'll only be able to expect the same results for a given ""system fingerprint"" which is just an opaque key they return as part of their response. It's not a settable parameter, it's just them doing you the kindness of telling you your prior results are no longer reproducible. System fingerprints don't appear to have any guaranteed lifetime. They might change multiple times per day for all I know, and there may even be more than one active at any given time.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","And they're closely linked to Microsoft. I really wonder if this is something like an 8x14B MoE, with the base model stemming from the Phi family research.

That being said, the WhatsApp version of llama 70b generates at a similar speed. They're using tricks of their own, but the real secret sauce may just be H100s.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Because why would OpenAI spend over a year quantizing GPT4 if the results were this good? Quantization is fast and cheap to apply.

The outputs are similar because they use the same fine tuning datasets and methods, so the models will converge to a similar point.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",it seems to have this capability https://arxiv.org/abs/1608.01281,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Completely different tokenizer, multimodal input and output and heavy focus on multilingual capabilities.
It's a completely different model from all the previous gpt-4s",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Dola contrastive decoding, AnyMal, LayerSkip, H-JEPA, Rho-1, Megaladon, MixtureOfAttention, V-Jepa, Codefusion, Phi-3, Better and faster language models paper by Meta, llava-interactive, MiniCPM, Jamba, Medusa-V2, Megabyte, IWM Jepa.

That’s just scratching the surface of potential directions of innovation known in the open source, over half of which have already been successfully applied and working on some commercially usable scale.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Was doing survey of video frame interpretation when Perceiver IO came out. It was at the top of optical flow estimation despite being general, which was really surprising for me at the time.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Please don't call a paper. It's a technical report at best.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","That doesn't mean they didn't synthetically train the voice generator with the help of an external voice generator. In fact if they were smart, they would have trained the parameters for a voice plugin/adapter layer and thereby have switchable voice personas.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I’m pretty confident most of it is from twitter hype. It should go down eventually. In practice I’d say it’s probably slightly better than GPT-4 Turbo, sometimes worse. Same model, more modalities.🤷",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I am almost certain. Only superficial APIs will be exposed, and the AI will need to depend on the API to be exposed to get any work done. It will be very simple things like move a calendar appointment with your voice. What is still well beyond the horizon is the AI interacting with your phone without the holy-sanction of the corporations bestowing their limited APIs for our use via AI.

We don't even need AI for proof of this, our access to user-facing APIs has gotten much worse over the last few decades. Try writing a plugin for the YouTube app on Android. There's a reason vanced exists, and the promise of somthing like an android YouTube API for improving user experience is not only nowhere to be found it is deliberatly withheld.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Agree. But as of now the main benefit seems to be speed not big gains in SOTA performance on benchmarks.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Expanding the modalities that a single NN can be trained on from end to end is going to have significant implications, if the scaling up of text only models has shown us anything. 

If there was a doubt that the neural networks we've seen up to now can serve as the basis for agents that contains an internal ""world model"" or ""understanding,"" then true end-to-end multimodality is exactly what is needed to move to the next step in intelligence. 

Sure, GPT-4o is not 10x smarter than GPT-4 Turbo. But for what it lacks in vertical intelligence gains, it's clearly showing impressive properties in **horizontal** gains -- reasoning across modalities rather than being highly intelligent in one modality only. 

I think what strikes me about the new model is that it shows us that true end-to-end multi-modality is possible -- and if pursued seriously, the final product on the other side looks and operate far more elegantly",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Most of the demos show the model engaging in conversation which is something other models can do. For example, other systems cannot react to being interrupted. If you look at the generated images, the accuracy is superior to current image generation models such as DALL-E 3, especially with text. There's also video understanding, so it's demonstrating a lot of novel capabilities",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I'd love for one of the downvoters to explain in intuitive or math terms why transfer function F that takes multimodal inputs as F(text,audio,video) into a ""single neural network"" is superior to transfer function G that takes as inputs the output of transfer functions (different neural networks converging at a common head) of multimodal inputs as G(h(text),j(audio),k(video)) IF it is not shown that F is a better transfer function than G. That is the point I was making. We are yet to be shown by OpenAI that F is better than G. If they have it then please show it!",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",[https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard),misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Rating is based only on blind votes.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Yah, I would bet against the ELO gain being this high. 100+ in coding is implausible from my own testing -- coding doesn't even have much of a spread since so much of the models tie.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",The last sentence hits so hard as a lay-person to ML.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Aren’t those all text-only benchmarks? They don’t take images or audio as input and so aren’t testing multimodal performance.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",This link shows it absolutely dominating GPT4-v. I don’t understand.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I wouldn’t dismiss it so easily if I were you, do you have evidence that it disappoints as much as other models when you go beyond primitive tasks? Or are you assuming that’s the case since that’s been the trend with recent models?

This model seems to prove to be much much better when it comes to unique out of distribution tasks that require complex interactions like real world scenarios that it wasn’t trained on, for example this person has had GPT-4-turbo and Claude Opus attempt to play Pokémon red by interacting with buttons and reacting to the latest instance of events happening in the game, the coherence of Claude 3 Opus and GPT-4 breaks down quickly in this task even when a lot of prompt engineering is attempted, but GPT4o seems to handle it not only decently but actually great.  It properly interacts with the components and actions in the game and successfully even seeming to learn and remember the actions as it goes along, at the same time it’s way cheaper and better latency than claude 3 opus and turbo.

https://x.com/VictorTaelin/status/1790185366693024155",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I'm seeing a lot worse quality with real world usage, so probably a quant. Granted, day 1 release it could just be some bug",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","The seed feature is only  available for GPT4, IIRC. Can’t pull up docs. atm, And they have said that deprecated models will be available for certain time,  IIRC. It’s not about deterministic results. It’s about statistical research as well as easing burden on devs. (Adding new models in languages that are strongly typed in a way that is idiomatic isn’t as easy as it is in Python. Not a major issue, but I would rather not have to revisit it as much as possible.)",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I'm not sure what that has to do with anything. Transformers don't need the entire sequence to generate a next token... If you look at side-by-side outputs of gpt-4o and gpt-4, you'll see they give very similar results. I would not be surprised at all if 4o started with a quantized 4 and maybe some additional tuning for audio embeddings -- or is 4 + tuning + quant... No one knows, you can't say from the 'capabilities'. 4 was multi-modal as well, they just never really released the api for video.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Really impressive results in multitask learning for brain computer interface applications too.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","It's still an extremely useful, efficient and interesting model, very underrated. Especially in use cases where exact copying of input subsequences is not super important, but people tend to be hyperfixated on generative text models these days and forget to study some papers",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","There is no reason you would have to do that to have switchable voices, you can just ask the model to speak in a different voice, or even ask it to talk faster, or talk in a different tone, or even just speak in whale noises entirely instead of using a human voice at all, You can even just ask it to make sounds of a coin being collected in a video game.. Same way you can ask ChatGPT to write text in mandarin or to speak in a jamaican or even speak in non-english binary or C++ entirely etc, ChatGPT doesn’t need different adapters to so all those things and neither would audio, it doesn’t require multiple adapters since it has general understanding of the modalities.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","How does twitter Hype help im-also-a-good-gpt2-chatbot in LMSys Arena? I have not used it, but assume the model name is not shown when the rate is asked to compare the outputs to their promt from two models?",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","You don't need API, you only need to get access to frontend. We've seen how good is AI with large enough context window for interpreting code.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","This is the biggest capabilities leap in coding abilities and general capabilities than the original GPT-4, ELO scores for the model have been posted by OpenAI employees on twitter",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I think we are kind of beating the same drum here. As an applied AI researcher that does not work with LLMs, I review many non-foundational/non-LLM deep learning papers with multimodal input data. I have had zero doubt for a long time that integration of multi-modal inputs to have a common latent embedding is possible and boosts performance because many non-foundational papers have shown this. But the expectation is that this leads to vertical gains as you call them. I want OpenAI to show that the horizontal gains (being able to take multimodal inputs and yield multimodal outputs) leads to the vertical intelligence gains that you mention. I have zero doubt that we will get there. But from what OpenAI has released with sparse performance metric data, it does not seem that GPT-4o is it. Maybe they are waiting for the bigger bang with GPT-5.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","The problem is that LLMs have different style, so it is relatively easy to discern the families once you play with them awhile. (OpenAI uses Latex, llama always tells you that you've raised a great question, etc.), so that introduces some level of bias.

There's a risk that LMSys corrupted data by removing the experimental models from direct chat, but permitted them to still be in area (with follow-up). Encouraged gaming to ""find gpt-4"".",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",The only audiovisual benchmark I see noted in their blog post is an Audio ASR beat over Whisper-3. Don't you think they'd show/share more beats on multimodal benchmarks if they had them to show?,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I think the disagreement is that your dominating = my marginal improvement over Clause-3/GPT-4. I just need more info hence the ""On first glance"" disclaimer. As others have mentioned, the multimode input integration is impressive. I just want to see bigger improvements in text tasks and I want to see some actual audio/video benchmark metrics before accepting this as a big leap forward. My guess is they really hedged today in anticipation of all of the above being shown with GPT-5.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Why are you comparing to GPT4-v?  The latest release is GPT-4-turbo-2024-04-09.  

The gains of gpt-4o are on par with to smaller than GPT-4-turbo-2024-04-09 compared to gpt-4-0125.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","how is the pokemon case an example for large scale implementation, outside of clickfarms?

so far, every real world use case that i've been working on with my teams couldnt be implemented, while we're steadily getting closer, they didnt cross a qa threshold. but it totally depends on the industry.

for accessibility, any improvement on text2speech and speech2text is great and welcome. only, implementation costs to switch providers (from google to amazon to openai) every quarter are way too high. 
so we defined thresholds of significant quality improvement that need to be achieved. (as i'm working in the german market: self-detected pronounciation-switches between german and mixed-in english/foreign words is what we're waiting for) 

for customer care self-set ice, any improvement is also great, but hallucinations and prompt manipulations are terrible. so, there needs to be minimal risk.

in education & journalism use cases, every mistake and hallucination in summarization a problem.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","4 multimodal takes turns back and forth to consume the tokens whereas 4o is consuming a continuous stream and predicting when to respond in an online fashion. It’s not the same as just writing to a sequence and then just sampling the latest predictions imo. That is not something that you get by just additional finetuning- that’s probably a new component of architecture plus some new training tricks at the least, regardless if some weights were recycled or not from earlier models.

btw the paper has ilya as a coauthor and it explicitly mentions as usecases a naturally interruptible voice translator model",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","What people here don't understand is the complexity of the integration required is well beyond near future AI capabilities. It is a difficult-to-specify multi-modal multi-faceted planning task, for which we don't even know how to generate a dataset for training let alone figure out how to build an architecture to solve it.

To create an analogy, self driving cars looked so promising people would say soon we can put the AI into construction vehicles and automatically build skyscrapers and bridges. No, each individual thing needs to be separately trained for, you can't just train on a couple of excavators and think it can generalise to cranes.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",I've already seen several people on twitter saying coding performance is worse than April 2024 GPT-4,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Elo scores are public voted. The improvement is likely due to twitter hype and people voting randomly to access the model,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",I doubt people are doing this enough to mess up the rankings lol,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Lol, the next evolution in LLM benchmark fraud: train LLMs to recognize and classify the anonymous lmsys models, deploy bots to vote for your company's LLM",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",We are saying the exact same thing. I was comparing to turbo.,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","It allows way more capabilities beyond just click farms. interactions with digital interfaces is at the core of a majority of remote knowledge work tasks that exist in todays world.

Editing photos or video in photoshop or after effects, doing in-depth research from multiple sources of information, putting together presentations for comprehensive projects, doing collaborative coding and working with front-end design references, bug testing such interfaces. Helping shop for houses online based on a users preferences, reserving required flights and vehicle rentals through various websites when given a vacation iternerary, I could go on. Nearly every remote knowledge work job is heavily dependent on multi-step long horizon interface interaction which current models like Claude Opus and Gpt-4-turbo fail at, any significant increase of accuracy in such multi-step long horizon interface interaction can dramatically expand the amount of such use cases that are now possible.

Not saying it’s AGI that can generalize just as well as a human on every long horizon autonomous task, but that still doesn’t change the fact that it’s a significant jump.

If GPT-4 gets 3% accuracy on a specific relatively difficult interface interaction test and GPT-4o now gets 30% accuracy on that same test, that’s a massive leap that allows much more things to be possible in that in-between of the 3% and 30% gap of difficulty, but it can simultaneously be true that it’s still far from fully being able to be integrated universally and efficiently into most knowledge work jobs. I’d say GPT-4 can maybe efficiently and autonomously do around 1% of remote knowledge work, I’d say GPT-4o is atleast double or triple the amount of use cases, so around 2-3%. Still maybe far from what you desire though which might require the 10% or 30% or 50%+ mark.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I understand the paper has ilya on it, and I agree, they might be using a similar technique. But people publish a lot of papers, does not mean you use every technique in every product.

All I'm saying is it's totally possible to just tack an audio input head onto g4, train it on dialog, and it will likely learn to only output stuff when there is vocal input from the user. If you get a collision where they are both talking, you can use a million strategies to combine the tokens.

I'm 100% *not trying to say* I know what 4o is, and you totally could be right that they're using that they're using some additional head trained with policy gradient to determine when to output speech like they do in that paper (but note, there are no 'hidden states' in transformers, so it would have the be a modified version of the paper anyway)... I'm just trying to say none of us know how much of gpt4 they recycled, and again the outputs are like token for token similar.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Yeah yeah yeah, long context was impossible with transformers, real video quality not for 20 years due to temporal consistency, live voice talk with LLM technology impossible because of latency, we know how all that went",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Maybe it’s the people you get recommended tweets from, thousands of human votes on LMsys say quite the opposite",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","As a rule you should pay basically attention to any sort of impressions from people who aren't doing rigorous analysis. These systems are highly stochastic, hard to subjectively evaluate, and very prone to confirmation bias. Just statistically, people have \~zero ability to evaluate models similar in performance with a few queries, but are \*incredibly\* convinced that they can do so for some reason.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","but random voting would equalize the results, thus understate the improvement of the best model",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",LMSys is actually [sponsoring](https://www.kaggle.com/competitions/lmsys-chatbot-arena) that. :),misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web",Maybe. I've also seen people saying coding performance is better. Just saying the initial numbers are maybe/probably overestimated,misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Sure, I agree. Just saying we should be sceptical about the increase in performance. It is way faster though (which is not very important to me at least).",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","You’re right, my bad.

In practice though, GPT-4o doesn’t feel much better at all. Been playing for hours and it feels benchmark hacked for sure. Disappointed. Yay new modalities though",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I tried it on understanding of AI papers, even simple questions like “What is JEPA in AI” GPT-4-turbo and regular GPT-4 get that wrong a majority of the time or just completely hallucinate answers, GPT-4o correctly responds to the question with the correct meaning of the acronym nearly every time. Also the coding ELO jump from GPT-4-turbo to GPT-4o is pretty massive, nearly 100 point jump, that’s a strong sign that it’s actually doing better in objective tests with objectively correct answers, difficult to “hack” benchmarks in coding ELO especially since the questions are constantly changing with new coding libraries and such, and it can’t just be knowledge cut off since it actually has the same knowledge cut off as GPT-4-turbo",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","I mean, on most benchmarks other than ELO it performs very, very slightly better than GPT-4T. This actually just reduces my trust in lmsys, because GPT-4o still gets very, very basic production code just completely wrong. It’s still bad at math, coding, struggles on the same logic puzzles, and has the same awful writing style. It feels similar to GPT-4T

On twitter I have seen more people agreeing with my description than with yours.🤷

Also, I tested your question on GPT-3.5 and it gets it right too. I am still not enthused.",misc
[N] GPT-4o,"https://openai.com/index/hello-gpt-4o/

- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)
- multimodal 
- faster and freely available on the web","Secondly, those papers were definitely in the training data. My bet is GPT-4o just remembers better.",misc
What's your favorite paper at ICLR2024? [D],Way too much to keep in track..,"Here are papers that I have found interesting after discussing with the authors at the conference:

# Self-Supervised Learning:

* [Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How (\*\*Oral\*\*)](https://arxiv.org/pdf/2306.03828)
* [Poly-View contrastive learning](https://arxiv.org/pdf/2403.05490)
* [R-MAE: Regions Meet Masked Autoencoeders](https://arxiv.org/pdf/2306.05411)

# Vision Transformers

* [FeatUp: A Model-Agnostic Framework for features at Any Resolution](https://arxiv.org/pdf/2403.10516)
* [Win-Win: Training High-Resolution Vision Transformers from Two Windows](https://openreview.net/pdf?id=N23A4ybMJr)

# Data selection / curation:

* ""[What Data Benefits My Classifier?"" Enhancing Model Performance and Interpretability through Influence-Based Data Selection (\*\*Oral\*\*)](https://openreview.net/pdf?id=HE9eUQlAvo)

# Learning with noise:

* [Why is SAM robust to Label Noise](https://arxiv.org/pdf/2405.03676)
* [Early Stopping Against Label Noise Without Validation Data](https://openreview.net/pdf?id=CMzF2aOfqp)
* [Robust Classification via Regression for Learning with Noisy Labels](https://openreview.net/pdf?id=wfgZc3IMqo)

# Deep Learning for Tabular Data:

* [TabR: Tabular Deep Learning Meets Nearest Neighbors](https://arxiv.org/pdf/2307.14338)

# Transformers, Mixture of Experts:

* [Small-scale proxies for large-scale Transformer training instabilities (\*\*Oral\*\*)](https://arxiv.org/pdf/2309.14322)
* [Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/pdf/2309.17453)
* [From Sparse to Soft Mixtures of Experts](https://arxiv.org/pdf/2308.00951)

# Vision Language Model

* [Consistency-guided Prompt Learning for Vision-Language Models](https://arxiv.org/pdf/2306.01195)

# Multi-Tasks Learning:

* [ZipUt!: Merging Models from different tasks without training](https://arxiv.org/pdf/2305.03053)

# Bayesian Deep Learning:

* [Variational Bayesian Last Layers](https://openreview.net/pdf?id=Sx7BIiPzys)",general_qa
What's your favorite paper at ICLR2024? [D],Way too much to keep in track..,The test of time award winner - variational bayes auto encoder,general_qa
What's your favorite paper at ICLR2024? [D],Way too much to keep in track..,My student's!!!!,general_qa
What's your favorite paper at ICLR2024? [D],Way too much to keep in track..,"Here you can find the top 15 ICLR-2024 papers sorted by citations.

[https://www.paperdigest.org/2024/05/most-influential-iclr-papers-2024-05/](https://www.paperdigest.org/2024/05/most-influential-iclr-papers-2024-05/)",general_qa
What's your favorite paper at ICLR2024? [D],Way too much to keep in track..,[im more concerned that Wojciech and Ilya's paper didnt win the TOT award.](https://iclr.cc/virtual/2024/test-of-time/23478),general_qa
What's your favorite paper at ICLR2024? [D],Way too much to keep in track..,"This is the paper for the curious: ""[Auto-Encoding Variational Bayes](https://ar5iv.labs.arxiv.org/html/1312.6114)"".

The [runner up](https://blog.iclr.cc/2024/05/07/iclr-2024-test-of-time-award/) looks cool too: ""[Intriguing properties of neural networks](https://ar5iv.labs.arxiv.org/html/1312.6199)"".",general_qa
What's your favorite paper at ICLR2024? [D],Way too much to keep in track..,One of my favourite papers,general_qa
What's your favorite paper at ICLR2024? [D],Way too much to keep in track..,"this is a pretty old (yet amazing paper)

  
 there is something similar released this yeartoo?",general_qa
What's your favorite paper at ICLR2024? [D],Way too much to keep in track..,"Well, that's the whole point of ""test of time""... It is still cool and amazing many years after.

We'll see in some 5-7 years which one of this year's papers will stand the test of time",general_qa
[D] Have someone tried to implement KANs from scratch?,"Recently I have been hearing a lot about this new architecture (kolmogorov-Arnold Networks) that might bring a new revolution in Deep Learning domain. 

Since many years MLP was the only architecture that was being used to solve any problem using neural networks, thus announcement of this new architecture is definitely a break through. Though many times in the past, lot of people had tried to do so but unfortunately they were unsuccessful.

If you still don't know about it, you could take help of following resources 👇🏻

Here is the research paper:
https://arxiv.org/abs/2404.19756

And the explanation video of this paper:
https://youtu.be/-PFIkkwWdnM

And if you have tried to implement it or found some video implementing it from scratch. Consider tagging the link in the comments.",https://github.com/mintisan/awesome-kan,misc
[D] Have someone tried to implement KANs from scratch?,"Recently I have been hearing a lot about this new architecture (kolmogorov-Arnold Networks) that might bring a new revolution in Deep Learning domain. 

Since many years MLP was the only architecture that was being used to solve any problem using neural networks, thus announcement of this new architecture is definitely a break through. Though many times in the past, lot of people had tried to do so but unfortunately they were unsuccessful.

If you still don't know about it, you could take help of following resources 👇🏻

Here is the research paper:
https://arxiv.org/abs/2404.19756

And the explanation video of this paper:
https://youtu.be/-PFIkkwWdnM

And if you have tried to implement it or found some video implementing it from scratch. Consider tagging the link in the comments.",Monarch matrices are going to completely replace MLP. Lots of interesting papers at NeurIPS and ICML.,misc
[D] Have someone tried to implement KANs from scratch?,"Recently I have been hearing a lot about this new architecture (kolmogorov-Arnold Networks) that might bring a new revolution in Deep Learning domain. 

Since many years MLP was the only architecture that was being used to solve any problem using neural networks, thus announcement of this new architecture is definitely a break through. Though many times in the past, lot of people had tried to do so but unfortunately they were unsuccessful.

If you still don't know about it, you could take help of following resources 👇🏻

Here is the research paper:
https://arxiv.org/abs/2404.19756

And the explanation video of this paper:
https://youtu.be/-PFIkkwWdnM

And if you have tried to implement it or found some video implementing it from scratch. Consider tagging the link in the comments.",Haven't gotten a chance to read about this yet. Heard about it in a tds article in my digest a week ago,misc
[D] Have someone tried to implement KANs from scratch?,"Recently I have been hearing a lot about this new architecture (kolmogorov-Arnold Networks) that might bring a new revolution in Deep Learning domain. 

Since many years MLP was the only architecture that was being used to solve any problem using neural networks, thus announcement of this new architecture is definitely a break through. Though many times in the past, lot of people had tried to do so but unfortunately they were unsuccessful.

If you still don't know about it, you could take help of following resources 👇🏻

Here is the research paper:
https://arxiv.org/abs/2404.19756

And the explanation video of this paper:
https://youtu.be/-PFIkkwWdnM

And if you have tried to implement it or found some video implementing it from scratch. Consider tagging the link in the comments.",Looks like the low hanging fruit rat race has already started. Two (very small) arXiv papers on basis function replacements and one of the authors removed their GitHub page (the conspiracy minded theory is to make sure nobody scoops them).,misc
[D] Have someone tried to implement KANs from scratch?,"Recently I have been hearing a lot about this new architecture (kolmogorov-Arnold Networks) that might bring a new revolution in Deep Learning domain. 

Since many years MLP was the only architecture that was being used to solve any problem using neural networks, thus announcement of this new architecture is definitely a break through. Though many times in the past, lot of people had tried to do so but unfortunately they were unsuccessful.

If you still don't know about it, you could take help of following resources 👇🏻

Here is the research paper:
https://arxiv.org/abs/2404.19756

And the explanation video of this paper:
https://youtu.be/-PFIkkwWdnM

And if you have tried to implement it or found some video implementing it from scratch. Consider tagging the link in the comments.",What's the tldr here for monarch matrices. Does it actually replace the mlp or is it a more efficient way of doing mlp operations or something else?,misc
[D] Have someone tried to implement KANs from scratch?,"Recently I have been hearing a lot about this new architecture (kolmogorov-Arnold Networks) that might bring a new revolution in Deep Learning domain. 

Since many years MLP was the only architecture that was being used to solve any problem using neural networks, thus announcement of this new architecture is definitely a break through. Though many times in the past, lot of people had tried to do so but unfortunately they were unsuccessful.

If you still don't know about it, you could take help of following resources 👇🏻

Here is the research paper:
https://arxiv.org/abs/2404.19756

And the explanation video of this paper:
https://youtu.be/-PFIkkwWdnM

And if you have tried to implement it or found some video implementing it from scratch. Consider tagging the link in the comments.","It's a lot more efficient at doing MLP operations so you run much larger networks with fixed hardware. Monarch Mixer is the generalization of the Fast Fourier Transform.

[https://www.youtube.com/live/IS59IwGLvVs&t=303](https://www.youtube.com/live/IS59IwGLvVs&t=303)

[https://arxiv.org/pdf/2310.12109](https://arxiv.org/pdf/2310.12109) 

[https://openreview.net/forum?id=cB0BImqSS9](https://openreview.net/forum?id=cB0BImqSS9)",misc
[D] Optical Flow For Video Classification,Can anyone help me with the Optical flow for video classification. For eg. Human Activity Classification. I didnt find any tutorials from scratch for code all I just found were papers.,"Have you tried OpenCV? There is a tutorial from scratch for general purpose optical flow ([here](https://docs.opencv.org/4.x/d4/dee/tutorial_optical_flow.html)). 

That could be a start. Then you can  ask less broad question.",misc
[Discussion] event sequence ORDER prediction,"I seem to have stumbled upon a problem that i can't google my way out of.

**\[MY TRAINING DATA\]**  
I have a dataset of bunch of sequential events. each event has 30-40 attributes, including the timestamp the event occured.

user 1: Event 1 > Event 2 > Event 3  
user 2: Event 1 > Event 2 > Event 3 > Event 4 > Event 5  
user 3: Event 1  
....

**\[THE PROBLEM TO SOLVE\]**  
I have a dataset of events, but i do not know which events belongs to which users. those users are different than the training set users, but we are infering they behave the same.  
   
For each event X, I need to solve for X. I need to figure out in what order that event occured. is it event 1? event 2? event 3?  

if X > 1, then event X-1 is also present in the dataset, although i have no way of linking them.

**\[CURRENT APPROACH\]**  
my manager is pushing to use LSTMs or transformers. I don't have much experience with them, but after doing some research i don't think its the correct approach. in fact, my research doesnt seem to have anything on this problem. am i the only one in the world who has it? ideas welcome. thanks (:","I suppose it really depends on what the features you have are. But some ideas to consider.

1. Try and find latent correlation between time steps. Perhaps unsupervised methods can create categorical variables you can leverage.

2. You can try and build a LSTM or Transformer that can “untangle” your labeled dataset. You can use semi supervised methods and corruption to strengthen results.

3. Are the distribution of event types the same across labeled and unlabeled data? Perhaps you can categorize them and use backwards difference encodings to give some sense of x leads to y or requires z before ect…",misc
[Discussion] event sequence ORDER prediction,"I seem to have stumbled upon a problem that i can't google my way out of.

**\[MY TRAINING DATA\]**  
I have a dataset of bunch of sequential events. each event has 30-40 attributes, including the timestamp the event occured.

user 1: Event 1 > Event 2 > Event 3  
user 2: Event 1 > Event 2 > Event 3 > Event 4 > Event 5  
user 3: Event 1  
....

**\[THE PROBLEM TO SOLVE\]**  
I have a dataset of events, but i do not know which events belongs to which users. those users are different than the training set users, but we are infering they behave the same.  
   
For each event X, I need to solve for X. I need to figure out in what order that event occured. is it event 1? event 2? event 3?  

if X > 1, then event X-1 is also present in the dataset, although i have no way of linking them.

**\[CURRENT APPROACH\]**  
my manager is pushing to use LSTMs or transformers. I don't have much experience with them, but after doing some research i don't think its the correct approach. in fact, my research doesnt seem to have anything on this problem. am i the only one in the world who has it? ideas welcome. thanks (:","Not sure I totally follow -- is it ""given some attributes of an event, infer whether this event was the first, second, ... for a given person""?

Or do you have data for a handful of events and you want to sort the events in terms of order?",misc
[Discussion] event sequence ORDER prediction,"I seem to have stumbled upon a problem that i can't google my way out of.

**\[MY TRAINING DATA\]**  
I have a dataset of bunch of sequential events. each event has 30-40 attributes, including the timestamp the event occured.

user 1: Event 1 > Event 2 > Event 3  
user 2: Event 1 > Event 2 > Event 3 > Event 4 > Event 5  
user 3: Event 1  
....

**\[THE PROBLEM TO SOLVE\]**  
I have a dataset of events, but i do not know which events belongs to which users. those users are different than the training set users, but we are infering they behave the same.  
   
For each event X, I need to solve for X. I need to figure out in what order that event occured. is it event 1? event 2? event 3?  

if X > 1, then event X-1 is also present in the dataset, although i have no way of linking them.

**\[CURRENT APPROACH\]**  
my manager is pushing to use LSTMs or transformers. I don't have much experience with them, but after doing some research i don't think its the correct approach. in fact, my research doesnt seem to have anything on this problem. am i the only one in the world who has it? ideas welcome. thanks (:",I did a project to predict if a vehicle is in collision course using CNN-LSTM model( kind of image sequence analysis). See if its useful: https://github.com/perseus784/Vehicle_Collision_Prediction_Using_CNN-LSTMs,misc
[Discussion] event sequence ORDER prediction,"I seem to have stumbled upon a problem that i can't google my way out of.

**\[MY TRAINING DATA\]**  
I have a dataset of bunch of sequential events. each event has 30-40 attributes, including the timestamp the event occured.

user 1: Event 1 > Event 2 > Event 3  
user 2: Event 1 > Event 2 > Event 3 > Event 4 > Event 5  
user 3: Event 1  
....

**\[THE PROBLEM TO SOLVE\]**  
I have a dataset of events, but i do not know which events belongs to which users. those users are different than the training set users, but we are infering they behave the same.  
   
For each event X, I need to solve for X. I need to figure out in what order that event occured. is it event 1? event 2? event 3?  

if X > 1, then event X-1 is also present in the dataset, although i have no way of linking them.

**\[CURRENT APPROACH\]**  
my manager is pushing to use LSTMs or transformers. I don't have much experience with them, but after doing some research i don't think its the correct approach. in fact, my research doesnt seem to have anything on this problem. am i the only one in the world who has it? ideas welcome. thanks (:","I don't think you can do this in an unsupervised way. However, if you had a labelled dataset, this is basically learning to rank, where ""best"" event is the first one, and further ones are ""less preferable"".",misc
[Discussion] event sequence ORDER prediction,"I seem to have stumbled upon a problem that i can't google my way out of.

**\[MY TRAINING DATA\]**  
I have a dataset of bunch of sequential events. each event has 30-40 attributes, including the timestamp the event occured.

user 1: Event 1 > Event 2 > Event 3  
user 2: Event 1 > Event 2 > Event 3 > Event 4 > Event 5  
user 3: Event 1  
....

**\[THE PROBLEM TO SOLVE\]**  
I have a dataset of events, but i do not know which events belongs to which users. those users are different than the training set users, but we are infering they behave the same.  
   
For each event X, I need to solve for X. I need to figure out in what order that event occured. is it event 1? event 2? event 3?  

if X > 1, then event X-1 is also present in the dataset, although i have no way of linking them.

**\[CURRENT APPROACH\]**  
my manager is pushing to use LSTMs or transformers. I don't have much experience with them, but after doing some research i don't think its the correct approach. in fact, my research doesnt seem to have anything on this problem. am i the only one in the world who has it? ideas welcome. thanks (:",thank you (: ! could you elaborate a bit on #2? any specific method i should look at?,misc
[Discussion] event sequence ORDER prediction,"I seem to have stumbled upon a problem that i can't google my way out of.

**\[MY TRAINING DATA\]**  
I have a dataset of bunch of sequential events. each event has 30-40 attributes, including the timestamp the event occured.

user 1: Event 1 > Event 2 > Event 3  
user 2: Event 1 > Event 2 > Event 3 > Event 4 > Event 5  
user 3: Event 1  
....

**\[THE PROBLEM TO SOLVE\]**  
I have a dataset of events, but i do not know which events belongs to which users. those users are different than the training set users, but we are infering they behave the same.  
   
For each event X, I need to solve for X. I need to figure out in what order that event occured. is it event 1? event 2? event 3?  

if X > 1, then event X-1 is also present in the dataset, although i have no way of linking them.

**\[CURRENT APPROACH\]**  
my manager is pushing to use LSTMs or transformers. I don't have much experience with them, but after doing some research i don't think its the correct approach. in fact, my research doesnt seem to have anything on this problem. am i the only one in the world who has it? ideas welcome. thanks (:",the first one is correct.,misc
[Discussion] event sequence ORDER prediction,"I seem to have stumbled upon a problem that i can't google my way out of.

**\[MY TRAINING DATA\]**  
I have a dataset of bunch of sequential events. each event has 30-40 attributes, including the timestamp the event occured.

user 1: Event 1 > Event 2 > Event 3  
user 2: Event 1 > Event 2 > Event 3 > Event 4 > Event 5  
user 3: Event 1  
....

**\[THE PROBLEM TO SOLVE\]**  
I have a dataset of events, but i do not know which events belongs to which users. those users are different than the training set users, but we are infering they behave the same.  
   
For each event X, I need to solve for X. I need to figure out in what order that event occured. is it event 1? event 2? event 3?  

if X > 1, then event X-1 is also present in the dataset, although i have no way of linking them.

**\[CURRENT APPROACH\]**  
my manager is pushing to use LSTMs or transformers. I don't have much experience with them, but after doing some research i don't think its the correct approach. in fact, my research doesnt seem to have anything on this problem. am i the only one in the world who has it? ideas welcome. thanks (:","I’m thinking something similar to the fill in blanks / correct the word trainings done on BERT and other encoders. So giving the model your attributes and events, but maybe flipping 2, and interjecting noise. Something to where you can get the model to try place events in order.",misc
[Discussion] event sequence ORDER prediction,"I seem to have stumbled upon a problem that i can't google my way out of.

**\[MY TRAINING DATA\]**  
I have a dataset of bunch of sequential events. each event has 30-40 attributes, including the timestamp the event occured.

user 1: Event 1 > Event 2 > Event 3  
user 2: Event 1 > Event 2 > Event 3 > Event 4 > Event 5  
user 3: Event 1  
....

**\[THE PROBLEM TO SOLVE\]**  
I have a dataset of events, but i do not know which events belongs to which users. those users are different than the training set users, but we are infering they behave the same.  
   
For each event X, I need to solve for X. I need to figure out in what order that event occured. is it event 1? event 2? event 3?  

if X > 1, then event X-1 is also present in the dataset, although i have no way of linking them.

**\[CURRENT APPROACH\]**  
my manager is pushing to use LSTMs or transformers. I don't have much experience with them, but after doing some research i don't think its the correct approach. in fact, my research doesnt seem to have anything on this problem. am i the only one in the world who has it? ideas welcome. thanks (:","Definitely sounds like a sequence model like a transformer or an LSTM is inappropriate then -- you aren't working with sequences! (At least not at inference time)

Another clarifying question. At training time, you don't have access to the entire sequence of events for a person? Just a number for each event like ""this was fourth""?",misc
[Discussion] event sequence ORDER prediction,"I seem to have stumbled upon a problem that i can't google my way out of.

**\[MY TRAINING DATA\]**  
I have a dataset of bunch of sequential events. each event has 30-40 attributes, including the timestamp the event occured.

user 1: Event 1 > Event 2 > Event 3  
user 2: Event 1 > Event 2 > Event 3 > Event 4 > Event 5  
user 3: Event 1  
....

**\[THE PROBLEM TO SOLVE\]**  
I have a dataset of events, but i do not know which events belongs to which users. those users are different than the training set users, but we are infering they behave the same.  
   
For each event X, I need to solve for X. I need to figure out in what order that event occured. is it event 1? event 2? event 3?  

if X > 1, then event X-1 is also present in the dataset, although i have no way of linking them.

**\[CURRENT APPROACH\]**  
my manager is pushing to use LSTMs or transformers. I don't have much experience with them, but after doing some research i don't think its the correct approach. in fact, my research doesnt seem to have anything on this problem. am i the only one in the world who has it? ideas welcome. thanks (:","i do have the entire sequence of events up until the time of training.  
  
another clarifying point is the people in the training set are different than those the model is gonna need to label (we are infering that they behave similarily, tho)",misc
[R] How Well Can Transformers Emulate In-context Newton's Method?,"**Paper**: [https://arxiv.org/abs/2403.03183](https://arxiv.org/abs/2403.03183)

**Code**: [https://anonymous.4open.science/r/transformer\_higher\_order-B80B/](https://anonymous.4open.science/r/transformer_higher_order-B80B/)

**Abstract**:

>Transformer-based models have demonstrated remarkable in-context learning capabilities, prompting extensive research into its underlying mechanisms. Recent studies have suggested that Transformers can implement first-order optimization algorithms for in-context learning and even second order ones for the case of linear regression. In this work, we study whether Transformers can perform higher order optimization methods, beyond the case of linear regression. We establish that linear attention Transformers with ReLU layers can approximate second order optimization algorithms for the task of logistic regression and achieve ϵ error with only a logarithmic to the error more layers. As a by-product we demonstrate the ability of even linear attention-only Transformers in implementing a single step of Newton's iteration for matrix inversion with merely two layers. These results suggest the ability of the Transformer architecture to implement complex algorithms, beyond gradient descent.","Well this sounds like anyone can make linear regression by the linear regressions. So to say the result, any coefficient by transformer can be use to approximate the linear regressions with representations of the result. For example, ChatGPT can do the regression by prompt.",general_qa
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!","This sounds about right. There are a \*lot\* of papers on those major conferences nowadays. So, as another disadvantage, you can imagine who is often left to review those things...",misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!","Is the abstract submission portal the same one as the main text? Do the checklist questions need to be answered at this stage, or can they be addressed anytime before the main submission? Thank you in advance! I’m a little confused and really appreciate the help",misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!","11:32PM EST, May14. Paper ID is nearly 14000. I'll register one more tomorrow, and it will be more than 15,000.",misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!",I've just submitted. Nearly 17000... crazy,misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!","And you may not be the last. I think the average submission is somewhere 8000. Check out ""ML paper acceptance rate"" on Github for historical data (I don't remember the exact name)",misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!",I just submitted mine yesterday and the paper ID was already halfway from 8000,misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!",How many submissions were there last year?,misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!",19k already!,misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!","Could someone nominate me as a reviewer or add my into the reviewer pool? I was a top reviewer for NeurIPS 2023 and have reviewed icml, iclr, aistats, aaai...... I am not sure why I was not invited this time.",misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!","I think it is the same, and there is an option to upload your PDF even though there is no asterisk next to it. Over the course of next week, you will ‘edit’ your submission to upload your paper. I answered the checklist questions on openreview with YES NO N/A. The PDF will have the justifications.",misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!","Damn, 12:07AM EST, May15. It's apparently over 14000.",misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!","if it's 7000 the day before submission date, it will probably be in the 10s of thousands by tomorrow",misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!","12343

(Source: https://github.com/tranhungnghiep/AI-Conference-Info)",misc
[D] Neurips 2024 submissions,"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it’s not the case! 7000 submissions already?!",Thank you!,misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?",I was waiting for this for so long but couldn't check the status because of the 2 day work vacation. Will have to live in anxiety now :|,misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","If you got a rebuttal rather than early reject, it's worth trying, I'd say.

MICCAI reviews are high variance, I'd interpret the decision as the meta-reviewer thinking that the reviewer's criticism isn't super convincing.",misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","I got a 3,3,5 for a novel application paper in generative modelling and have to rebut.
Most concerns are about lack of comparative experiments, but since the application and methodology is novel, there are no baselines to compare against.
What are the chances of acceptance after rebuttal given that we are not allowed to include new exp in the rebuttal?",misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?",It's worth trying. I have seen rebuttal increased the scores by 1-2 per reviewer in the previous MICCAI. The rate of acceptance in rebuttal phase might be 19/54 ~ 35% in my estimation.,misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","Got 2 papers in rebuttal with both identical scores of (3,4,5). A bit surprised that this is still not enought to get an early accept but god knows if it will be enough for after rebuttals. From my experience of other CS conferences average scores of a weak accept (>4.0) after rebuttal should get through.",misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","Got 2 weak accepts and 1 weak reject. Pretty insane that new experimental results can't be included in the revision process -- does adding a t-test count as ""new experimental results""? Oh well, if this doesn't work out I'll submit to a workshop.",misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?",Is this the first year that reviews and rebuttals will be published? I'd like to see some concrete examples of good rebuttals rather than just their tips on good rebuttals. I've written longer responses to reviewers before but the 4000 character limit seems pretty short.,misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","Finally got a look at the reviews. 

4, 4, 1, 3

I'm very new to this stuff and will let the main author deal with it. Kind of sad looking at the rejection reviews.",misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?",Sorry human!,misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","I am sorry but do you know what is percentage of rejection, or the rate of acceptance of rebuttal papers?",misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?",How did you come up with this 19/54 estimate?,misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?",Do you have any primary meta review?I have not seen that in the cmt3.,misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?",I wonder the same thing here also as one of my reviewer explicitly asks for comparison to two specific other methods,misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","As far as I know, they have published reviews for the accepted paper for previous MICCAI conferences as well. For instance, look at these reviews from 2023: https://conferences.miccai.org/2023/papers/",misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","Based on the statistics provided in their emails, here are my guess: 11% early accept, 54% rebuttal, the rest are early rejection. Around 30% will be accepted in total, so abit above a third of papers in rebuttal will get acceptance.",misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","The early accepted papers account for 11% of all submissions, and 54% enter rebuttal. These numbers come from their emails. There should be around 30% of papers getting accepted, similar to previous conferences, so they need 19% more.",misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?",No,misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?",Unfortunately I think adding a new row/column in a table is prohibited. Your best bet is to explain to the best of your ability why those comparisons weren't initialling in the manuscript.,misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","Perfect, thanks!",misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?",Thanks a lot for your feedback. That is really a bummer as the only weaknesses pointed out by this reviewer is that it lacks the comparison with these two methods. It will be hard to improve scores when reviewers asked for other experiments then,misc
[Discussion] MICCAI 2024 decisions,"Hi all,

I thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.

I got a rebuttal invitation for an application paper and all the reviewers mentioned ""lack of technical novelty"" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","So are you guys not going to address these demands? I'm in the same boat where a reviewer asks for an additional baseline and a statistical test. I can easily provide these and would definitely argue, that those do not substantially change my paper. But those rebuttal guidelines really are unsettling...",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","1. Those are only 5 datasets. For evaluating tabular classifiers, you should use tens of datasets, they are readily available. Also, describe evaluation procedure, e.g. use 5-fold CV for testing. See e.g. ""A novel selective naïve Bayes algorithm"" Chen et al., which use 65 datasets.
2. You must compare to XGBoost, LightGBM and CatBoost on large-scale datasets from their respective papers. Especially since scalability and speed is one of your selling points. If you aimed specifically at boosting for small data, then you don't need this, but it isn't stated anywhere.
3. One of major advantages of XGBoost, LightGBM and CatBoost is being able to use custom loss functions. This allowed them to be easily used e.g. for ranking. If you don't support this, you should explicitly state this limitation.
4. Number of estimators is just a hyperparameter, why show large tables with this? Just present the best result for each dataset.
5. Your implementation doesn't support class weights, as far as I can tell. This is a huge limitation, since almost all datasets are imbalanced, often heavily.
6. You must not embed scalers inside your code. You can destroy data sparsity, affect categorical variables, and do other stuff outside user's control this way. Add checks and throw exceptions if you absolutely require this.
7. You only support numerical data, in constrast to LightGBM or CatBoost. You should highlight this limitation.
8. This works only for classification, not even regression. This is, again, a huge limitation, but probably can be fixed, as far as I can tell.

EDIT:

9. You also don't handle missing values, which is pretty nicely handled in XGBoost, LightGBM and CatBoost, so that they can be actively used to select the split point.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",How does this approach differ to simply using linear models within XGBoost. XGBoost does support this [as well](https://xgboost.readthedocs.io/en/stable/parameter.html).,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Are you tuning the other algorithms hyper-parameters or just using defaults?

It would be interesting if you can include a larger dataset, for example from a Kaggle competition where Xgboost was good and compare it to your method.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Looking at the code I see something strange: during predict you use the minmax scaling on the predict features  (which might have a different range than the features on the  training data). If your predict dataset just added a single data point to your training data it could potentially throw everything off. Instead you might want to ""freeze"" the scaling function based on the training data. 

And it seems that you are using adaboost with a potentially strong learner (SERF) correct? The Wikipedia entry on adaboost references a paper on this topic you might want to see.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","What does the name SEFR stand for?

I looked at the ""SEFR: A Fast Linear-Time Classifier for Ultra-Low Power Devices"" paper from 2020 by the same authors and it seems this is just a straightforward linear classifier (i.e. linear regression with a threshold)? What is novel about this? It seems basically the same formulation you find in Wikipedia's ""linear classifier"" article.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",How fast is it for training compared to XGBoost? Is it viable to train it with LOOCV?,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","So, maybe this is a naive question, but what does it even mean to do boosting on a linear model? Doesn't ""linear boosting"" just produce another linear model that has the exact same structure as the original?

A boosting-like procedure is frequently used in the iterative solution of large, sparse linear systems of equations; is that the kind of thing you're doing? If so then it's probably inaccurate to describe it as ""boosting"", because those are well-known methods that go by other names.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Linear from SERF stands for linear time complexity but not being a linear model, right? Personally, I find the naming confusing and thought it might be something like boosting linear models (of which one should note that a straightforward ensemble of linear models is still a linear model)",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",I would like to see this in scikit-learn. Are you planning to add it?,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Sorry for my naive question, doesn’t standard boosting (like AdaBoost) work with any base classifier already?",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",Let me know if you want to partner on establishing the performance on some typical enterprise datasets!,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",Just check on kaggle,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Damn, peer reviewed in the reddit comments.

Honestly though it's pretty cool of you to go through it diligently and add these points. I'd be very happy if I was the researcher.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Thanks for your points! First of all, I should point out that I am an independent researcher, and I am not affiliated with any institutes, so this is my side project.

1. You are right. In the paper that will be available soon, the number of datasets will be much higher. Also, we have used 10-fold CV. I added this to the README file.
2. The large-scale datasets will also be included.
3. This will be supported in future. I added this to the README file.
4. I want to show that our algorithm reaches best results sooner than others.
5. Thanks for pointing this out! This will be added soon. Added to README.
6. The SEFR algorithm requires the feature values to be positive. This is the reason of scaling. But I will implement a better mechanism. Added to README.
7. We have highlighted this in the documentation.
8. Yes, it is in future plans. 

Once again, thanks for your helpful and insightful comments!",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",This is a high quality peer review,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","How is it only being applicable to classification a weakness? It's a classification method, not a regression one, right? Granted, the two problems are closely related, but they're not the same thing. You wouldn't say a change point detection method has a weakness that it can't be applied to forecasting, for example.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","The categorical feature handling in lightgbm is just label encoding? I mean how hard is to target encode or one hot encode on your own ? 

Also, isn’t that the idea behind gbm - you take a bunch of weak learners and use the ensemble for prediction. You can replace the decision tree stump with a simple shallow neural network as well.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",Wait what since when did xgboost handle nan values i moved to sklearn due to that,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Thanks for pointing it out. Yes, XGBoost supports this but our approach is different, since the linear classifier that is being used is SEFR which has different characteristics. Also, ADABoost is used here.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","I use the defaults for all of the algorithms (the one proposed and the ones referenced). On the larger datasets, thanks for your suggestion! We are planning to have it.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",That MinMax scaling is certainly one of our limitations. This is because SEFR cannot accept negative values. But we are working on that. Thanks for your suggestion of the Wikipedia entry!,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","SEFR stands for Scalable, Efficient, Fast ClassifieR. Yes, it is a straightforward classifier, but in that algorithm, the goal was to get a decent accuracy with the lowest possible computation time and memory footprint. That algorithm can be trained even on cheapest microcontrollers (you can search it on YouTube to see videos of training on €4 microcontrollers), but its accuracy is higher than simple algorithms like Naive Bayes or Linear Regression, or even Decision Trees.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","FWIW if training speed is your concern, LightGBM was historically the best pick",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","The numbers are reported in the README of GitHub Repo. The SAMME version is very fast and it can be trained with LOOCV on many datasets. Also when the number of records/features are not too high, SAMME.R also can be used for LOOCV. For setting these parameters, please see here:  
[https://linearboost.readthedocs.io/en/latest/usage.html#parameters](https://linearboost.readthedocs.io/en/latest/usage.html#parameters)",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","No, boosting a linear classifier will make it better at handling complex data patterns.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",They confirmed in another comment that it is a linear classifier but maybe because of thresholding it's possible to combine them without just getting another linear model?,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Actually SEFR is both linear, and linear-time.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Yes, this is in our plans!",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",Do you mean participating in competitions?,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","As the researcher, I should say that I am indeed very happy to get this high-quality peer review!",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Fair enough, those are reasonable answers. Showing that this tends to overfit less, works better for small datasets etc. would be pretty valuable. Good luck with this!",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Good point, but I see this as a weakness because this is a tabular learning method compared to boosting frameworks, which naturally lend themselves to regression problems, and also other ones (e.g. ranking) via loss functions. And they are powerful at regression, e.g. for time series forecasting, so I see not supporting regression as a quite major limitation. However, the general idea does seem to be able to support regression in the future, so this is more of a implementation downside rather than general approach one.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Reminds me of the drama behind the MAMBA paper's peer review process: 

https://youtu.be/N6Piou4oYx8?si=7o8jFOJcFslTjjOC&t=1664

>> Rejected by peer reviewers .... now this is a really dumb reason to reject a paper because the long range arena [the task the reviewer was complaining about] is a completely different task to language modeling, and Mamba is specifically a language model.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Except it isn't the same as label encoding. In fact, none of the three major boosting implementations use one-hot encoding style of handling categorical variables.

LightGBM uses partition split, which for regression trees can efficiently check the partition of the set into two maximum homogeneity subsets, see [the docs](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#categorical-feature-support) and the original paper: ""On Grouping for Maximum Homogeneity"" W. Fisher. XGBoost [also offers](https://xgboost.readthedocs.io/en/stable/tutorials/categorical.html#optimal-partitioning) partition split for categorical variables, with the same algorithm.

You could use one-hot encoding, but then to represent ""variable has value A or B, and not C"" you would have to use 2 or 3 splits, whereas with partition split you only use one.

CatBoost, on the other hand, uses [Ordered Target Encoding](https://github.com/catboost/catboost/blob/master/catboost/tutorials/categorical_features/categorical_features_parameters.ipynb) instead, described in the linked notebook. It can also combine them during learning, but I don't know the details.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Since... always, this was one of the main ideas in the original paper ""XGBoost: A Scalable Tree Boosting System"" T. Chen, C. Guestrin. It's called a ""default direction"" in the paper, and the whole Algorithm 3 there is meant to handle this. The idea is basically to have a split, but determine whether for missing values you should go to the left or right child. This is selected based on minimizing the loss function, and in a differentiable way.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",Using AdaBoost with a custom model is hardly novel. With the [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) implementation you can provide any type of classifier to use as a base estimator.,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","If you use defaults your claim of being ""better"" no longer holds, unless the default hyperparams are the optimal and that never happens. 

This is a very common problem in ML papers and sadly most comparison tables are invalid.

My recommendation is: tune am xgboost model to the optimal hyperparams and if you have better results then we have a discussion.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",Could you explain the intuition behind SEFR and your version? Why is it competitive with GBMs? The SEFR algo from the paper seems like it couldn't handle interactions between variables,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",">its accuracy is higher than simple algorithms like Naive Bayes or Linear Regression, or even Decision Trees

That's a super strong claim. I assume you mean based on your tests, for which you used the default parameters? Like another commenter said, you should be comparing on optimal hyperparameters across multiple datasets if you're going to make a claim like that. Even then, the No Free Lunch Theorem suggests otherwise if they're comparably computationally constrained.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",No longer the case. Xgboost introduced histogram tree method in v2.0 which is identical to lightgbm's,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","But it doesnt. By definition, the solution of the linear regression are the weights and intercept such that the error is minimal. That is an analytical solution and no other solution (of a linear model) can produce a smaller error.

Plus any linear combination of weights is still a linear combination. So if you do a linear regression, calculate the residuals, add a linear regression to it and substitute that one into your equation y=bx+c+resid = bx + c + (b2x +c2), it is still a linear regression with the new beta being b+b2 and the new intercept being c + c2 (which are both worse then the first linear solution since it is an analytically minimal solution)",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","How, though? Exactly what operation are you doing that you are calling ""linear boosting"", and how does it differ from just fitting the original model?

E.g. consider logistic regression with y=p(w^T x + c). If you're doing ""linear boosting"" to update the w and c vectors then that's not changing the model at all, and is maybe equivalent to changing the model fitting procedure. Whereas if you're doing boosting by doing something like y = p(w^T x + c) + a * p(w2^T x + c2) then that's just regular boosting, and the model is no longer linear.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",Thank you for the suggestions!,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","On top of the advantages you mentioned, I think the labels produced by partition splitting should also tend to be sparser than one hot encoded ones even when storing the one hot encoded labels in a sparse format.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/catboost/catboost/blob/master/catboost/tutorials/categorical_features/categorical_features_parameters.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/catboost/catboost/master?filepath=catboost%2Ftutorials%2Fcategorical_features%2Fcategorical_features_parameters.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","Certainly! This is the very first draft of our algorithm, and I will do comparisons based on the best selected hyperparameters.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","SEFR was originally designed to be extremely time and resource-efficient. Because of that, it has been implemented in numerous microcontroller applications. But apart from that, SEFR is also a good weak learner for boosting. It is a minimalistic building block, and by future improvements, it can handle interactions as well.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","We tested SEFR on numerous datasets with grid search on hyperparameters to find the optimal results of them. We reported some of them in the paper in arXiv, but it is consistently more accurate than the other simple algorithms.",misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",Thanks! So it finds interactions via boosting or was there a more fundamental change to SEFR to handle interactions?,misc
"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","Hi All!

We're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  
The key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.

We believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo: [https://github.com/LinearBoost/linearboost-classifier](https://github.com/LinearBoost/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.

We'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!",">it is consistently more accurate than the other simple algorithm

Do you have a hypothesis as to why that's true? Usually, resource constrained, parallel, approximate, or heuristic algorithms show the exact opposite behavior.",misc
[D] Language model for TimeSeries Forecasting from Amazon,"Time series forecasting is super important for many industries, like retail, energy, finance, etc. 

I delivered many projects in this area with statistical models, deep learning models (LSTM, CNN) and always it was a challenge. 

With a great development in language model space I was thinking how LLM architecture could be used for forecasting and while I was exploring this idea I found that Amazon already delivered multiple **pretrained time series forecasting models** based on language model architectures. 



If you are interesting check following resources: 

[https://github.com/amazon-science/chronos-forecasting](https://github.com/amazon-science/chronos-forecasting)

[https://www.amazon.science/blog/adapting-language-model-architectures-for-time-series-forecasting](https://www.amazon.science/blog/adapting-language-model-architectures-for-time-series-forecasting)



What do you think, will a such models make a forecasting more accurate?",I'm glad they are open sourcing them. Although I don't think people make the switch to these new models soon enough. I know people in industry still just using out the box Prophet.,misc
[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.","Monitoring whether a candidate switches tabs during an interview is not a productive practice. As a lead ML engineer, I rely heavily on multiple resources, including GitHub, StackOverflow, and YouTube, which is a common and essential part of our workflow. At any given moment, I might have dozens of tabs open to assist with various tasks, and I also frequently consult tools like ChatGPT. The key should be on performance and the quality of work delivered, not on how it's achieved. Try assessing a candidate’s ability to effectively solve problems and deliver solutions, rather than restricting and monitoring their method of finding information.",misc
[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.",Maybe try working on something that is not spying on people.,misc
[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.","Pretty much 100% of candidates are using AI to outperform on online automated screening and assessments in the hiring process.

These are the candidates you should be hiring right now, not attempting to screen out.  Clearly, they are better at using AI than you.",misc
[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.",The easiest thing to do here is probably to check if they are still on the interview test tab either by looking for something that's always visible on that tab or by training a CNN to classify whether the screen is that tab or something else if there isn't any persistent feature you can look for - happy to help if you wanna DM me,misc
[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.","This is a bit of a silly response as in interview settings we are assessing abilities via simple toy problems. A few minutes of ""planning"" on chat gpt can get you a general approach that is promising, obscuring most of what we're trying to infer in the first place.  I wouldn't care if someone used it for a longer take home (not that we give them), but for simple interview problems it's an issue.",misc
[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.",Its not spying if the user knows that the screen is being recorded throughtout the interview,misc
[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.","Simple toy problems will get you a simple toy engineer. Are you solving “simple interview problems” at work? I think not. I generally need a single 2 hour interview at most to determine if the candidate is a good fit. And I usually ask questions about applied data science, ml development, system design and tools, and offer some real-world cases for discussion, preferably those my team have had trouble with in the past or are dealing with at the moment. Because why else hire a new person? So that they could do the heavy lifting. It’s not a college entrance exam, it’s a job. I never ask to do any brain teasers, live coding or algorithms, yet so far I’ve been quite satisfied with the people I’ve hired over the years.",misc
[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.","So? If you give a simple problem and they have the tools to solve it to your satisfaction, what exactly are you complaining about? Give a harder problem, or be happy they can do the job.",misc
[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.",Do they have a choice?,misc
[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.","Yes, they can choose to not participate in the interview.",misc
[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.",Working is not optional because we all need to eat. If enough companies deploy systems like this participation becomes effectively mandatory.,misc
[D] Video analysis tools for detecting cheating in interviews,"Hi everyone,

I am looking for any video analysis tools that I can use for the following usecase:

I have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.

Thanks in advance for any inputs.","Working may not be optional for most people, but working for this company is.",misc
[P] Time series forecasting,"Time series Forecasting

Hi everyone I am trying first forecasting project. 

I have a time series over 1 year which is made by users check-ins everyday in a physical center located on a single country/nation.
I want to produce synthetic data  to do forecasting and simulations.

Now I would like to understand if I need to use ML  algorithm or just pick up uniformly random time and places. My understanding tells me that doing so I would lose any correlation between users-time-center location.

So I was naturally leaning towards ML.. which frameworks should I study for this?","Synthetic time series data tends to be pretty difficult to get right. Your model is more than likely just going to learn your synthetic data generation function rather than anything meaningful about the timeseries. The only way you can really do synthetic timeseries data well is if you have strong knowledge of the actual underlying process which generates the timeseries, such as in physical systems. In this case it doesn't seem that you have that knowledge so I doubt it will really improve your model. 

In any case you definitely will need to incorporate some randomness to prevent the model from just memorizing your generating function, but it is important that you apply this randomness in a way that makes sense for your domain. 

You could also try training a GAN to generate time series data. There is quite a bit of research out there on that",misc
[P] Time series forecasting,"Time series Forecasting

Hi everyone I am trying first forecasting project. 

I have a time series over 1 year which is made by users check-ins everyday in a physical center located on a single country/nation.
I want to produce synthetic data  to do forecasting and simulations.

Now I would like to understand if I need to use ML  algorithm or just pick up uniformly random time and places. My understanding tells me that doing so I would lose any correlation between users-time-center location.

So I was naturally leaning towards ML.. which frameworks should I study for this?","Thanks a lot this very insightful. I was messing around with Prophet and chronos forecssting but I am far to succed. I am aslo trying to use ARIMA.

I could also do a simple random pick (uniform)  on a fixed center location grid. But I dont’ know I feel like I am losong seasonality and correlation between the center location and the user location somehow.
The problem is how do the models sense and reproduce the fact that a User will come back more or less to the same center since the location is maybe closer to them. The problem is I do not have the info on where the User lives. I just see in the data that they come back for check in in the same center. 

I am not sure if I have to look elsewhere.
I will look into GANs yes.


What do you mean “incorporating randonmess”?",misc
[P] Time series forecasting,"Time series Forecasting

Hi everyone I am trying first forecasting project. 

I have a time series over 1 year which is made by users check-ins everyday in a physical center located on a single country/nation.
I want to produce synthetic data  to do forecasting and simulations.

Now I would like to understand if I need to use ML  algorithm or just pick up uniformly random time and places. My understanding tells me that doing so I would lose any correlation between users-time-center location.

So I was naturally leaning towards ML.. which frameworks should I study for this?","I basically mean what you are saying. You will want to generate your timeseries non deterministically by randomly generating check in events. In your case though it will be difficult because you don't have information about the underlying process that generates the timeseries. 

You would be able to do it if we were looking at the trajectory of a spaceship for example. You would know based on the ship specifications and laws of phyisics that if the ship is moving along some particular trajectory at some velocity, then chances are in the next step it will continue along that trajectory, or at the very least it can only physically deviate by so much from it's current position in a single timestep. This knowledge can be used to generate random trajectories that abide by the physical limitations and should be almost as good as the real thing.",misc
[P] Time series forecasting,"Time series Forecasting

Hi everyone I am trying first forecasting project. 

I have a time series over 1 year which is made by users check-ins everyday in a physical center located on a single country/nation.
I want to produce synthetic data  to do forecasting and simulations.

Now I would like to understand if I need to use ML  algorithm or just pick up uniformly random time and places. My understanding tells me that doing so I would lose any correlation between users-time-center location.

So I was naturally leaning towards ML.. which frameworks should I study for this?","Crystal clear.. then it means there is no chance in my case to generate good forecasted data?  

My understing is that the any forecasting approach I will use, will struggle .. I tried prophet and ARIMA, and with ARIMA for example I get a straight line, so it isn’t peaking anything like seasonality and center location  I believe.",misc
[P] Time series forecasting,"Time series Forecasting

Hi everyone I am trying first forecasting project. 

I have a time series over 1 year which is made by users check-ins everyday in a physical center located on a single country/nation.
I want to produce synthetic data  to do forecasting and simulations.

Now I would like to understand if I need to use ML  algorithm or just pick up uniformly random time and places. My understanding tells me that doing so I would lose any correlation between users-time-center location.

So I was naturally leaning towards ML.. which frameworks should I study for this?","Your only hope for synthetic data would probably be to train a GAN to do it for you. 

Not sure exactly what is going on with your forecasting though. Are your events sparsely distributed throughout time? If so I think you might get a better result from a tree based model",misc
[P] Time series forecasting,"Time series Forecasting

Hi everyone I am trying first forecasting project. 

I have a time series over 1 year which is made by users check-ins everyday in a physical center located on a single country/nation.
I want to produce synthetic data  to do forecasting and simulations.

Now I would like to understand if I need to use ML  algorithm or just pick up uniformly random time and places. My understanding tells me that doing so I would lose any correlation between users-time-center location.

So I was naturally leaning towards ML.. which frameworks should I study for this?","Basically it’s a time series , each rows is a check-in in one of the centers that provide the service for which the users subscribe and can use it freely. 
so a row of the dataframe is like

check_in_id, User_id , chceck_in_date_and_time, center_id, Center_city, center_latitude, center_longitude

Thats’it. Of course there is a seasonality and of course some centers get more check-ins and users usually check-in in the centers in the same city.
Would this be enough for GANs or tree model?

Now I do not have the user location/base. But I might retrive it. 

I am the only DA in my company (the first one just arrived) and the database is badly organized. So they first asked me a quick behavioural analysis with some csv they provided, then I will do some data base and data wearhousing.

And I have 1 year of data right now. Order of ~10^6 check ins.

I thought maybe using a bayesian forecasting approach would help, since I coud gauge seasonality and location with priors.",misc
[P] Time series forecasting,"Time series Forecasting

Hi everyone I am trying first forecasting project. 

I have a time series over 1 year which is made by users check-ins everyday in a physical center located on a single country/nation.
I want to produce synthetic data  to do forecasting and simulations.

Now I would like to understand if I need to use ML  algorithm or just pick up uniformly random time and places. My understanding tells me that doing so I would lose any correlation between users-time-center location.

So I was naturally leaning towards ML.. which frameworks should I study for this?","10^6 events should be enough to train on unless you have like hundreds of locations. I think your issue might be with the way you are framing the problem in your models. What does your input data look like and what exactly are you trying to to predict?

I think you should frame this as location specific. First sum the check-ins for each timestep at location A to get a nice equally spaced timeseries. You can pick your resolution according to your needs. Your training objective should then be the following: given the last n timesteps at location A, predict the number of check-ins for each of the next k time steps at location A. So you will have to train an individual model for each location assuming you do end up using gradient boosted trees. If you continue to use arima or a non ml method then you just need the one model, but you will need to forecast each location separately. 

I would make sure that the above makes sense before you go into doing any synthetic data generation. If you do want to use a GAN to generate the data you might also want to do that by location.",misc
[P] Time series forecasting,"Time series Forecasting

Hi everyone I am trying first forecasting project. 

I have a time series over 1 year which is made by users check-ins everyday in a physical center located on a single country/nation.
I want to produce synthetic data  to do forecasting and simulations.

Now I would like to understand if I need to use ML  algorithm or just pick up uniformly random time and places. My understanding tells me that doing so I would lose any correlation between users-time-center location.

So I was naturally leaning towards ML.. which frameworks should I study for this?","that sounds really bright thanks! I did not think I coul just simulte each center separately. 
Yes unfortunately I have ~2500 centers in the same country. Would this be a huge problem?

my data really looks like a table , where eachy row is like this

check_in_id, User_id , chceck_in_date_and_time, center_id, Center_city, center_latitude, center_longitude

for example

3335, 45, 2020-10-5 10:00 UCT, 554, New york, 54, 40.7128° N,
74.0060° W

I have this for 1 year. The centers offer a service they use usually ~6 time a month on average",misc
[P] Time series forecasting,"Time series Forecasting

Hi everyone I am trying first forecasting project. 

I have a time series over 1 year which is made by users check-ins everyday in a physical center located on a single country/nation.
I want to produce synthetic data  to do forecasting and simulations.

Now I would like to understand if I need to use ML  algorithm or just pick up uniformly random time and places. My understanding tells me that doing so I would lose any correlation between users-time-center location.

So I was naturally leaning towards ML.. which frameworks should I study for this?","It will only be a potential problem for the locations that have very few check-ins. However, you should be able to group similar locations together unless for some reason you think that the number of check ins is highly sensitive to location. 

Another thing you can do is reframe the problem for low frequency locations as a binary output that represents whether or not you expect to see a checkin event in some set lookahead window such as 3 days. This will give you more 'positive' examples in your time series which should be easier to learn",misc
[P] Time series forecasting,"Time series Forecasting

Hi everyone I am trying first forecasting project. 

I have a time series over 1 year which is made by users check-ins everyday in a physical center located on a single country/nation.
I want to produce synthetic data  to do forecasting and simulations.

Now I would like to understand if I need to use ML  algorithm or just pick up uniformly random time and places. My understanding tells me that doing so I would lose any correlation between users-time-center location.

So I was naturally leaning towards ML.. which frameworks should I study for this?",I see. I think you gave me a HUGE help with this really,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","Also, for academic labs Nature requires open source code. It's double standards that they didn't for DeepMind.",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","Let’s be honest, even 20 millions people signed it, do you think they would fear the pressure and open source it? Has any close source become open source after public signing letter?",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",You all want to make a positive impact? Write and sign a letter to complain for the double standard applied at Nature. They must provide the code for reproducibility as everyone else publishing there.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",Not gonna happen.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","I don’t know this space that well, but I’d imagine that this technology could be used for as much bad as it could good — the doc doesn’t seem to address this. Do you have a stance on this in regards to potential dangers of open sourcing?",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","This technology is potentially more dangerous than a single nuclear weapon.  I REALLY don’t want this in the hands of an evil, determined actor to start creating designer drugs/viruses meant to harm.",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",">We are submitting the follow as a Letter to the Editor

Not a great look to have a typo in the first 5 words of the petition",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","> Indeed, one of us (RD) was a reviewer, and despite repeated requests, he was not given access to code during the review.

Interesting",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",Didn't Kohli say that they were planning to open source it in 6 months?,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",The technology reaches a point where it can be used for big magnitudes of good or evil. In this specific case virus engineering research. Until the AIs are able to self regulate we need to cool down on open souring.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",They open sourced millions of simulated protein data no? Not good enough?,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","This is the real issue. Google invested the money to develop AF3. It’s their prerogative how open/accessible the model is. But, Nature should not have gone against their own policies to act as basically an advertisement for Google. That’s a disservice to their readers and anyone who publishes there that doesn’t get the same benefit. 

If anything, the open letter should be to get Nature to retract the AF3 paper. If the scientific community has no way to verify the results of a paper, then the paper is invalid.",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","One of the reviewers agreed and was removed. 

>But no one besides Novartis& Lilly etc. has access to the ligand structure prediction. All the data on ligand binding in the paper is irreproducible and should not have been published. As Reviewer #3 for @nature I recommended taking it out and saving it for promotional material. https://twitter.com/RolandDunbrack/status/1789081040281079942

>Possibly this is why they might have demanded that #reviewer3 was removed from review of the revision, a privilege which no other set of authors would be granted by @nature
. https://twitter.com/RolandDunbrack/status/1789083883394253096


> There is no such thing as AlphaFold3. There is only http://alphafoldserver.com/. The difference is throttling rigorous scientific and biomedical research. https://twitter.com/RolandDunbrack/status/1789884648782205140


> I am unhappy with DeepMind on the biological science that will not be accomplished. Ok, we can’t do any ligand because that may deprive them of revenue. But we can’t do high-throughput benchmarks or protocol development applications to cancer or other diseases.
https://twitter.com/RolandDunbrack/status/1789083096865743018",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",That's a really good point. What IRKs me most is that they've made the REALLY useful stuff completely inaccessible. Isomorphic sure has an advantage in the drug design space.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","Irreproducible, overblown advertisements published solely on the basis of the institutional imprimatur of the authors? In *Nature*?

<always has been meme>",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",It may also encourage a competitor to develop its own open-source version.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",Yes GPT2 was released after public pressure.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",Its also a value statement that what they do ist not considered appropriate conduct,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","I tend to disagree, none of the stuff AF3 does is impossible today.   
If you can make use of the AF3 output you probably have enough knowledge to get them using currently available tools.

This is like  ""GPT-2 is too dangerous to release"".",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",You are getting downvoted but you are absolutely correct.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","We can face head first and deal with the potential dangers of an open source model, its exponentially harder to deal with these same problems on a closed source model. Obscurity does not really work as a safety mechanism, as it has been proved thousands of times in this field, it only makes it harder to address this type of issues.",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",Agree that it should talk about downsides,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","Thanks for raising this issue, it is important, even if open-source ideologues would prefer to plant their heads in the sand and pretend that if they don't talk about something that it isn't an issue.",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",Agreed. Any paper without code is just an ad.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","Looks like they caved (sort of).

https://twitter.com/pushmeet/status/1790086453520691657

https://twitter.com/maxjaderberg/status/1790086549205401947

They’re releasing the code and weights for *academic* use within 6 months (we’ll see if this will actually materialize).

My guess is it will be per research group licensing, so not completely free to the public at large.",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","Could that maybe go both ways? One reason to make it inaccessible is because it works too well. Another reason to make it inaccessible is because it doesn't work very well at all.

I don't know anything about Isomorphic specifically, but that's a pretty common trick among tech startups generally: claim to have mind-blowing technology in order to build hype and get investor money, but also claim that the technology is too powerful / you're still working on patents / whatever as a stopgap to prevent people from finding out that your tech doesn't actually work yet, or only works in a prohibitively restrictive subset of applications.",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","Reproducibility and impact factor are negatively correlated, change my mind (I think there was actually a study showing this at some point?)",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","The authors of OpenFold have already started doing it, on saturday the PI said that it would prob take around 6 months to catch up tho",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",once it was completely obsolete,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","Thanks for the note! Like I said, I’m not a domain expert so that’s helpful context.",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","Thanks. This is the elephant in the room which will likely cause this letter to quickly be dismissed by Deepmind. 

IMO I would think Deepmind would start opening partnerships with specific medical orgs, giving them quota larger than 10 per day. Hopefully GDM will be given ample resources to continue scaling up",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","I would be very surprised if it were smoke and mirrors. They have a glowing endorsement from some of the members of Rosetta, the best physics based alternative for MDM. AF has a good track record and previous models have long been known to be able to perform blind docking. If AF2 can do it the chances their new model does better isn't unlikely. The proof will be in the patents we see a few years from now.",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",The use statement specifically forbids you from training any models or performing any docking simulations. They know what they have gives them an edge so they’re trying to winnow it down.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",Knowing Google from reasonably very close quarters - this makes total sense.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","I have no idea if that's true but it wouldn't be even remotely surprising if it were - the most popular publications are the ones with the most surprising results, and surprising results are also the most likely ones to be wrong.",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","Really wonder if this works. In the paper they say they filed a patent. Good luck going around that. If they patent msa + diffusion model we are screwed, maybe. Haven’t looked into the patent though…",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","It was like within a year of the paper coming out... 

Also that's very much debatable, GPT2 is still relevant and a nice smaller model benchmark.",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","But how would anyone know that those things actually work really well, if no one is allowed to use them?",misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",What they will be able to do against some nerds and the internet? If someone open source it its over. With or without patent.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",It’s so Isomorphic Labs and Deepmind can monetize them first.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w",Then I fear you don’t understand how patents works.,misc
[D] Please consider signing this letter to open source AlphaFold3,"https://docs.google.com/forms/d/e/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g/viewform

Google DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.

AF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.

Please sign the letter !

AF3 : https://www.nature.com/articles/s41586-024-07487-w","That's certainly one theory, and it might even be part of what they're thinking, but this issue of whether it actually works or not is a different matter. 


Like, they can say and do whatever they want, but if nobody can use the product then we really have no way of knowing if it really works. If it doesn't work well then hiding it away isn't going to give them any real advantage in the long run.",misc
[D] Moving my threshold using few shot examples,"I have a BERT based classifier and have decided that I want a different threshold for my model’s decision boundary. I have a only a few (dozen) examples of labels that exemplify this new threshold. It seems to me shifting the last layer predictions to this new decision boundary without gradient training should be easy and wouldn’t need many examples.
Any ideas on how to implement this?",Use AUROC plot to find threshold,misc
[D] Moving my threshold using few shot examples,"I have a BERT based classifier and have decided that I want a different threshold for my model’s decision boundary. I have a only a few (dozen) examples of labels that exemplify this new threshold. It seems to me shifting the last layer predictions to this new decision boundary without gradient training should be easy and wouldn’t need many examples.
Any ideas on how to implement this?",But how do I edit this threshold into my model?,misc
[D] Moving my threshold using few shot examples,"I have a BERT based classifier and have decided that I want a different threshold for my model’s decision boundary. I have a only a few (dozen) examples of labels that exemplify this new threshold. It seems to me shifting the last layer predictions to this new decision boundary without gradient training should be easy and wouldn’t need many examples.
Any ideas on how to implement this?","I just realised that auroc applies only to binary classification i think. Just fact check this. 
Then coming to ur question i think changing the bias term in the last layer would help u. Not really sure how u would do that for multi class classification.",misc
[D] Time series forecasting with extremely limited amount of data,"Hey everyone,

I am looking for some suggestions to work on this task. I have a few time series with only 30/40 observations and of course we all agree this is a really limited amount data. 
I want to forecast some financial metrics and I have only these few observations because data were collected on monthly basis.

Do you have any suggestions? Of course I must try with a simple regression as first, but it would be highly appreciated if you know some other methods that I may try.
I read something related to few shot learning, but it seems to me that many applications use LSTM o other neural networks and I think that although they're thought to address these kind of problems, all the papers I've read so far use series with 100/120 observations and I don't know if it might work for me.

Thanks for sharing your knowledge 🙂","This is a totally normal amount of monthly data. Use any reasonable statistical model. [""Forecasting: Principles and Practice""](https://otexts.com/fpp3/) is your best friend. In particular, [this chapter](https://otexts.com/fpp3/long-short-ts.html) on short time series can be helpful. Just use AutoARIMA or AutoETS, and remember to do time series CV (expanding window) for evaluation.

Also, note that you should carefully assess the confidence intervals and p-values of statistical tests, due to limited sample size.",misc
[D] Time series forecasting with extremely limited amount of data,"Hey everyone,

I am looking for some suggestions to work on this task. I have a few time series with only 30/40 observations and of course we all agree this is a really limited amount data. 
I want to forecast some financial metrics and I have only these few observations because data were collected on monthly basis.

Do you have any suggestions? Of course I must try with a simple regression as first, but it would be highly appreciated if you know some other methods that I may try.
I read something related to few shot learning, but it seems to me that many applications use LSTM o other neural networks and I think that although they're thought to address these kind of problems, all the papers I've read so far use series with 100/120 observations and I don't know if it might work for me.

Thanks for sharing your knowledge 🙂",Just throwing a rock in the dark. See if u can find some data which is similar to this and is available in open-source. Then train a model for forecasting that data and then fine tune ur model with this new points u have. If it were a physical system i could speak about some guarantees like how the model is learning the dynamics of the system and hence can be used to finetune on another data set. But i am not sure if similar logic is applicable to finance data. But u could try using something like timeGPT which is actually something that exists. See if that model is enough to capture the dynamics of ur data.,misc
ML Feature Compression [D],"Hey All,

We know that feature reduction/Compression can be used via AutoEncoders, SVD, PCA, etc. 

- Are there any methods that anyone can think of other than these that have worked for them?
- When using feature reduction, are there any techniques/gotcha’s that you’ve learned over the years that you’d want to share?","Check out some spectral “clustering” methods!

These methods (Laplacian Eigenmaps, Diffusion Maps) are more or less based in the following steps-

1) build a graph on the data (typically by taking a Gaussian kernel over pairs of points, but there are many variations)

2) compute the graph Laplacian (or some normalized Laplacian, or a normalized transition markov matrix)

3) perform PCA (or SVD when applicable) to obtain eigenvectors, which contain the new features.

They’re called clustering methods, but in reality the  graph laplacian is an extremely powerful object, and its spectra describes various aspects of the geometry of a dataset. These methods are highly utilized on lots of datasets, such as single-cell rna sequencing data, financial data, seismic data, medical images & video & more. In fact word2vec  (and some variants) which is widely used for text data is prove-ably a spectral method!

These are very cool from a theoretical standpoint- especially Diffusion Maps, which learns features of the geometry of how the data is organized by relating a diffusion and markov operator on the data, and therefore organizes the data by asking the question- how would heat propagate through the graph of this data? (It actually models solutions to the heat equation on the “intrinsic manifold” that the data is “sampled” from). The nice thing about diffusion maps is that it preserves a metric on the data.

This all leads into manifold learning methods (of which there are many), there are lots of cool variants of all these methods that have been extended.

Here are some sources-

[nice tutorial](https://towardsdatascience.com/spectral-clustering-for-beginners-d08b7d25b4d8)

[Diffusion Maps](https://www.sciencedirect.com/science/article/pii/S1063520306000546)

[Laplacian eigenmaps](https://www2.imm.dtu.dk/projects/manifold/Papers/Laplacian.pdf)

[Short paper on local vs global feature embedding](https://www.pnas.org/doi/epdf/10.1073/pnas.0709842104)

[word2vec uses the graph spectra](https://arxiv.org/pdf/2002.12317)",misc
ML Feature Compression [D],"Hey All,

We know that feature reduction/Compression can be used via AutoEncoders, SVD, PCA, etc. 

- Are there any methods that anyone can think of other than these that have worked for them?
- When using feature reduction, are there any techniques/gotcha’s that you’ve learned over the years that you’d want to share?","One thing that I have found to help with dimensionality in Neural Networks is semi supervision or self supervision. You essentially put your inputs in, reduce dimensionality while corrupting / dropping information. Then use the reduce composition to try and recreate the inputs in a decoder and use some sort of distance as your loss (MSE, cosine, ect..). I like to warm up the network with self supervision then move to a semi supervision model to get really strong features for other algorithms.",misc
ML Feature Compression [D],"Hey All,

We know that feature reduction/Compression can be used via AutoEncoders, SVD, PCA, etc. 

- Are there any methods that anyone can think of other than these that have worked for them?
- When using feature reduction, are there any techniques/gotcha’s that you’ve learned over the years that you’d want to share?",Is this like using dropout while training an autoencoder?,misc
ML Feature Compression [D],"Hey All,

We know that feature reduction/Compression can be used via AutoEncoders, SVD, PCA, etc. 

- Are there any methods that anyone can think of other than these that have worked for them?
- When using feature reduction, are there any techniques/gotcha’s that you’ve learned over the years that you’d want to share?",This is just an auto encoder though right?,misc
ML Feature Compression [D],"Hey All,

We know that feature reduction/Compression can be used via AutoEncoders, SVD, PCA, etc. 

- Are there any methods that anyone can think of other than these that have worked for them?
- When using feature reduction, are there any techniques/gotcha’s that you’ve learned over the years that you’d want to share?","It’s similar, it’s almost like teaching your base model to encode the input data natively by manipulating cost functions and adding a decoder for training, but removing it for downstream use.",misc
[D] Time series Anomaly detection with diffusion models,"Hello all, I am working on a project on time series anomaly detection using diffusion models. Previously I have used a CycleGAN to learn the mapping x -> z -> x\_hat. Then I measure the reconstruction error between x and x\_hat to detect anomalies. This is fairly straightforward as the latent space in GANs is simply a gaussian distribution but in the case of diffusion models I think it gets complicated because of the N iterations in the forward and reverse process. My question is how do I condition the diffusion model to produce a near identical x\_hat compared to x? Can I combine a VAE (variational auto encoder) along with the diffusion model to help do this? Any input would be much appreciated.","> My question is how do I condition the diffusion model to produce a near identical x_hat compared to x?

If you don't use a VAE then diffusion models are invertible. Just solve the differential equation backwards in time.

If you do use a VAE then diffusion models are still invertible in the latent space, but reconstruction error will mostly be a result of deficiencies in your VAE.",misc
[D] Time series Anomaly detection with diffusion models,"Hello all, I am working on a project on time series anomaly detection using diffusion models. Previously I have used a CycleGAN to learn the mapping x -> z -> x\_hat. Then I measure the reconstruction error between x and x\_hat to detect anomalies. This is fairly straightforward as the latent space in GANs is simply a gaussian distribution but in the case of diffusion models I think it gets complicated because of the N iterations in the forward and reverse process. My question is how do I condition the diffusion model to produce a near identical x\_hat compared to x? Can I combine a VAE (variational auto encoder) along with the diffusion model to help do this? Any input would be much appreciated.","The original paper SDE diffusion paper from song also gives a way to calculate the density p(x) by solving a neural ode. in principle, p(x) should be larger for non-anomaly data so you might even use this as discrimniator but not sure. Other than that, you can use diffusion inversion (there's ddim inversion, null-text inversion for latent diffusion models and re-noising inversion and few other methods based on fix point iterartions, these methods basically also solve the reverse ODE that the other commentor mentioned) to get a latent that reconstructs a given input.",misc
[D] Time series Anomaly detection with diffusion models,"Hello all, I am working on a project on time series anomaly detection using diffusion models. Previously I have used a CycleGAN to learn the mapping x -> z -> x\_hat. Then I measure the reconstruction error between x and x\_hat to detect anomalies. This is fairly straightforward as the latent space in GANs is simply a gaussian distribution but in the case of diffusion models I think it gets complicated because of the N iterations in the forward and reverse process. My question is how do I condition the diffusion model to produce a near identical x\_hat compared to x? Can I combine a VAE (variational auto encoder) along with the diffusion model to help do this? Any input would be much appreciated.",Why use diffusion models.are you out of your mind,misc
[D] Time series Anomaly detection with diffusion models,"Hello all, I am working on a project on time series anomaly detection using diffusion models. Previously I have used a CycleGAN to learn the mapping x -> z -> x\_hat. Then I measure the reconstruction error between x and x\_hat to detect anomalies. This is fairly straightforward as the latent space in GANs is simply a gaussian distribution but in the case of diffusion models I think it gets complicated because of the N iterations in the forward and reverse process. My question is how do I condition the diffusion model to produce a near identical x\_hat compared to x? Can I combine a VAE (variational auto encoder) along with the diffusion model to help do this? Any input would be much appreciated.","Can you elaborate a little bit more? I only have a elementary understanding of diffusion models, what do you mean by ""solve the differential equation backwards in time.""? Also what do you mean by ""reconstruction error will mostly be a result of deficiencies in your VAE.""?",misc
[D] Time series Anomaly detection with diffusion models,"Hello all, I am working on a project on time series anomaly detection using diffusion models. Previously I have used a CycleGAN to learn the mapping x -> z -> x\_hat. Then I measure the reconstruction error between x and x\_hat to detect anomalies. This is fairly straightforward as the latent space in GANs is simply a gaussian distribution but in the case of diffusion models I think it gets complicated because of the N iterations in the forward and reverse process. My question is how do I condition the diffusion model to produce a near identical x\_hat compared to x? Can I combine a VAE (variational auto encoder) along with the diffusion model to help do this? Any input would be much appreciated.","It's hard to see this from the original literature on diffusion models, but what a diffusion model is actually doing is solving a differential equation that has the noise distribution or the data distribution as the initial condition, and the time evolution of the differential equation produces a sample from the other distribution.

Differential equations are generally invertible, and so diffusion models are too; if x = x(t=0) is your data sample, and z = x(t=1) is the corresponding noise sample, then you can compute z from x by solving the differential equation from t=0 to t=1, and conversely you can calculate x from z by solving the differential equation from t=1 to t=0.

Another approach that includes diffusion, but is easier, faster, and more general, is called ""flow matching"". There's a good library that does it here: https://github.com/atong01/conditional-flow-matching

With flow matching you use differential equation solvers explicitly, which means that it's easy to run the generative model both forwards and backwards in time.

EDIT: If you do diffusion in a latent space by mapping data samples x to some latent space using a VAE, then the model isn't totally invertible because the VAE isn't invertible. VAE's only *approximate* the reconstruction of data samples, and the quality of that approximation depends on how well the test data distribution matches the training data distribution.",misc
[D] Time series Anomaly detection with diffusion models,"Hello all, I am working on a project on time series anomaly detection using diffusion models. Previously I have used a CycleGAN to learn the mapping x -> z -> x\_hat. Then I measure the reconstruction error between x and x\_hat to detect anomalies. This is fairly straightforward as the latent space in GANs is simply a gaussian distribution but in the case of diffusion models I think it gets complicated because of the N iterations in the forward and reverse process. My question is how do I condition the diffusion model to produce a near identical x\_hat compared to x? Can I combine a VAE (variational auto encoder) along with the diffusion model to help do this? Any input would be much appreciated.","Okay that explains it, thanks for the link will look into it.",misc
[D] Time series Anomaly detection with diffusion models,"Hello all, I am working on a project on time series anomaly detection using diffusion models. Previously I have used a CycleGAN to learn the mapping x -> z -> x\_hat. Then I measure the reconstruction error between x and x\_hat to detect anomalies. This is fairly straightforward as the latent space in GANs is simply a gaussian distribution but in the case of diffusion models I think it gets complicated because of the N iterations in the forward and reverse process. My question is how do I condition the diffusion model to produce a near identical x\_hat compared to x? Can I combine a VAE (variational auto encoder) along with the diffusion model to help do this? Any input would be much appreciated.","u/bregav I have run into an issue using the library that you linked, can I DM you for some help?",misc
[D] Time series Anomaly detection with diffusion models,"Hello all, I am working on a project on time series anomaly detection using diffusion models. Previously I have used a CycleGAN to learn the mapping x -> z -> x\_hat. Then I measure the reconstruction error between x and x\_hat to detect anomalies. This is fairly straightforward as the latent space in GANs is simply a gaussian distribution but in the case of diffusion models I think it gets complicated because of the N iterations in the forward and reverse process. My question is how do I condition the diffusion model to produce a near identical x\_hat compared to x? Can I combine a VAE (variational auto encoder) along with the diffusion model to help do this? Any input would be much appreciated.",yeah feel free,misc
[D] Time series Anomaly detection with diffusion models,"Hello all, I am working on a project on time series anomaly detection using diffusion models. Previously I have used a CycleGAN to learn the mapping x -> z -> x\_hat. Then I measure the reconstruction error between x and x\_hat to detect anomalies. This is fairly straightforward as the latent space in GANs is simply a gaussian distribution but in the case of diffusion models I think it gets complicated because of the N iterations in the forward and reverse process. My question is how do I condition the diffusion model to produce a near identical x\_hat compared to x? Can I combine a VAE (variational auto encoder) along with the diffusion model to help do this? Any input would be much appreciated.",just sent you,misc
"[N] PADRI TTS — 'Plan Ahead, Don't Rush It' Text-to-Speech","Blog: [https://picovoice.ai/blog/orca-true-streaming-tts/](https://picovoice.ai/blog/orca-true-streaming-tts/)

Doc: [https://picovoice.ai/docs/orca/](https://picovoice.ai/docs/orca/)

GitHub: [https://github.com/Picovoice/orca/](https://github.com/Picovoice/orca/)

https://i.redd.it/afefdw5eo80d1.gif",Why does it need an access key? It looks good but the fact that we need an internet connection removed all the stars in my eyes,misc
"[N] PADRI TTS — 'Plan Ahead, Don't Rush It' Text-to-Speech","Blog: [https://picovoice.ai/blog/orca-true-streaming-tts/](https://picovoice.ai/blog/orca-true-streaming-tts/)

Doc: [https://picovoice.ai/docs/orca/](https://picovoice.ai/docs/orca/)

GitHub: [https://github.com/Picovoice/orca/](https://github.com/Picovoice/orca/)

https://i.redd.it/afefdw5eo80d1.gif","So they can monitor your usage. You only get 5 hours.

I'd never be comfortable using a service like this. No matter how great or safe it may look, if it's constantly monitoring you & pinging home, I don't trust it.",misc
[D] Thoughts on DSPy,"I have been tinkering with DSPy and thought I will share my 2 cents here for anyone who is planning to explore it:

The core idea behind DSPy are two things:

1.	⁠Separate programming from prompting
2.	⁠incorporate some of the best practice prompting techniques under the hood and expose it as a “signature”

Imagine working on a RAG. Today, the typical approach is to write some retrieval and pass the results to a language model for natural language generation. But, after the first pass, you realize it’s not perfect and you need to iterate and improve it. Typically, there are 2 levers to pull:

1.	⁠Document Chunking, insertion and Retrieval strategy
2.	⁠Language model settings and prompt engineering

Now, you try a few things, maybe document the performance in a google sheet, iterate and arrive at an ideal set of variables that gives max accuracy.

Now, let’s say after a month, model upgrades, and all of a sudden the accuracy of your RAG regresses. Again you are back to square one, cos you don’t know what to optimize now - retrieval or model? You see what the problem is with this approach? This is a very open ended, monolithic, brittle and unstructured way to optimize and build language model based applications.

This is precisely the problem DSPy is trying to solve. Whatever you can achieve with DSPy can be achieved with native prompt engineering and program composition techniques but it is purely dependent on the programmers skill. But DSPy provides native constructs which anyone can learn and use for trying different techniques in a systematic manner.

DSPy the concept:

Separate prompting from programming and signatures

DSPy does not do any magic with the language model. It just uses a bunch of prompt templates behind the scenes and exposes them as signatures. Ex: when you write a signature like ‘context, question -> answer’, DSPy adds a typical RAG prompt before it makes the call to the LLM. But DSPy also gives you nice features like module settings, assertion based backtracking and automatic prompt optimization.

Basically, you can do something like this with DSPy,

“Given a context and question, answer the following question. Make sure the answer is only “yes” or “no””. If the language model responds with anything else, traditionally we prompt engineer our way to fix it. In DSPy, you can assert the answer for “yes” or “no” and if the assertion fails, DSPy will backtrack automatically, update the prompt to say something like, “this is not a correct answer- {previous_answer} and always only respond with a “yes” or “no”” and makes another language model call which improves the LLMs response because of this newly optimized prompt. In addition, you can also incorporate things like multi hops in your retrieval where you can do something like “retrieve -> generate queries and then retrieve again using the generated queries” for n times and build up a larger context to answer the original question.

Obviously, this can also be done using usual prompt engineering and programming techniques, but the framework exposes native easy to use settings and constructs to do these things more naturally. DSPy as a concept really shines when you are composing a pipeline of language model calls where prompt engineering the entire pipeline or even module wise can lead to a brittle Pipeline.

DSPy the Framework:

Now coming to the framework which is built in python, I think the framework as it stands today is

1.	⁠Not production ready
2.	⁠Lacks clear documentation
3.	⁠Poorly designed with not so clean interfaces and abstractions

To me it felt like a rushed implementation with little thought for design thinking, testing and programming principles. The framework code is very hard to understand with a lot of meta programming and data structure parsing and construction going behind the scenes that are scary to run in production.

This is a huge deterrent for anyone trying to learn and use this framework. But, I am sure the creators are thinking about all this and are working to reengineer the framework. There’s also a typescript implementation of this framework that is fairly less popular but has a much better and cleaner design and codebase:

https://github.com/dosco/llm-client/

My final thought about this framework is, it’s a promising concept, but it does not change anything about what we already know about LLMs. Also, hiding prompts as templates does not mean prompt engineering is going away, someone still needs to “engineer” the prompts the framework uses and imo the framework should expose these templates and give control back to the developers that way, the vision of separate programming and prompting co exists with giving control not only to program but also to prompt.

Finally, I was able to understand all this by running DSPy programs and visualizing the LLM calls and what prompts it’s adding using my open source tool - https://github.com/Scale3-Labs/langtrace . Do check it out and let me know if you have any feedback.","Nice write up. I did a presentation on DSPy for my coworkers a while back. It is very interesting.


Edit:
I kind of understand your criticism of the documentation, however, it is a product of a The  Stanford NLP lab, which is impressive for how much the grad students have done on it. They have made tremendous improvements by leaps and bounds since their DSP paper. hopefully the PI has students/postdocs who continue to work on it after the lead author moves on.",misc
[P] SimpleGEMM: Fast and minimal tensor core matrix multiplication in CUDA,"Hello all! Sharing my side project here: [https://github.com/andylolu2/simpleGEMM](https://github.com/andylolu2/simpleGEMM) !

This is an *extremely* minimalistic but fast implementation of matrix multiplication in CUDA. The source code is a single, 200-line CUDA/C++ file which implements fp16 tensor core matrix multiplication, optimised for Turing (SM75) architecture. The goal is to:

1. Write a matmul kernel that does not sacrifice performance. In fact, it's faster than PyTorch/CuBLAS if you [test it on a T4 in Colab](https://colab.research.google.com/github/andylolu2/simpleGEMM/blob/master/colab/simpleGEMM.ipynb)!
2. Make it hackable for new purposes. For example if you want to add a new custom prologue (e.g. Matmul + some reduction), just go to line 186, add your code, and recompile! Full flexibility with no C++ templating shenanigans.
3. Keep it as simple as possible. Hopefully someone learning CUDA will find this useful!

Of course, I didn't implement *everything* from scratch. Most of the this builds upon Nvidia CUTLASS's new CuTe interface for things like memory layout, data copying and using tensor core instructions.

*Aside:*

*Why not OpenAI Triton? I love triton, but sometimes it's hard to get the extra 10-20% performance if you are doing something off its main optimisation path. In fact,* [*triton's matmul for Turing GPUs is quite slow*](https://github.com/openai/triton/issues/189) *(because they mainly optimise for SM80+). I just enjoy having full control over the hardware, knowing that if I have infinite time I can squeeze very single bit of performance out.*","Just Saw this bit

> Assumes the inputs are in row-major layout. (Though you probably only want to use a row-major layout anyway, as other combinations are 10-30% slower.)

Can you elaborate on why col-major would be slower? I understand this one is built for row major, but couldn’t you adjust the algorithm for col major and get the same performance? I.e. there’s nothing inherently worse with col major, right?",misc
[P] SimpleGEMM: Fast and minimal tensor core matrix multiplication in CUDA,"Hello all! Sharing my side project here: [https://github.com/andylolu2/simpleGEMM](https://github.com/andylolu2/simpleGEMM) !

This is an *extremely* minimalistic but fast implementation of matrix multiplication in CUDA. The source code is a single, 200-line CUDA/C++ file which implements fp16 tensor core matrix multiplication, optimised for Turing (SM75) architecture. The goal is to:

1. Write a matmul kernel that does not sacrifice performance. In fact, it's faster than PyTorch/CuBLAS if you [test it on a T4 in Colab](https://colab.research.google.com/github/andylolu2/simpleGEMM/blob/master/colab/simpleGEMM.ipynb)!
2. Make it hackable for new purposes. For example if you want to add a new custom prologue (e.g. Matmul + some reduction), just go to line 186, add your code, and recompile! Full flexibility with no C++ templating shenanigans.
3. Keep it as simple as possible. Hopefully someone learning CUDA will find this useful!

Of course, I didn't implement *everything* from scratch. Most of the this builds upon Nvidia CUTLASS's new CuTe interface for things like memory layout, data copying and using tensor core instructions.

*Aside:*

*Why not OpenAI Triton? I love triton, but sometimes it's hard to get the extra 10-20% performance if you are doing something off its main optimisation path. In fact,* [*triton's matmul for Turing GPUs is quite slow*](https://github.com/openai/triton/issues/189) *(because they mainly optimise for SM80+). I just enjoy having full control over the hardware, knowing that if I have infinite time I can squeeze very single bit of performance out.*",simple is slower always. CUDA is great because GEMM and other ML primivites are tuned for every nvidia card individually.,misc
[P] SimpleGEMM: Fast and minimal tensor core matrix multiplication in CUDA,"Hello all! Sharing my side project here: [https://github.com/andylolu2/simpleGEMM](https://github.com/andylolu2/simpleGEMM) !

This is an *extremely* minimalistic but fast implementation of matrix multiplication in CUDA. The source code is a single, 200-line CUDA/C++ file which implements fp16 tensor core matrix multiplication, optimised for Turing (SM75) architecture. The goal is to:

1. Write a matmul kernel that does not sacrifice performance. In fact, it's faster than PyTorch/CuBLAS if you [test it on a T4 in Colab](https://colab.research.google.com/github/andylolu2/simpleGEMM/blob/master/colab/simpleGEMM.ipynb)!
2. Make it hackable for new purposes. For example if you want to add a new custom prologue (e.g. Matmul + some reduction), just go to line 186, add your code, and recompile! Full flexibility with no C++ templating shenanigans.
3. Keep it as simple as possible. Hopefully someone learning CUDA will find this useful!

Of course, I didn't implement *everything* from scratch. Most of the this builds upon Nvidia CUTLASS's new CuTe interface for things like memory layout, data copying and using tensor core instructions.

*Aside:*

*Why not OpenAI Triton? I love triton, but sometimes it's hard to get the extra 10-20% performance if you are doing something off its main optimisation path. In fact,* [*triton's matmul for Turing GPUs is quite slow*](https://github.com/openai/triton/issues/189) *(because they mainly optimise for SM80+). I just enjoy having full control over the hardware, knowing that if I have infinite time I can squeeze very single bit of performance out.*","I think the main reason is that if your data is in column major you need to do a transpose before issuing the tensor core instructions (tensor cores only handles row-major). Now in theory, there is a specialised instruction for that (ldmatrix{.trans}) which should do the transpose for you when you copy from smem to rmem. I think this instruction runs slower than the non-transpose version? I haven't benchmarked it myself.

Another possibility is that iterating over rows in gmem (in the GEMM outer loop) is faster for row-major data than column-major because of better cache locality.",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","Good question- We have something called “dropout” to help with this. 

Basically with dropout we prevent too much energy getting in the model by making a small hole. In this case the solar radiation should drain out without incident.",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","That actually sounds like the plot of a (dumb) sci-fi movie. 

Humans and AI living together in harmony. 

One random bit-flip: Skynet.",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",Okbuddylecunn? Okbuddysvm? Okbuddyhinton? OkbuddySchmid? MLcirclejerk? What should it be called?,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",Wrong transformers. They were clearly talking about the giant space robots led by Optimus Prime.,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","Oh, hell, stop downvoting them, have the laugh and enjoy your Sunday :)",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","Incidentally, does any one else think 'transformer' was a stupid name to use for the neural network?",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","That was a nice laugh. 

I miss what this sub used to be.",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",You could try prompting for aluminium foil. Perhaps it can protect the transformer.,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","I'll reply with an earnest answer because it's fun to talk about.

LLMs are generally trained on supercomputers which, like any computer, [are susceptible to cosmic radiation interference](https://physicsworld.com/a/cosmic-challenge-protecting-supercomputers-from-an-extraterrestrial-threat/). However, as the article mentions, supercomputers are more likely to experience ill effects from cosmic radiation given their much larger surface area.

As an administrator of several such systems, I can say that the errors that result from such radiation are either virtually non-existent, or impossible to discern. Hypothetically, if one did occur, it would likely manifest as a bit flip on a specific piece of hardware.

If the bit flip occurred in RAM, for example, it could register as an ECC error, which may or may not cause a kernel panic. If the node panics, your training job would fail and requeue, with the node being set to `down` in whichever scheduler you use. Your team would go through running diagnostics on the node, they would all pass, and you would return the node to the production queue, unable to replicate the issue.

It is far more likely that training performance will suffer from a piece of hardware that fails due to a manufacturer defect than from cosmic radiation. Eg, that ECC error you just experienced is much more likely a bad DIMM than a bit flip from space.

That being said, due to the recency of the solar storm, this was actually a topic of conversation that was brought up, albeit in jest, during a couple calls and threads at work. It was interesting to talk about and revisit some of the examples laid out in the article.

TL;DR: cosmic rays caused by solar storms are a minimal, virtually imperceptible risk to LLM training supercomputers",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",Isn’t this why we use ECC?,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",Pshh what are you some kind of hobbyist? We only host our models in faraday cages 5 miles underground,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","During periods of high solar activity I switch all models over to cisformers for exactly this reason. You can mostly use the same parameter values, just flip the sign.",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",I wish I had friends that would understand this joke if I told it to them,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","This reminds me of the scifi novelette, 'For a Breath I Tarry', where the protagonist AI is described in the beginning as being an anomaly due to construction during a solar storm or flare or something like that.",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",llama is going to become sentient,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",It seems like the solar storm has wrapped our sense of time and thrown us back to April 1st! Anyone else feeling the temporal distortion over there?,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","Omfg, what a time to be alive!

https://en.m.wikipedia.org/wiki/Transformer",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",Dropout is all you need. Ivy league schools hate this one trick!,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",Just gotta flip the sign [in the right place](https://openai.com/index/fine-tuning-gpt-2/#_5VLCK1KHEBCzRHpnOQQ0Lj)...,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","if model.aboutToBeEvil():
        ~~dont()~~
        do()",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",Isn't that the plot to Age of Ultron?,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",Okaybuddyhinton sounds cool ngl,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","OkBuddyRelu?

OkAgentRelu?

SmoothConvexHull?",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","absolutely, obviously should've been called Kolmogorov-Vapnik-Dot-Product-Machines",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",Transformer is the most generic name possible.,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",We can’t afford to turn on ECC and lose 2GB on our 4090s,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",I've heard this is a best practice. Some also advice to use geothermal energy,misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","Overfit of neural networks are controlled by dropping random neurons. All problems can be solved that way. Too much pollution? Dropout on factories. Dislike the congress? Dropout. Wanna lose some weight? That's right, Dropout.",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","I thought this would be linking the article about Jeff Dean and [I forget the other engineer's name] debugging a solar flare-caused bit flip in the Google search index that was causing them issues in like the early 2000s. Pretty legendary story, they went through the code line by line, couldn't find the bug, and then started printing out raw bits and debugging those and found a bit that was not supposed to be flipped.

Or at least that's how I vaguely remember it.",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","Created.

Go to r/okaybuddyhinton for low effort ML content",misc
[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?,"Hi all,

While reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.

""Widespread voltage control problems and protective system problems can occur,"" NOAA warns. ""Some grid systems may experience complete collapse or blackouts. Transformers may experience damage."" 

I'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?",Lol at SmoothConvexHull,misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","Eh, sick of the hype around these things. You're not replacing software engineers with this, let alone interns. Maybe rebrand it as a helper.",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","Congratulations on open sourcing this!
But I have a piece of advice: You really should not still call your license ""MIT License (Modified)"" when you add a sentence prohibiting commercial use, as the intention of your new license is far away from what most people would expect for an MIT license. It would be better to find another name for this license.",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","I watched the third example - it looks like pickledb already supported numpy arrays and discussed this in the GitHub issue link. Darwin then unnecessarily implemented this by creating a new class over the pickledb class. 


Is this a fundamental issue in Darwin that it made a logical error when trying to ascertain if the functionality already existed? I know other LLMs do want to assume that if they're asked to do something the functionality doesn't exist.",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","Open devin

Devika

Darwin

How would you compare across the three?",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","After checking the github, it *looks* like the main parts of this are various small Python functions (in \`/functions\`) which mostly do standard parsing things and a generic API and UI written in REACT.  The only part that seems to do anything interesting is in [agent.py](https://github.com/Cognation/darwin/blob/9a276fcc63eecadb049648d576f12d291850712d/functions/agent.py#L36) and that pulls [this](https://smith.langchain.com/hub/shankerabhigyan/openai-tools-agent-darwin) from langchain smith which, itself, is forked from the fairly popular [hwchase17/openai-tools-agent](https://smith.langchain.com/hub/hwchase17/openai-tools-agent).  The only thing changed on this that has a ton of impact is the System statement, but that's just the prefix: 

>*You are a Professional Software Developer Agent. Your Job is to answer all the user's query as correctly as possible. You should always search the web for relevant documentation and examples before writing any code.*

So, I guess my question is: what differentiates this project from, for example, the [basic quickstart](https://docs.smith.langchain.com/) for LangSmith?  Or asking Chatgpt with this same prefix?

---

I'd also recommend having a way to have users run this in docker, since I don't want to have to mess around with my system \`npm\` or create a conda env to try this out.",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","Just wondering. Darwin is the name of the core Mac OS. I'm sure nobody wants to draw the attention of Apple's legal team.

https://en.wikipedia.org/wiki/Darwin_(operating_system)",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","Who are you?
Is this a final year college project?",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","Cool! Any open-source project that tries to compete with Devin is welcome! Thank you for open-sourcing this. It looks simple to use and efficient! It has some issues understanding the whole codebase before making pull requests (for example, missing to see that there was already a functionality leveraging numpy and ending up reimplementing it), but this is easily solvable by using some context extension method or an RAG and adding a preprocessing phase to extract some metadata about the various parts of the project source. I'm sure you are going to handle this, and I will check the repo on GitHub often to give you feedback. Good luck! 

A humble suggestion: you should call it Darvin or Derwin, not Darwin. It would be much better and unique (and a pun about Devin, like you did in the first video! 😉). Not to mention that, if you call it Darwin, no one will ever find it on Google among billions of search results about the real Darwin! 😄",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.",Have you ever used aider? I’ve never seen one of these tools more thoughtfully implemented. Running it in a loop is not difficult.,misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.",Can this work with any API key for any model?,misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.",Is devin actually working well?,misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.",Can this run locally with say ollama or any others. Any doc would help.,misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","You are not wrong, only for now. But soon LLMs will get better at it. The other thing to note is, it's not going to replace logic anytime soon, just the repetitive tasks. The ablity to make critical decisions will still lie in te hands of software engineers.",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","thanks for pointing it out. I don't have much idea about how different licenses exactly work and in the excitement of releasing this, didn't do much research on it. But I will surely update this.",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","Yes, Darwin did make a mistake understanding things. However the most plausible thing I can think of is this - 

since the solution was already mentioned and I still asked it to code a solution, what Darwin did was reiterate the same solution with better error handling and edge cases. The same can be seen in the final generated code. 

Howerver, you are not wrong. LLMs do tend to think at times about the functionality not existing when they are asked to do something.",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","Darwin is being developed keep in mind the tasks of machine learning engineer. As an applied AI engineer, I have to go through lot of research papers everyday. With Darwin, I can ask it to explain me things, have a critical discussion and can even ask it to cite more papers and sources. More than that , once I am done, I can ask it to write the code based on the paper i just read, which need not be just one file but a complete directory. And even on top of that, Darwin comes with built-in code editor where it can execute the freshly written code and look for errors all by itself and re-iterate. I can then jump in and take over, right inside the Darwin code editor. Just this saves roughly 30\~40 percent of my time.

  
Darwin also comes with other features like GitHub issue resolution, that lets you solve github issues by just providing the issue url. Darwin can then write code, execute, debug and reiterate before you ask it to raise a PR for the issue.",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.",Darwin sucks.,misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","Darwin is developed and being developed keep in mind the tasks of machine learning engineer. As an applied AI engineer, I have to go through lot of research papers everyday. With Darwin, I can ask it to explain me things, have a critical discussion and can even ask it to cite more papers and sources. More than that , once I am done, I can ask it to write the code based on the paper i just read, which need not be just one file but a complete directory. And even on top of that, Darwin comes with built-in code editor where it can execute the freshly written code and look for errors all by itself and re-iterate. I can then jump in and take over, right inside the Darwin code editor. Just this saves roughly 30\~40 percent of my time.

And I haven't mentioned Darwin's GitHub issue resolution feature that lets you solve github issues by just providing the issue url. Darwin can then write code, execute, debug and reiterate before you ask it to raise a PR for the issue.

  
The other reason of not using LangSmith is, it could be a blackbox at times and also not very specific.

  
It's interesting that you mentioned [this](https://smith.langchain.com/hub/shankerabhigyan/openai-tools-agent-darwin) and found out one of the contributors of Darwin who built it during the pre-release phase. I suggest if you head over to our repo and you will notice other contributors too.  
Rest be assured, the docker will be released in the repo in a day or so.",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","Thanks for pointing it out. I did get a lot of those requests.  
What do you think about ""Darvin"" XD",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","I am an Applied AI Engineer, and I can assure you this is not a final year project XD",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","You pointed it out correctly. It does have some issues understanding the whole code base right now and I will be working on it next to make it better probably using RAG, but I'll have to think about it.

Your humble suggestions are always welcome :)  . And I am glad that somebody got the ""Darvin"" pun 🙌",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","I did try out aider. However, Darwin, centralized around ML engineers, is not just limited to writing codes but tasks like going through research papers and keeping itself update to ever changing knowledge and documentation using the help of internet. 

But there certain things that Darwin is currently missing like the knowledge of complete codebase which aider handles swiftly and can be referenced to improve Darwin.",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","No, right now it only supports openai models. The flexibility to adapt different models and api is surely in the pipeline although not prioritised. At the moment we are aiming to include features that can make the daily job easy for software engineers",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","I can't say much about it because it is still in early access and I myself haven't tried it but I have watched other use it and bulid things.

So to answer your question... well, to some extent yes. It won't get your all job done, but still manage to do most of it while you sit back and relax. You still need to provide some manual intervention here and there.",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","Hi, as of now, it only supports openai models. The flexibility to adapt different models and api is surely in the pipeline although not prioritised. At the moment we are aiming to include features that can make the daily job easy for software engineers and improving the existing ones",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","Fair enough, appreciate the humble response. I know how much work it is to create something like this.",misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.",I did some research and I am thinking of changing it to ACSL or AGPL. Thanks again for pointing it out :),misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.",Sounds good! Will try out,misc
[P] DARWIN - open-sourced Devin alternative,"**🚀 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! 🤖**  
 DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what’s better? Its open-sourced.

DARWIN is also capable of training a machine learning model and solving GitHub issues.  
 Watch our video tutorials to witness DARWIN's features in action:  
 📹 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https://www.loom.com/share/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  
 📹 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https://www.loom.com/share/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  
 📹 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https://www.loom.com/share/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)

We are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  
[ Access Darwin](https://github.com/Cognation/darwin)

Join us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.

Share your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  
 Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.",No worries mate,misc
"[P] A look at the latest major open LLM releases: Mixtral, Llama 3, Phi-3, and OpenELM",No additional context provided.,Great summary!,misc
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks",You need to think about the receptive field. Convolutional kernels are definitely enough to preserve spacial information.,general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks","Not only is u/swegmesterflex correct that the convs would definitely learn spatial information, remember crossattn is used in diffusion U-NETs, which uses positional embeddings for each patch of the image.",general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks","At nolifegamer: I couldnt see such attention in the referred code. It seemed to me the attention is performed only within the features of each pixel, not using any other Information than the features themselves. Can you point me to the line of code for spatial conditioning?",general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks","Interesting. Does prompting ""someone hanging upside down from a tree branch with their legs"" generate an adequate image?",general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks",The resnet blocks in the referred code seem to use kernels not greater than 3. This means a pixel can locally only coordinate with its direct neighbor? This feels like it would be a too slow flow of Information. Can convolutions that operate on the image border detect this by noticing the absence of any feature and stream this as spatial hint into the Pipeline?,general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks","It doesn't explicitely call it a positional embeddings, but [this](https://github.com/lucidrains/denoising-diffusion-pytorch/blob/main/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py#L252) line corresponds to the creation of a parameter concatenated to the key-value pairs, so it seems feasable that this would learn positional embeddings? Also, for future reference, if you are replying to someone, either do u/NoLifeGamer2 or click the ""reply"" button on their comment, because then we get notified.",general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks",The quoted implementation does not condition the unet with a prompt. Its just denoising based on what it was trained with,general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks","Yes, but you stack several of these layers in series, increasing the receptive field. As discussed in the other comments you also downscale, which will also increase the receptive field when paired with kernels of the same size.",general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks","There is also downsampling, which doubles the width of subsequent convolutional receptive fields.",general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks","Convolutions effectively move this kernel left to right to to bottom, so in the output tensor after the resnet block, features in a spacial region of the output tensor are influenced by pixels in the corresponding spacial region of the input.",general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks","It creates the qkv as follows

self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)

So these are just 1x1 convolutions so q k and v have no additional conditioning, it seems whatever extra kv pairs are appended, they can be attended only via the queries which are not location conditionable?",general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks","Yes this is true. Reviewing the code again there are 2 resnet blocks chained at the bottom, and each of these has two blocks which begin the forward with a 3x3 conv. So 4 times 3x3 convolution on a 8x8 image this should indeed give a good informational coverage / large field of sight.

Coming back to the initial question, when we start a DDIM on pure noise and have trained the network to denoise cat images, it must somehow „see“ a cat in the total noise, which must be sort of the mean of all cats. How do the convolutions organize that the upper pixels of the noise move a step toward average cat head and the bottom pixels move toward average cat feet? How do these pixels learn their position in the image? Is the position somehow learned during downsampling even when processing pure randomness?",general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks",Yes. So i thought number of downsample ops should somehow correspond to image size. I.e. The innermost layer should be „small enough“?,general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks",I was refering to the mem\_kv part where a parameter is created that is catted to the kv on line 265. It is not beyond the realms of possiblity that this param learns a positional encoding.,general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks","Yes, but isnt also an according q / query required to „trigger“ the right appended key?",general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks","The flattening process of an image is deterministic, so each patch will always end up at the same place in the query.",general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks",Ok and the values are entire feature maps rather than single channel values as I had believed? So the net can tweak the values to look different depending on the position?,general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks",Yep! The important features of the network in different localized patches can be learned depending on their location!,general_qa
[D] How do unets achieve spatial consistency?,"Hi,
I have been reading through unet pytorch implementations here https://github.com/lucidrains/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever „knows“ its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?

So when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?

I think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels „not enough“. 

Neither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.

So How can the unet achieve spatial consistency/spatial  auto-conditioning?

Thanks","In general I guess this is correct, in the current case the mem_kv seems to consist of just few extra pixels per attention head. Its not an entire map.",general_qa
[D] should active learning samples classes uniformly,"When using active learning to sample images from an unlabeled dataset, existing works usually does so by trying to have an uniform number of image per class. This approach allow to mitigate the class imbalance issue that can exist in some datasets.

However, when building up a dataset, we  want our training set to be as close as possible to the real dataset in term of class distribution. Thus, is the approach of AL methods wrong for trying to sample an uniform number of image per class?","Interesting perspective, do you think there's a way to combine class distribution with active learning to achieve a more balanced approach?",misc
[D] should active learning samples classes uniformly,"When using active learning to sample images from an unlabeled dataset, existing works usually does so by trying to have an uniform number of image per class. This approach allow to mitigate the class imbalance issue that can exist in some datasets.

However, when building up a dataset, we  want our training set to be as close as possible to the real dataset in term of class distribution. Thus, is the approach of AL methods wrong for trying to sample an uniform number of image per class?","In the end, for real applications, you want your algorithm to perform well on any class not just the most abundant in your training/testing dataset. If you keep an unbalanced training dataset, you might have an algorithm which performs a little better on the abundant classes but very poorly on rare classes compared to a balanced training dataset.",misc
[D] should active learning samples classes uniformly,"When using active learning to sample images from an unlabeled dataset, existing works usually does so by trying to have an uniform number of image per class. This approach allow to mitigate the class imbalance issue that can exist in some datasets.

However, when building up a dataset, we  want our training set to be as close as possible to the real dataset in term of class distribution. Thus, is the approach of AL methods wrong for trying to sample an uniform number of image per class?","https://stats.stackexchange.com/questions/357466/are-unbalanced-datasets-problematic-and-how-does-oversampling-purport-to-he

Try both, maybe one works better for you.",misc
Can one use squared inverse of KL divergence as another divergence metric? [D],"I came across this doubt (might be dumb), but it would be great if someone can throw some light on this:

The KL Divergence between two distributions p and q is defined as : `$D_{KL}(p || q) = E_{p}[\log \frac{p}{q}]$`

depending on the order of p and q, the divergence is mode seeking or mode covering.

However, can one use `$\frac{-1}{D_{KL}(p || q)}$` as a divergence metric?

Or maybe not a divergence metric (strictly speaking), but something to measure similarity/dissimilarity between the two distributions?

**Edit**:

it is definitely not a divergence as `-1/KL(p,q) <= 0` also as pointed in the discussion, `1/KL(p,p) = +oo`.

However, I am thinking it from this point: if `KL(p,q)` is decreasing `=>` `1/KL(p,q)` is increasing `=>` `-1/KL(p,q)` is decreasing. Although, `-1/KL(p,q)` is unbounded from below hence can reach `-oo`. Question is, does the above equivalence, make `-1/KL(p,q)` useful as a metric for any application. Or is it considered somewhere in any literature.","Divergence metrics are usually zero when the two inputs are the same. In particular, `KL(p,p) = 0`, as well as `abs(p-p)=0`. This basically says that the ""distance"" between ""equal"" inputs is zero.

Your divergence will produce infinity in this case: `1/KL(p, p) = +oo`. This would mean that `p` is extremely dissimilar to itself, which is strange.",misc
Can one use squared inverse of KL divergence as another divergence metric? [D],"I came across this doubt (might be dumb), but it would be great if someone can throw some light on this:

The KL Divergence between two distributions p and q is defined as : `$D_{KL}(p || q) = E_{p}[\log \frac{p}{q}]$`

depending on the order of p and q, the divergence is mode seeking or mode covering.

However, can one use `$\frac{-1}{D_{KL}(p || q)}$` as a divergence metric?

Or maybe not a divergence metric (strictly speaking), but something to measure similarity/dissimilarity between the two distributions?

**Edit**:

it is definitely not a divergence as `-1/KL(p,q) <= 0` also as pointed in the discussion, `1/KL(p,p) = +oo`.

However, I am thinking it from this point: if `KL(p,q)` is decreasing `=>` `1/KL(p,q)` is increasing `=>` `-1/KL(p,q)` is decreasing. Although, `-1/KL(p,q)` is unbounded from below hence can reach `-oo`. Question is, does the above equivalence, make `-1/KL(p,q)` useful as a metric for any application. Or is it considered somewhere in any literature.",[deleted],misc
Can one use squared inverse of KL divergence as another divergence metric? [D],"I came across this doubt (might be dumb), but it would be great if someone can throw some light on this:

The KL Divergence between two distributions p and q is defined as : `$D_{KL}(p || q) = E_{p}[\log \frac{p}{q}]$`

depending on the order of p and q, the divergence is mode seeking or mode covering.

However, can one use `$\frac{-1}{D_{KL}(p || q)}$` as a divergence metric?

Or maybe not a divergence metric (strictly speaking), but something to measure similarity/dissimilarity between the two distributions?

**Edit**:

it is definitely not a divergence as `-1/KL(p,q) <= 0` also as pointed in the discussion, `1/KL(p,p) = +oo`.

However, I am thinking it from this point: if `KL(p,q)` is decreasing `=>` `1/KL(p,q)` is increasing `=>` `-1/KL(p,q)` is decreasing. Although, `-1/KL(p,q)` is unbounded from below hence can reach `-oo`. Question is, does the above equivalence, make `-1/KL(p,q)` useful as a metric for any application. Or is it considered somewhere in any literature.","Thanks, it is definitely not a divergence as `-1/KL(p,q) <= 0` also as you pointed, `1/KL(p,p) = +oo`. 

However, I am thinking it from this point: if `KL(p,q)` is decreasing `=>` `1/KL(p,q)` is increasing `=>` `-1/KL(p,q)` is decreasing. Although, `-1/KL(p,q)` is unbounded from below hence can reach `-oo`. Question is, does the above equivalence, make `-1/KL(p,q)` useful as a metric for any application. Or is it considered somewhere in any literature.",misc
Can one use squared inverse of KL divergence as another divergence metric? [D],"I came across this doubt (might be dumb), but it would be great if someone can throw some light on this:

The KL Divergence between two distributions p and q is defined as : `$D_{KL}(p || q) = E_{p}[\log \frac{p}{q}]$`

depending on the order of p and q, the divergence is mode seeking or mode covering.

However, can one use `$\frac{-1}{D_{KL}(p || q)}$` as a divergence metric?

Or maybe not a divergence metric (strictly speaking), but something to measure similarity/dissimilarity between the two distributions?

**Edit**:

it is definitely not a divergence as `-1/KL(p,q) <= 0` also as pointed in the discussion, `1/KL(p,p) = +oo`.

However, I am thinking it from this point: if `KL(p,q)` is decreasing `=>` `1/KL(p,q)` is increasing `=>` `-1/KL(p,q)` is decreasing. Although, `-1/KL(p,q)` is unbounded from below hence can reach `-oo`. Question is, does the above equivalence, make `-1/KL(p,q)` useful as a metric for any application. Or is it considered somewhere in any literature.",Agree. OP would want a better reason than what they describe which just seems to break how people interpret the values (breaking a convention for the sake of breaking a convention isn’t a positive). OP should have some compute efficiency or better convergence properties with common optimizers,misc
Can one use squared inverse of KL divergence as another divergence metric? [D],"I came across this doubt (might be dumb), but it would be great if someone can throw some light on this:

The KL Divergence between two distributions p and q is defined as : `$D_{KL}(p || q) = E_{p}[\log \frac{p}{q}]$`

depending on the order of p and q, the divergence is mode seeking or mode covering.

However, can one use `$\frac{-1}{D_{KL}(p || q)}$` as a divergence metric?

Or maybe not a divergence metric (strictly speaking), but something to measure similarity/dissimilarity between the two distributions?

**Edit**:

it is definitely not a divergence as `-1/KL(p,q) <= 0` also as pointed in the discussion, `1/KL(p,p) = +oo`.

However, I am thinking it from this point: if `KL(p,q)` is decreasing `=>` `1/KL(p,q)` is increasing `=>` `-1/KL(p,q)` is decreasing. Although, `-1/KL(p,q)` is unbounded from below hence can reach `-oo`. Question is, does the above equivalence, make `-1/KL(p,q)` useful as a metric for any application. Or is it considered somewhere in any literature.","I don't think this behavior is particularly useful. It is actually very important that the value which represents absolute similarity is pinned to some real number. Without this it means that it is difficult to say how different two distributions actually are. You would always need a third distribution to measure relative to. Think about it like this, if I gave you two distributions that are highly similar, your equation would output a very large positive number. But how far is that number from absolute similarity? Well it is infinitely far away, same as if I gave you two completely different distributions. 

This also has terrible effects on programmability. It is very easy to check if something is equal to 0, it is not as easy to check if a function is going infinite. Additionally you will have big issues doing nearest neighbor search without defining some sort of scaling factor specific to your domain that prevents the distances from getting too large.",misc
Can one use squared inverse of KL divergence as another divergence metric? [D],"I came across this doubt (might be dumb), but it would be great if someone can throw some light on this:

The KL Divergence between two distributions p and q is defined as : `$D_{KL}(p || q) = E_{p}[\log \frac{p}{q}]$`

depending on the order of p and q, the divergence is mode seeking or mode covering.

However, can one use `$\frac{-1}{D_{KL}(p || q)}$` as a divergence metric?

Or maybe not a divergence metric (strictly speaking), but something to measure similarity/dissimilarity between the two distributions?

**Edit**:

it is definitely not a divergence as `-1/KL(p,q) <= 0` also as pointed in the discussion, `1/KL(p,p) = +oo`.

However, I am thinking it from this point: if `KL(p,q)` is decreasing `=>` `1/KL(p,q)` is increasing `=>` `-1/KL(p,q)` is decreasing. Although, `-1/KL(p,q)` is unbounded from below hence can reach `-oo`. Question is, does the above equivalence, make `-1/KL(p,q)` useful as a metric for any application. Or is it considered somewhere in any literature.","> useful as a metric

You can check whether this satisfies all the defining criteria for divergences (https://en.wikipedia.org/wiki/Divergence_(statistics)) or metrics. For both, the first criterion is `d(x,x)=0`: the distance/divergence from a point to itself is zero. The function you're proposing doesn't have this property. How does it affect its interpretation and usefulness? If you want to use it to measure some kind of ""distance"" between things, then it'll probably be highly unintuitive to have a point be infinitely far from itself.

I don't think I've ever seen this in literature either.",misc
[P] LoRA from scratch implementation for LLM classifier training,No additional context provided.,"I love these from scratch project, it is good to understand the quirks behind the tech you use. PEFT LoRA was really handy on whisper fine-tuning I did last year. 

  
Thanks for sharing, I'll try to check the full book as well",misc
[P] LoRA from scratch implementation for LLM classifier training,No additional context provided.,thank you big time for posting this. I so want to do this but am swamped with work. But will absolutely check it out later,misc
[P] LoRA from scratch implementation for LLM classifier training,No additional context provided.,"I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't 
render large Jupyter Notebooks, so just in case, here is an 
[nbviewer](https://nbviewer.jupyter.org/) link to the notebook:

https://nbviewer.jupyter.org/url/github.com/rasbt/LLMs-from-scratch/blob/main/appendix-E/01_main-chapter-code/appendix-E.ipynb

Want to run the code yourself? Here is a [binder](https://mybinder.org/) 
link to start your own Jupyter server and try it out!

https://mybinder.org/v2/gh/rasbt/LLMs-from-scratch/main?filepath=appendix-E%2F01_main-chapter-code%2Fappendix-E.ipynb



------

^(I am a bot.) 
[^(Feedback)](https://www.reddit.com/message/compose/?to=jd_paton) ^(|) 
[^(GitHub)](https://github.com/JohnPaton/nbviewerbot) ^(|) 
[^(Author)](https://johnpaton.net/)",misc
[P] LoRA from scratch implementation for LLM classifier training,No additional context provided.,This is very nice.... I really appreciate this post man. I love this community...,misc
[P] LoRA from scratch implementation for LLM classifier training,No additional context provided.,Actually really excited to check this one out. Getting a better understanding of these fine tuning optimizations have been on the back burner for a while. Thanks for sharing!,misc
[P] LoRA from scratch implementation for LLM classifier training,No additional context provided.,"Glad this looks useful, and no rush, LoRA is an evergreen :)",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","Very interesting to note these:

[https://en.wikipedia.org/wiki/Solomonoff%27s\_theory\_of\_inductive\_inference](https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference)

Solomonoff proved that this induction is incomputable, but noted that ""this incomputability is of a very benign kind"", and that it ""in no way inhibits its use for practical prediction""

Though Solomonoff's inductive inference is not computable, several AIXI-derived algorithms approximate it in order to make it run on a modern computer. The more computing power they are given, the closer their predictions are to the predictions of inductive inference (their mathematical limit is Solomonoff's inductive inference)

Solomonoff's induction naturally formalizes Occam's razor\[4\]\[5\]\[6\]\[7\]\[8\] by assigning larger prior credences to theories that require a shorter algorithmic description.

&#x200B;

Hmm.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","The wikipedia article on AIXI is written so well that I suspect that Hutter himself wrote it anonymously.

https://en.wikipedia.org/wiki/AIXI

I would go as far as to say it may be one of the best CS articles on  wikipedia.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks",RemindMe! 1 month,misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","> noted that ""this incomputability is of a very benign kind"", and that it ""in no way inhibits its use for practical prediction""

Do you have any idea what he meant by this? 

The whole point of approximation is that you can quantify and control how much error there is in your calculation, but a non computable function is one where you cannot know how close or far you are from a solution. In that sense a non computable function cannot be approximated, by definition.

A ""benign kind of incomputability"" seems like a contradiction in this respect.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks",I mean this is not that uncommon... I personally know many professors who wrote their own Wikipedia article,misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","I will be messaging you in 1 month on [**2024-06-12 02:43:22 UTC**](http://www.wolframalpha.com/input/?i=2024-06-12%2002:43:22%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/MachineLearning/comments/1cpcwuz/r_marcus_hutters_work_on_universal_artificial/l3nsv1e/?context=3)

[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F1cpcwuz%2Fr_marcus_hutters_work_on_universal_artificial%2Fl3nsv1e%2F%5D%0A%0ARemindMe%21%202024-06-12%2002%3A43%3A22%20UTC) to send a PM to also be reminded and to reduce spam.

^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201cpcwuz)

*****

|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)|
|-|-|-|-|",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","I know man. I know. Those were very interesting, and hence I put them all up, it is sure shot contradictory.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","Many uncomputable problems have computable approximations.   

For example, it’s uncomputable to find the shortest program that generates a given output. But you can find *short* programs that do the same just by searching through the space of programs with optimization algorithms.  

You’ll never know if you have the shortest, but you’ll get one that works.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks",He means that it is upper semi computable. You can check out his book or that of li and vitanyi if you want more details. Basically you can run all programs and maintain the sum of of 2^(-l(p)) for all programs that produce an output x.,misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","Ok, I checked the IP that wrote most of the article: 
https://whatismyipaddress.com/ip/150.203.212.110
Belongs to the Australian National University, where Hutter is a professor. Very likely that it was written by Hutter or one of his students",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","In my opinion that is not an approximation. It's sort of a solution to a problem, but it's not an approximation.

I think the distinction you're drawing there is actually between *possible* and *impossible*. And for that reason I think that ""non computable"" is bad terminology given its definition, because colloquially it does seem to imply that a non computable function literally cannot ever be evaluated, which isn't true.

Like, sure, the halting problem is non computable, but you always have the option of letting a program run until it stops, and sometimes it does indeed stop. You just can't know that it will beforehand, and (in general) you also can't know how close you are to a stopping point.

If I had my way we'd replace ""non computable"" with ""non approximable"", because that gives a very concrete and correct colloquial understanding of the meaning of the thing.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","James Bowery, who is a friend of Marcus Hutter and one of the judges for the Hutter prize, is active on the talk page.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","You should read up on computability theory, because other people have spent several decades writing books about this. Some noncomputable problems have approximate solutions, and others do not. Some very common problems (rendering the mandelbrot set) are uncomputable but can be easily approximated.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","> Some very common problems (rendering the mandelbrot set) are uncomputable but can be easily approximated.

Thank you, that's an illuminating example that clarified this perfectly.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","Do people actually develop provable error bounds and convergence rates for noncomputable problems? That's what approximation consists of and I've never seen anyone do it for a noncomputable problem. The closest thing I've seen is when people specialize a noncomputable problem to a more tractable subset of that problem, but that's not an approximation of a noncomputable problem; it's an approximation of a different problem that resembles a noncomputable problem.

I'm definitely not an expert in this though, so if you know of any specific examples of such things that I should read about then I'm definitely interested.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","Does it? Can you specify the noncomputable problem that *rendering* the mandelbrot set consists of?

I think if you actually try to write that problem down concretely you'll find that it's not actually non computable, or that it's not the same thing that people actually do when they render the mandelbrot set.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","Are you claiming you can literally render the Mandelbrot set? Infinite recursive calculation with real numbers is quite the trick.

A close approximation with bounded recursion and finite precision is computable, which is the point.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks","No I don't think you can render the mandelbrot set. And, more importantly, for any specific ""approximate"" rendering of it I don't think you can calculate an error bound on how close the value of each pixel is to what it should be for the true mandelbrot set.

That's what approximation is: it's a calculation in which the error on the result is calculable and controllable. That's not possible for a noncomputable problem.",misc
[R] Marcus Hutter's work on Universal Artificial Intelligence,"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https://www.amazon.co.uk/Universal-Artificial-Intelligence-Algorithmic-Probability/dp/3540221395) in 2005 and [one](https://www.amazon.co.uk/Introduction-Universal-Artificial-Intelligence-Robotics/dp/1032607025/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of ""universal intelligence"" in their paper [https://arxiv.org/abs/0712.3329](https://arxiv.org/abs/0712.3329)

In my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:

https://preview.redd.it/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49

Youtube: [https://www.youtube.com/watch?v=7TgOwMW\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https://www.youtube.com/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)

  
Outline:

I. Introduction

* 00:38 : Biography
* 01:45 : From Physics to AI
* 03:05 : Hutter Prize
* 06:25 : Overview of Universal Artificial Intelligence
* 11:10 : Technical outline

II. Universal Prediction

* 18:27 : Laplace’s Rule and Bayesian Sequence Prediction
* 40:54 : Different priors: KT estimator
* 44:39 : Sequence prediction for countable hypothesis class
* 53:23 : Generalized Solomonoff Bound (GSB)
* 57:56 : Example of GSB for uniform prior
* 1:04:24 : GSB for continuous hypothesis classes
* 1:08:28 : Context tree weighting
* 1:12:31 : Kolmogorov complexity
* 1:19:36 : Solomonoff Bound & Solomonoff Induction
* 1:21:27 : Optimality of Solomonoff Induction
* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines
* 1:28:37 : Large Language Models (LLMs)
* 1:37:07 : Using LLMs to emulate Solomonoff induction
* 1:41:41 : Loss functions
* 1:50:59 : Optimality of Solomonoff induction revisited
* 1:51:51 : Marvin Minsky

III. Universal Agents

* 1:52:42 : Recap and intro
* 1:55:59 : Setup
* 2:06:32 : Bayesian mixture environment
* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy
* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)
* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence
* 2:15:35 : AIXI explicit formula
* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)
* 2:33:09 : Multiagent setting
* 2:39:38 : Grain of Truth problem
* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria
* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs
* 2:56:13 : Outro: Brief philosophical remarks",",Personally I like ""An inexact result adequate for a given purpose"" as a definition for approximation (American Heritage Dictionary).

[Here](https://arxiv.org/pdf/0909.0801) is Hutter and colleagues on approximating AIXI (Hutter's AI framework built around Solomonoff induction):

> As the AIXI agent is only asymptotically computable, it is by no means an algorithmic solution to the general reinforcement learning problem. Rather it is best understood as a Bayesian optimality notion for decision making in general unknown environments. As such, its role in general AI research should be viewed in, for example, the same way the minimax and empirical risk minimisation principles are viewed in decision theory and statistical machine learning research. These principles define what is optimal behaviour if computational complexity is not an issue, and can provide important theoretical guidance in the design of practical algorithms.

You are certainly correct that we have desirable guarantees about algorithms that approximate the Mandelbrot set, but this doesn't mean it's impossible to usefully approximate things where such guarantees are harder to come by.",misc
Integrating Multiple AIs for Single Purposes—Seeking Research and Keywords [R],"I'm currently conducting research on AI and am curious if there are any studies discussing the use of multiple AI systems collaboratively for a single purpose. For example, using a language AI to assist a voice recognition AI to more accurately determine what sounds correspond to specific words. Are there specific keywords or phrases I should use to search for research in this area?","""ensemble machine learning model""",misc
Integrating Multiple AIs for Single Purposes—Seeking Research and Keywords [R],"I'm currently conducting research on AI and am curious if there are any studies discussing the use of multiple AI systems collaboratively for a single purpose. For example, using a language AI to assist a voice recognition AI to more accurately determine what sounds correspond to specific words. Are there specific keywords or phrases I should use to search for research in this area?",Multi-agent systems,misc
Integrating Multiple AIs for Single Purposes—Seeking Research and Keywords [R],"I'm currently conducting research on AI and am curious if there are any studies discussing the use of multiple AI systems collaboratively for a single purpose. For example, using a language AI to assist a voice recognition AI to more accurately determine what sounds correspond to specific words. Are there specific keywords or phrases I should use to search for research in this area?",r/learnmachinelearning,misc
Integrating Multiple AIs for Single Purposes—Seeking Research and Keywords [R],"I'm currently conducting research on AI and am curious if there are any studies discussing the use of multiple AI systems collaboratively for a single purpose. For example, using a language AI to assist a voice recognition AI to more accurately determine what sounds correspond to specific words. Are there specific keywords or phrases I should use to search for research in this area?",Have you considered looking into multi-agent systems or cooperative AI?,misc
Integrating Multiple AIs for Single Purposes—Seeking Research and Keywords [R],"I'm currently conducting research on AI and am curious if there are any studies discussing the use of multiple AI systems collaboratively for a single purpose. For example, using a language AI to assist a voice recognition AI to more accurately determine what sounds correspond to specific words. Are there specific keywords or phrases I should use to search for research in this area?",For your particular example I don't think an ensemble of models is really what you are looking for. It sounds more like you should be looking into multimodal embedding techniques for this. But could you elaborate on how exactly the language model would be assisting the voice model?,misc
Integrating Multiple AIs for Single Purposes—Seeking Research and Keywords [R],"I'm currently conducting research on AI and am curious if there are any studies discussing the use of multiple AI systems collaboratively for a single purpose. For example, using a language AI to assist a voice recognition AI to more accurately determine what sounds correspond to specific words. Are there specific keywords or phrases I should use to search for research in this area?",Your question is too vague. There are multiple models/concepts that can potentially match what you are describing,misc
Integrating Multiple AIs for Single Purposes—Seeking Research and Keywords [R],"I'm currently conducting research on AI and am curious if there are any studies discussing the use of multiple AI systems collaboratively for a single purpose. For example, using a language AI to assist a voice recognition AI to more accurately determine what sounds correspond to specific words. Are there specific keywords or phrases I should use to search for research in this area?","For chaining multiple models (and code) together, look at LangChain and no-code workflow tools like Flowise.",misc
Integrating Multiple AIs for Single Purposes—Seeking Research and Keywords [R],"I'm currently conducting research on AI and am curious if there are any studies discussing the use of multiple AI systems collaboratively for a single purpose. For example, using a language AI to assist a voice recognition AI to more accurately determine what sounds correspond to specific words. Are there specific keywords or phrases I should use to search for research in this area?",[Mixture of experts](https://en.wikipedia.org/wiki/Mixture_of_experts),misc
Integrating Multiple AIs for Single Purposes—Seeking Research and Keywords [R],"I'm currently conducting research on AI and am curious if there are any studies discussing the use of multiple AI systems collaboratively for a single purpose. For example, using a language AI to assist a voice recognition AI to more accurately determine what sounds correspond to specific words. Are there specific keywords or phrases I should use to search for research in this area?","Try the application streams of AAMAS:https://www.southampton.ac.uk/\~eg/AAMAS2023/forms/contents.htm#4F 

or have a look at Mike Wooldridge's book",misc
Integrating Multiple AIs for Single Purposes—Seeking Research and Keywords [R],"I'm currently conducting research on AI and am curious if there are any studies discussing the use of multiple AI systems collaboratively for a single purpose. For example, using a language AI to assist a voice recognition AI to more accurately determine what sounds correspond to specific words. Are there specific keywords or phrases I should use to search for research in this area?","This is a supposition but maybe what he wants to use the language model to correct. Lets say the audio model predicts something but semantically or the context does not make sense, the LLM detects that and modifies the prediction.",misc
[D] Dealing with conflicting training configurations in reference works.,"I am working on active learning for object detection, and I am at the stage where I need to setup my training configuration to run the experiments. I am not planning on rerunning the experiments of the other works because I don't have the compute, nor time. But I will still be comparing my results with theirs, and for that I will have to follow the training configurations used in those works.

The problem is different papers report different configurations, although they are comparing their results with each other. The paper that other methods usually compare themselves with is the [MI-AOD - CVPR21](https://github.com/yuantn/MI-AOD) paper, since it is the first AL method for object detection in CVPR. For RetinaNet, they train 26 epochs with LR of 0.001, stepping by 0.1 at epoch 20.

Then comes the [CVPR22 ](https://arxiv.org/abs/2204.07965)paper which uses the standard 1x schedule for RetinaNet training (12 epochs, 0.02 LR, and steps at epoch 8 and 11). Yet, they're comparing their results with the MI-AOD paper and it doesn't seem like they rerun the experiments with their settings because the mAP looks exactly the same as the one reported in the original. I can only judge it by looks because they only show the comparison as plots of mAP in each AL cycle and don't write down the values in the table. They also don't have the code published.

Then you have [PPAL - CVPR24](https://arxiv.org/abs/2211.11612) that claims to use the same config as MI-AOD, [but in their code](https://github.com/ChenhongyiYang/PPAL/blob/15875ed7a524675bc6daeba79b3716a0abca2b64/configs/voc_active_learning/al_train/retinanet_26e.py#L22) they're using an LR of 0.002 instead of 0.001 like they claim in the paper. And they also compare their results with the last two, despite differing configs and it doesn't seem like they rerun the experiments here either (again plots only, no table).

There are also several other works outside of CVPR, and they usually tend to follow the MI-AOD settings.

My question is, since the above three are all in CVPR, I would be required to at least compare my method with theirs, but how do I decide what config to use? Do I just follow the latest CVPR one as reported in their paper and use their reported results for the previous works for comparison?","Welcome to research. We spent hours digging through different active learning papers and reproducing them. Before spending too much time on your end send an email to all the first authors of the papers. Als them for clarification. People are usually very helpful as they want their research to be cited. Either you’re able to reproduce their results or you’re at least able to fully understand them. 
Also note that the model backbones and heads might differ between papers. 
If you want to publish it’s probably enough to show things work on the same datasets. We went further and noticed many methods won’t for anything outside the paper evaluated.",misc
[D] Dealing with conflicting training configurations in reference works.,"I am working on active learning for object detection, and I am at the stage where I need to setup my training configuration to run the experiments. I am not planning on rerunning the experiments of the other works because I don't have the compute, nor time. But I will still be comparing my results with theirs, and for that I will have to follow the training configurations used in those works.

The problem is different papers report different configurations, although they are comparing their results with each other. The paper that other methods usually compare themselves with is the [MI-AOD - CVPR21](https://github.com/yuantn/MI-AOD) paper, since it is the first AL method for object detection in CVPR. For RetinaNet, they train 26 epochs with LR of 0.001, stepping by 0.1 at epoch 20.

Then comes the [CVPR22 ](https://arxiv.org/abs/2204.07965)paper which uses the standard 1x schedule for RetinaNet training (12 epochs, 0.02 LR, and steps at epoch 8 and 11). Yet, they're comparing their results with the MI-AOD paper and it doesn't seem like they rerun the experiments with their settings because the mAP looks exactly the same as the one reported in the original. I can only judge it by looks because they only show the comparison as plots of mAP in each AL cycle and don't write down the values in the table. They also don't have the code published.

Then you have [PPAL - CVPR24](https://arxiv.org/abs/2211.11612) that claims to use the same config as MI-AOD, [but in their code](https://github.com/ChenhongyiYang/PPAL/blob/15875ed7a524675bc6daeba79b3716a0abca2b64/configs/voc_active_learning/al_train/retinanet_26e.py#L22) they're using an LR of 0.002 instead of 0.001 like they claim in the paper. And they also compare their results with the last two, despite differing configs and it doesn't seem like they rerun the experiments here either (again plots only, no table).

There are also several other works outside of CVPR, and they usually tend to follow the MI-AOD settings.

My question is, since the above three are all in CVPR, I would be required to at least compare my method with theirs, but how do I decide what config to use? Do I just follow the latest CVPR one as reported in their paper and use their reported results for the previous works for comparison?","I am currently doing quite a big batch of experiments and am also struggling with hparams for baselines, The issue that I found is that if you go down the ""we use the same set of parameters"" for each method route, which seems fair at first, is that the ranking can change a lot depending on what schedule you use. Imo the best thing to do is to do a parameter sweep for each method on a validation set and pick the best performing one. However, that is obviously very expensive. So I think just citing results from the other papers with their schedules (even if they are different) is probably still better than just forcing all methods to use the same one. Obviously there are caveats to that as well, for example for pre-training running more epochs is often better and comparing a 90 to a 300 epoch schedule isn't fair either, but if one method works better with slightly bigger LR and another one with smaller LR it doesn't really make sense to compare them with one uniform setup either.",misc
[D] Dealing with conflicting training configurations in reference works.,"I am working on active learning for object detection, and I am at the stage where I need to setup my training configuration to run the experiments. I am not planning on rerunning the experiments of the other works because I don't have the compute, nor time. But I will still be comparing my results with theirs, and for that I will have to follow the training configurations used in those works.

The problem is different papers report different configurations, although they are comparing their results with each other. The paper that other methods usually compare themselves with is the [MI-AOD - CVPR21](https://github.com/yuantn/MI-AOD) paper, since it is the first AL method for object detection in CVPR. For RetinaNet, they train 26 epochs with LR of 0.001, stepping by 0.1 at epoch 20.

Then comes the [CVPR22 ](https://arxiv.org/abs/2204.07965)paper which uses the standard 1x schedule for RetinaNet training (12 epochs, 0.02 LR, and steps at epoch 8 and 11). Yet, they're comparing their results with the MI-AOD paper and it doesn't seem like they rerun the experiments with their settings because the mAP looks exactly the same as the one reported in the original. I can only judge it by looks because they only show the comparison as plots of mAP in each AL cycle and don't write down the values in the table. They also don't have the code published.

Then you have [PPAL - CVPR24](https://arxiv.org/abs/2211.11612) that claims to use the same config as MI-AOD, [but in their code](https://github.com/ChenhongyiYang/PPAL/blob/15875ed7a524675bc6daeba79b3716a0abca2b64/configs/voc_active_learning/al_train/retinanet_26e.py#L22) they're using an LR of 0.002 instead of 0.001 like they claim in the paper. And they also compare their results with the last two, despite differing configs and it doesn't seem like they rerun the experiments here either (again plots only, no table).

There are also several other works outside of CVPR, and they usually tend to follow the MI-AOD settings.

My question is, since the above three are all in CVPR, I would be required to at least compare my method with theirs, but how do I decide what config to use? Do I just follow the latest CVPR one as reported in their paper and use their reported results for the previous works for comparison?","This method works surprisingly well for CV tasks from object detection to segmentation: https://openaccess.thecvf.com/content/CVPR2022/papers/Elezi_Not_All_Labels_Are_Equal_Rationalizing_the_Labeling_Costs_for_CVPR_2022_paper.pdf
And it’s easy to implement.",misc
[D] Dealing with conflicting training configurations in reference works.,"I am working on active learning for object detection, and I am at the stage where I need to setup my training configuration to run the experiments. I am not planning on rerunning the experiments of the other works because I don't have the compute, nor time. But I will still be comparing my results with theirs, and for that I will have to follow the training configurations used in those works.

The problem is different papers report different configurations, although they are comparing their results with each other. The paper that other methods usually compare themselves with is the [MI-AOD - CVPR21](https://github.com/yuantn/MI-AOD) paper, since it is the first AL method for object detection in CVPR. For RetinaNet, they train 26 epochs with LR of 0.001, stepping by 0.1 at epoch 20.

Then comes the [CVPR22 ](https://arxiv.org/abs/2204.07965)paper which uses the standard 1x schedule for RetinaNet training (12 epochs, 0.02 LR, and steps at epoch 8 and 11). Yet, they're comparing their results with the MI-AOD paper and it doesn't seem like they rerun the experiments with their settings because the mAP looks exactly the same as the one reported in the original. I can only judge it by looks because they only show the comparison as plots of mAP in each AL cycle and don't write down the values in the table. They also don't have the code published.

Then you have [PPAL - CVPR24](https://arxiv.org/abs/2211.11612) that claims to use the same config as MI-AOD, [but in their code](https://github.com/ChenhongyiYang/PPAL/blob/15875ed7a524675bc6daeba79b3716a0abca2b64/configs/voc_active_learning/al_train/retinanet_26e.py#L22) they're using an LR of 0.002 instead of 0.001 like they claim in the paper. And they also compare their results with the last two, despite differing configs and it doesn't seem like they rerun the experiments here either (again plots only, no table).

There are also several other works outside of CVPR, and they usually tend to follow the MI-AOD settings.

My question is, since the above three are all in CVPR, I would be required to at least compare my method with theirs, but how do I decide what config to use? Do I just follow the latest CVPR one as reported in their paper and use their reported results for the previous works for comparison?","What do you think about batch size? Can I compare my method that I ran with a batch size of 8 with gradient accumulation of 2, with the other works that use batch size of 16 (usually 2 distributed over 8 GPUs)? RetinaNet has batch norm layers, so it is affected by batch size. Although it might be negligible.

I am having trouble with VRAM too (I have really limited compute, self-funded, and didn't write my method to be compatible with multi-GPU training), so I am wondering if this is possible.",misc
[D] Dealing with conflicting training configurations in reference works.,"I am working on active learning for object detection, and I am at the stage where I need to setup my training configuration to run the experiments. I am not planning on rerunning the experiments of the other works because I don't have the compute, nor time. But I will still be comparing my results with theirs, and for that I will have to follow the training configurations used in those works.

The problem is different papers report different configurations, although they are comparing their results with each other. The paper that other methods usually compare themselves with is the [MI-AOD - CVPR21](https://github.com/yuantn/MI-AOD) paper, since it is the first AL method for object detection in CVPR. For RetinaNet, they train 26 epochs with LR of 0.001, stepping by 0.1 at epoch 20.

Then comes the [CVPR22 ](https://arxiv.org/abs/2204.07965)paper which uses the standard 1x schedule for RetinaNet training (12 epochs, 0.02 LR, and steps at epoch 8 and 11). Yet, they're comparing their results with the MI-AOD paper and it doesn't seem like they rerun the experiments with their settings because the mAP looks exactly the same as the one reported in the original. I can only judge it by looks because they only show the comparison as plots of mAP in each AL cycle and don't write down the values in the table. They also don't have the code published.

Then you have [PPAL - CVPR24](https://arxiv.org/abs/2211.11612) that claims to use the same config as MI-AOD, [but in their code](https://github.com/ChenhongyiYang/PPAL/blob/15875ed7a524675bc6daeba79b3716a0abca2b64/configs/voc_active_learning/al_train/retinanet_26e.py#L22) they're using an LR of 0.002 instead of 0.001 like they claim in the paper. And they also compare their results with the last two, despite differing configs and it doesn't seem like they rerun the experiments here either (again plots only, no table).

There are also several other works outside of CVPR, and they usually tend to follow the MI-AOD settings.

My question is, since the above three are all in CVPR, I would be required to at least compare my method with theirs, but how do I decide what config to use? Do I just follow the latest CVPR one as reported in their paper and use their reported results for the previous works for comparison?","I have read it and referenced it in my literature review. It's a nice paper. But they don't test it on RetinaNet, just SSD, so no reference configs. I want to test on RetinaNet. Might try SSD too if I have time later.",misc
[D] Dealing with conflicting training configurations in reference works.,"I am working on active learning for object detection, and I am at the stage where I need to setup my training configuration to run the experiments. I am not planning on rerunning the experiments of the other works because I don't have the compute, nor time. But I will still be comparing my results with theirs, and for that I will have to follow the training configurations used in those works.

The problem is different papers report different configurations, although they are comparing their results with each other. The paper that other methods usually compare themselves with is the [MI-AOD - CVPR21](https://github.com/yuantn/MI-AOD) paper, since it is the first AL method for object detection in CVPR. For RetinaNet, they train 26 epochs with LR of 0.001, stepping by 0.1 at epoch 20.

Then comes the [CVPR22 ](https://arxiv.org/abs/2204.07965)paper which uses the standard 1x schedule for RetinaNet training (12 epochs, 0.02 LR, and steps at epoch 8 and 11). Yet, they're comparing their results with the MI-AOD paper and it doesn't seem like they rerun the experiments with their settings because the mAP looks exactly the same as the one reported in the original. I can only judge it by looks because they only show the comparison as plots of mAP in each AL cycle and don't write down the values in the table. They also don't have the code published.

Then you have [PPAL - CVPR24](https://arxiv.org/abs/2211.11612) that claims to use the same config as MI-AOD, [but in their code](https://github.com/ChenhongyiYang/PPAL/blob/15875ed7a524675bc6daeba79b3716a0abca2b64/configs/voc_active_learning/al_train/retinanet_26e.py#L22) they're using an LR of 0.002 instead of 0.001 like they claim in the paper. And they also compare their results with the last two, despite differing configs and it doesn't seem like they rerun the experiments here either (again plots only, no table).

There are also several other works outside of CVPR, and they usually tend to follow the MI-AOD settings.

My question is, since the above three are all in CVPR, I would be required to at least compare my method with theirs, but how do I decide what config to use? Do I just follow the latest CVPR one as reported in their paper and use their reported results for the previous works for comparison?",I mean that's the same effective batch size so I think that's totally fine. Yeah for one method you'll update the batch norm mean/std twice as often (with half the sample count each) but I cannot imagine that being too much of an issue. I gues you can't just switch to layer norm?,misc
[D] Dealing with conflicting training configurations in reference works.,"I am working on active learning for object detection, and I am at the stage where I need to setup my training configuration to run the experiments. I am not planning on rerunning the experiments of the other works because I don't have the compute, nor time. But I will still be comparing my results with theirs, and for that I will have to follow the training configurations used in those works.

The problem is different papers report different configurations, although they are comparing their results with each other. The paper that other methods usually compare themselves with is the [MI-AOD - CVPR21](https://github.com/yuantn/MI-AOD) paper, since it is the first AL method for object detection in CVPR. For RetinaNet, they train 26 epochs with LR of 0.001, stepping by 0.1 at epoch 20.

Then comes the [CVPR22 ](https://arxiv.org/abs/2204.07965)paper which uses the standard 1x schedule for RetinaNet training (12 epochs, 0.02 LR, and steps at epoch 8 and 11). Yet, they're comparing their results with the MI-AOD paper and it doesn't seem like they rerun the experiments with their settings because the mAP looks exactly the same as the one reported in the original. I can only judge it by looks because they only show the comparison as plots of mAP in each AL cycle and don't write down the values in the table. They also don't have the code published.

Then you have [PPAL - CVPR24](https://arxiv.org/abs/2211.11612) that claims to use the same config as MI-AOD, [but in their code](https://github.com/ChenhongyiYang/PPAL/blob/15875ed7a524675bc6daeba79b3716a0abca2b64/configs/voc_active_learning/al_train/retinanet_26e.py#L22) they're using an LR of 0.002 instead of 0.001 like they claim in the paper. And they also compare their results with the last two, despite differing configs and it doesn't seem like they rerun the experiments here either (again plots only, no table).

There are also several other works outside of CVPR, and they usually tend to follow the MI-AOD settings.

My question is, since the above three are all in CVPR, I would be required to at least compare my method with theirs, but how do I decide what config to use? Do I just follow the latest CVPR one as reported in their paper and use their reported results for the previous works for comparison?",I feel switching to layer norm is more significant of a change than accumulation.,misc
[D] How to train very shallow (dot product) networks with huge embeddings on a GPU cluster?,"In the olden days we used dozens of parameter servers and hundreds of CPU machines to train such heavy embedding light compute models and achieved impressive throughput. Nowadays with GPU clusters with high speed NVlink, looks like the throughput actually gets much worse. Of course I am talking about a dozen or so GPU machines each with say 8 A100. The tensor core utilization is very minimal (< 1%), but the GPUs are very busy due to all2all communication. I am trying to wrap my head around what the bottleneck maybe with the latter setup, is it simply that all2all (or ring all reduce etc) is intrinsically slower than parameter server when the number of parameters gets large, no matter how fast the nvlink is?",Maybe it is limited before reaching the NVMe? Its FP16 spec is 312 TFLOPS while  HBM's bandwith is \~2TBytes/second.  If your matrix is so large that you have to fill its memory with parameters then it will perform well below its FLOPs limit.,general_qa
[D] How to train very shallow (dot product) networks with huge embeddings on a GPU cluster?,"In the olden days we used dozens of parameter servers and hundreds of CPU machines to train such heavy embedding light compute models and achieved impressive throughput. Nowadays with GPU clusters with high speed NVlink, looks like the throughput actually gets much worse. Of course I am talking about a dozen or so GPU machines each with say 8 A100. The tensor core utilization is very minimal (< 1%), but the GPUs are very busy due to all2all communication. I am trying to wrap my head around what the bottleneck maybe with the latter setup, is it simply that all2all (or ring all reduce etc) is intrinsically slower than parameter server when the number of parameters gets large, no matter how fast the nvlink is?","One idea is to build a parameter server with 6400G Infiniband. And then you basically let one of the GPU be the node driver to combine 8 GPUs, and then each node is connected to the parameter server at 400G.

Basically intranode is way faster than internode. 

If all nodes are already connected via 1600G Infiniband, then I would repurpose those slots on the switches and adapter cards. Though you need 400G cards to get up to 6400G.",general_qa
[D] How to train very shallow (dot product) networks with huge embeddings on a GPU cluster?,"In the olden days we used dozens of parameter servers and hundreds of CPU machines to train such heavy embedding light compute models and achieved impressive throughput. Nowadays with GPU clusters with high speed NVlink, looks like the throughput actually gets much worse. Of course I am talking about a dozen or so GPU machines each with say 8 A100. The tensor core utilization is very minimal (< 1%), but the GPUs are very busy due to all2all communication. I am trying to wrap my head around what the bottleneck maybe with the latter setup, is it simply that all2all (or ring all reduce etc) is intrinsically slower than parameter server when the number of parameters gets large, no matter how fast the nvlink is?",I also just realized google tpu has sparse core that aims precisely to solve this issue. Similar with Grace hopper. But if it’s just dot product the tensorcore can be as weak as a CPU. Looks like nobody cares to fill this void,general_qa
[D] How to train very shallow (dot product) networks with huge embeddings on a GPU cluster?,"In the olden days we used dozens of parameter servers and hundreds of CPU machines to train such heavy embedding light compute models and achieved impressive throughput. Nowadays with GPU clusters with high speed NVlink, looks like the throughput actually gets much worse. Of course I am talking about a dozen or so GPU machines each with say 8 A100. The tensor core utilization is very minimal (< 1%), but the GPUs are very busy due to all2all communication. I am trying to wrap my head around what the bottleneck maybe with the latter setup, is it simply that all2all (or ring all reduce etc) is intrinsically slower than parameter server when the number of parameters gets large, no matter how fast the nvlink is?",Train models independently and perform some averages at scheduled intervals,general_qa
[D] How to train very shallow (dot product) networks with huge embeddings on a GPU cluster?,"In the olden days we used dozens of parameter servers and hundreds of CPU machines to train such heavy embedding light compute models and achieved impressive throughput. Nowadays with GPU clusters with high speed NVlink, looks like the throughput actually gets much worse. Of course I am talking about a dozen or so GPU machines each with say 8 A100. The tensor core utilization is very minimal (< 1%), but the GPUs are very busy due to all2all communication. I am trying to wrap my head around what the bottleneck maybe with the latter setup, is it simply that all2all (or ring all reduce etc) is intrinsically slower than parameter server when the number of parameters gets large, no matter how fast the nvlink is?",Ok so sounds like the total bandwidth of the 100 ps to 500 trainer setup is still a lot higher than the total hbm bandwidth between the gpu nodes (around 16). We are really bottlenecked by network rather than compute. I wonder if nvidia has any solution for this situation or we basically need to spin up our own?,general_qa
