{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"I'm interested in this. The trend from GPT4 to GPT4-Turbo, to this seems like they're making the flagship models smaller. Maybe they've found a good path to distill the alignment into progressively smaller models.\n\nIf it was something like speculative decoding, quantization, or hardware improvements, you'd think that they'd go back and apply it to the older models to save on serving costs.","category":"misc"}
{"instruction":"Can one use squared inverse of KL divergence as another divergence metric? [D]","context":"I came across this doubt (might be dumb), but it would be great if someone can throw some light on this:\n\nThe KL Divergence between two distributions p and q is defined as : `$D_{KL}(p || q) = E_{p}[\\log \\frac{p}{q}]$`\n\ndepending on the order of p and q, the divergence is mode seeking or mode covering.\n\nHowever, can one use `$\\frac{-1}{D_{KL}(p || q)}$` as a divergence metric?\n\nOr maybe not a divergence metric (strictly speaking), but something to measure similarity\/dissimilarity between the two distributions?\n\n**Edit**:\n\nit is definitely not a divergence as `-1\/KL(p,q) <= 0` also as pointed in the discussion, `1\/KL(p,p) = +oo`.\n\nHowever, I am thinking it from this point: if `KL(p,q)` is decreasing `=>` `1\/KL(p,q)` is increasing `=>` `-1\/KL(p,q)` is decreasing. Although, `-1\/KL(p,q)` is unbounded from below hence can reach `-oo`. Question is, does the above equivalence, make `-1\/KL(p,q)` useful as a metric for any application. Or is it considered somewhere in any literature.","response":"[deleted]","category":"misc"}
{"instruction":"[D] Seminal papers list since 2018 that will be considered cannon in the future","context":"Hi there,\n\nA recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML\/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  \n\nWhere should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","response":"They also moved the layer norm","category":"misc"}
{"instruction":"[D] Real chances to be accepted in NeurIPS 2024 - Other conferences","context":"Hey!\n\nThis is my first time submitting to NeurIPS.\n\nDoes anyone know when the reviews are visible to the authors? August, or is it possible that earlier? If we have really bad reviews... The best thing is to exit the submission path, right? In that case, which alternatives do you recommend on those dates?\n\nMy topic is NN reliability, but I am always underconfident about my research and I always think that it is not enough, more if I think in a conference as Neurips. Do you think that everybody submits good papers or is there a large quantity of rubbish papers? I read a lot of bad opinions here about the reviewing process... So, I am a little afraid.\n\nThis year, there are 20000ish submissions. So, I don't know what to do, if continue the submission or submit to another conference. As the gap that I am filling is clear, I am sure that others are covering that gap and submitting it to NeurIPS. Is there any other conference that outputs the results first than NeurIPS? I am trying to think in a smart way. So hard to be a researcher...\n\nThank you!","response":"The major 3 conferences (NeurIPS, ICLR, ICML) have deadlines and notification dates aligned in such a way that if you were rejected from say NeurIPS, you will be able to resubmit to the next one (ICLR) very soon. And you will have some idea of the result based on your reviews, so you will have some time to incorporate feedback and improve your paper (which you should be doing during rebuttal anyway).\n\nI can't judge your paper unless I've reviewed it, but you will acquire a feel for paper quality over time. I don't think you should submit to a different conference - if the paper's good it will have a good chance, and if it's decent it will have a decent chance, and there's really no point in submitting a work you know is subpar to any conference. Generally our team always submits to the top 3 and shoot lower only if it is rejected all 3 times.","category":"misc"}
{"instruction":"[P] SimpleGEMM: Fast and minimal tensor core matrix multiplication in CUDA","context":"Hello all! Sharing my side project here: [https:\/\/github.com\/andylolu2\/simpleGEMM](https:\/\/github.com\/andylolu2\/simpleGEMM) !\n\nThis is an *extremely* minimalistic but fast implementation of matrix multiplication in CUDA. The source code is a single, 200-line CUDA\/C++ file which implements fp16 tensor core matrix multiplication, optimised for Turing (SM75) architecture. The goal is to:\n\n1. Write a matmul kernel that does not sacrifice performance. In fact, it's faster than PyTorch\/CuBLAS if you [test it on a T4 in Colab](https:\/\/colab.research.google.com\/github\/andylolu2\/simpleGEMM\/blob\/master\/colab\/simpleGEMM.ipynb)!\n2. Make it hackable for new purposes. For example if you want to add a new custom prologue (e.g. Matmul + some reduction), just go to line 186, add your code, and recompile! Full flexibility with no C++ templating shenanigans.\n3. Keep it as simple as possible. Hopefully someone learning CUDA will find this useful!\n\nOf course, I didn't implement *everything* from scratch. Most of the this builds upon Nvidia CUTLASS's new CuTe interface for things like memory layout, data copying and using tensor core instructions.\n\n*Aside:*\n\n*Why not OpenAI Triton? I love triton, but sometimes it's hard to get the extra 10-20% performance if you are doing something off its main optimisation path. In fact,* [*triton's matmul for Turing GPUs is quite slow*](https:\/\/github.com\/openai\/triton\/issues\/189) *(because they mainly optimise for SM80+). I just enjoy having full control over the hardware, knowing that if I have infinite time I can squeeze very single bit of performance out.*","response":"Just Saw this bit\n\n> Assumes the inputs are in row-major layout. (Though you probably only want to use a row-major layout anyway, as other combinations are 10-30% slower.)\n\nCan you elaborate on why col-major would be slower? I understand this one is built for row major, but couldn\u2019t you adjust the algorithm for col major and get the same performance? I.e. there\u2019s nothing inherently worse with col major, right?","category":"misc"}
{"instruction":"[R] The Platonic Representation Hypothesis","context":"arxiv: [https:\/\/arxiv.org\/pdf\/2405.07987](https:\/\/arxiv.org\/pdf\/2405.07987)  \nProject page: [https:\/\/phillipi.github.io\/prh\/](https:\/\/phillipi.github.io\/prh\/)  \ngithub: [https:\/\/github.com\/minyoungg\/platonic-rep\/](https:\/\/github.com\/minyoungg\/platonic-rep\/)\n\nInteresting positional paper on the convergence of self-supervised, multi-modal representation learning.","response":"cool","category":"misc"}
{"instruction":"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","context":"\\*Update\\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.\n\nI think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.\n\ntl;dr Use the parallel scan\\[1\\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\\\sum\\_{i=0}\\^{j-1} k(i)\\_a\\^n q(j)\\_b\\^m v(i)$$ and the normalization $\\\\sum\\_{i=0}\\^{j-1} k(i)\\_a\\^n q(j)\\_b\\^m$. k(i)\\_a\\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\\_b\\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \\* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.\n\n# Background\n\nI was inspired to think about this because I was implementing MAMBA\\[2\\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \\[a\\_1, a\\_2, a\\_3, a\\_4, ...\\]. You can compute all partial sums by first adding a\\_i to a\\_{i -1}, where a\\_{-1} is zero, and generally a\\_{-n} is defined to be zero. Then take the result, call it r = \\[a\\_1, a\\_1+a\\_2, a\\_2 + a\\_3, ...\\], and compute r\\_i + r\\_{i-2}, which gives \\[a\\_1, a\\_1+a\\_2, a\\_1+a\\_2+a\\_3, ...\\]. The first 4 partial sums are already complete. The next step would be r\\_i + r\\_{i-2\\*\\*2}, and the next step, just increase the power of 2 until i-2\\*\\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.\n\n# The Meat of It\n\nIn the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.\n\nLet's assume we have a tensor x of shape (sequence\\_length, embedding\\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\\[:,i\\]\\*\\*n)\\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\\[y\\[0,:\\], y\\[0,:\\]+y\\[1,:\\], ...\\]. Now multiply the result by q\\[:,j\\]\\*\\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)\\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\\\sum\\_{n, m} A\\_{n, m} q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)\\*v).\n\nWhat is left is to find the Taylor series coefficients A\\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\\\cdot k$ in place of $q\\[:,j,:\\] \\\\cdot k\\[:,i,:\\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\\\cdot k) = 1 + (q \\\\cdot k) + (q \\\\cdot k)\\*\\*2 \/ 2! + ... + (q \\\\cdot k)\\*\\*n \/ n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\\\cdot k)\\*\\*n \/n! for every n. It can be done but I'm not going to do it. Just assume that A\\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\\\sum\\_{n, m} A\\_{n, m} x\\[:,j\\]\\*\\*m \\* ParallelPartialSum((x\\[:,i\\]\\*\\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:\n\n$$  \n(\\\\sum\\_{n, m} A\\_{n, m} q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)\\*v)) \/ (\\\\sum\\_{n, m} A\\_{n, m} q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)))  \n$$\n\nWhere again, A\\_{n, m} are the Taylor series coefficients for exp( q \\\\cdot k).\n\n# Take-Aways\n\nOne big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.\n\nNon-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.\n\n\\[1\\] [https:\/\/en.wikipedia.org\/wiki\/Prefix\\_sum](https:\/\/en.wikipedia.org\/wiki\/Prefix_sum)\n\n\\[2\\] [https:\/\/arxiv.org\/abs\/2312.00752](https:\/\/arxiv.org\/abs\/2312.00752)\n\n# Update\n\nActually there is a better algorithm for the parallel scan given in the wiki link above\\[1\\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.\n\n# Update 2\n\n@[Lajamerr\\_Mittesdine](https:\/\/www.reddit.com\/user\/Lajamerr_Mittesdine\/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\\_Mittesdine](https:\/\/www.reddit.com\/user\/Lajamerr_Mittesdine\/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.\n\n    import numpy as np\n    \n    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.\n    def parallel_partial_sum(arr): \n        \"\"\"Parallel scan (prefix sum) implementation.\"\"\"\n        n = len(arr)\n        steps = np.ceil(np.log2(n))\n        \n        for i in range(steps):\n            # check if this is the numerator or denominator\n            if len(arr.shape)==2:            \n                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)\n            else:\n                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)\n    \n        return arr\n    \n    def compute_taylor_basis_function(q, k, v, n, m, i, j):\n        \"\"\"Compute a Taylor basis function for given powers n and m.\"\"\"\n        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise\n        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise\n        if len(v.shape) == 2:\n            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast\n            q_power = np.expand_dims(q_power, axis=-1)\n        partial_sum_kv = parallel_partial_sum(k_power * v)\n        basis_function = q_power * partial_sum_kv\n        return basis_function\n    \n    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):\n        \"\"\"Compute the causal self-attention using Taylor series approximation.\"\"\"\n        attention_numerator = np.zeros_like(v)\n        attention_denominator = np.zeros_like(v[:,0])\n    \n        for n in range(max_n + 1):\n            for m in range(max_m + 1):\n                for j in range(q.shape[-1]):\n                    for i in range(k.shape[-1]):\n                        # note, either i or j loop can be removed because basis functions can be computed in parallel\n                        A_nmij = 1.0  # Simplified coefficient for illustration\n                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)\n                        attention_numerator += A_nmij * basis_function\n                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)\n                        attention_denominator += A_nmij * normalization_basis_function\n        \n        attention_denominator = np.expand_dims(attention_denominator, axis=-1)\n        attention = attention_numerator \/ attention_denominator\n        return attention\n    \n    # Example usage\n    sequence_length = 10\n    embedding_dim = 4\n    \n    # Randomly initialize q, k, v tensors\n    q = np.random.rand(sequence_length, embedding_dim)\n    k = np.random.rand(sequence_length, embedding_dim)\n    v = np.random.rand(sequence_length, embedding_dim)\n    \n    # Compute the causal self-attention\n    attention_output = compute_causal_self_attention(q, k, v)\n    \n    print(\"Causal Self-Attention Output:\")\n    print(attention_output)","response":"Which part is the most confusing? Maybe I can rewrite that part.","category":"misc"}
{"instruction":"[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis","context":"https:\/\/arxiv.org\/pdf\/2405.08790","response":"The real innovation isn't going to happen until researchers determine how KAN interacts with contemporary naming practices for models. KANba? KANsformer? We don't know yet how those names for models will perform.\n\n(EDIT: I am legitimately surprised they didn't call this [KANvolution](https:\/\/www.reddit.com\/r\/programming\/comments\/1crv9td\/the_first_convolutionalkans\/)! C'mon guys!)","category":"misc"}
{"instruction":"[D] Confusion: What does the y-axis of calibration curves represent?","context":"I was going through the paper [On Calibration of Modern Neural Networks](https:\/\/arxiv.org\/pdf\/1706.04599), and saw that the authors used the following definition for the \"fraction of positives\" which shows up on the y-axis of the calibration curve. \n\nhttps:\/\/preview.redd.it\/pq9eqj16bq0d1.png?width=944&format=png&auto=webp&s=be71a70ff0e6ba77b672ca9b4315c6e7ba3d1011\n\nFrom my understanding, the above equation is calculating the average accuracy in the bin m.\n\nHowever, my original understanding about the \"fraction of positives\" was that it was the proportion of actual positive outcomes within the bin m, which intuitively makes more sense in the context of calibration curves. I have also seen this interpretation of calibration curves.\n\nCan you fill in the hole in my knowledge?","response":"What's the difference between \"fraction of positives\" vs \"accuracy\" in your understanding? Accuracy means \"fraction of _correct answers_\" in this case, is that what you mean or does \"positive\" means \"y-hat == 1\" to you?  \n  \nI'd guess the paper's definition makes most sense given the multi class models that are analysing - the result is not just 0\/1 outcomes, there are potentially hundreds of classes being assigned as the outcome.","category":"general_qa"}
{"instruction":"[D] ACL 2024 Decisions","context":"Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? \ud83c\uddf9\ud83c\udded\ud83d\udc18","response":"Same here, what was the recommendation from meta reviewer?","category":"misc"}
{"instruction":"[D] Are PyTorch high-level frameworks worth using?","context":"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.\n\nWould these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?\n\nIf you think they're useful, let me know which one you'd recommend.","response":"Don't use high-level frameworks. Spend time on the basics and write your own stuff. You'll benefit in a long term.","category":"misc"}
{"instruction":"[P] Needle in a Needlestack (NIAN)","context":"**Code**: [https:\/\/github.com\/llmonpy\/needle-in-a-needlestack](https:\/\/github.com\/llmonpy\/needle-in-a-needlestack)\n\n**Website**: [https:\/\/nian.llmonpy.ai\/](https:\/\/nian.llmonpy.ai\/)\n\n**Description**:\n\n>Needle in a haystack (NIAH) has been a wildly popular test for evaluating how effectively LLMs can pay attention to the content in their context window. As LLMs have improved NIAH has become too easy. **Needle in a Needlestack** (**NIAN**) is a new, more challenging benchmark. Even GPT-4-turbo struggles with this benchmark.\n\n>NIAN creates a list of limericks from a large database of limericks and asks a question about a specific limerick that has been placed at a test location. Each test will typically use 5 to 10 test limericks placed at 5 to 10 locations in the prompt. Each test is repeated 2-10 times.","response":"> As LLMs have improved NIAH has become too easy\n\nIMO the test was flawed from the beginning. It only tests the attention portion, not the actual usefulness of the fact that it attended to those tokens. NIAN kinda proves what a lot of people were saying regarding NIAH.","category":"misc"}
{"instruction":"[Discussion] MICCAI 2024 decisions","context":"Hi all,\n\nI thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.\n\nI got a rebuttal invitation for an application paper and all the reviewers mentioned \"lack of technical novelty\" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","response":"Unfortunately I think adding a new row\/column in a table is prohibited. Your best bet is to explain to the best of your ability why those comparisons weren't initialling in the manuscript.","category":"misc"}
{"instruction":"[D] Computer Vision Tooling - Multistage data processing","context":"At my line of work, I have to take a picture, detect\/segment tens up to hundreds of points of interest and summarize it's sizes. \n\nI have ML model that is mostly precise but makes few stupid mistakes so it's not perfectly reliable (as expected) and occasionaly needs manual intervention that corrects it's output.\n\nCurrently, I use \n\n1) CVAT to upload an image, run prediction and correct\/approve the results. Then I download the image and apply\n\n2) Python script to run postprocessing\n\nThis workflow is good for few projects and few relatively-savy users but as time passes projects pile up and team grows. At the moment, there are several different tasks, each needs a bit different posprocessing and more and more people working with it.\n\n**Do you know any software that can help me to implement this workflow without manually showeling data from CVAT\u00a0to scripts?** \n\nI looked around if it's possible to extend CVAT but it's meant as annotation tool not a link in a production chain so I didn't found anything (appart plugging my own models into it). As an alternative I was thinking about writing my own solution. I would be able to write a backend but I cannot write the frontend part. I don't know javascript and searching through github for any decent frontend supporting tools (like brushes for segmentation) and label handling(fix mislabeled stuff etc)   led nowhere so I gave up thinking about it.","response":"Something like parchyderm + labelstudio?\n\nNever used it yet, but I've had similar questions and that seemed the best answer to me.\n\nPachyderm manages the data processing, versionning and lineage.\n\nLabelstudio is a super versatile data annotation toolbox.\n\nAnd there is some integration between them.\n\nCVAT is great but from little experience I had with it, it does not do well with complex pipelines, annotations type and data versionning\/lineage issues. However its interface is better from what I remember.","category":"misc"}
{"instruction":"[D] GPT-4o \"natively\" multi-modal, what does this actually mean?","context":"What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?\n\nE.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system \"self-select\" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","response":"Does this mean that there's an inductive bias where each exemplar of video\/audio + text only happens within that time context or is it continually training in streams of some sort?","category":"general_qa"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"Reminds me of the drama behind the MAMBA paper's peer review process: \n\nhttps:\/\/youtu.be\/N6Piou4oYx8?si=7o8jFOJcFslTjjOC&t=1664\n\n>> Rejected by peer reviewers .... now this is a really dumb reason to reject a paper because the long range arena [the task the reviewer was complaining about] is a completely different task to language modeling, and Mamba is specifically a language model.","category":"misc"}
{"instruction":"[D] ACL 2024 Decisions","context":"Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? \ud83c\uddf9\ud83c\udded\ud83d\udc18","response":"Did it get in with 2?","category":"misc"}
{"instruction":"Nanogpt alternative [D]","context":"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model\n\nAlso, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? \n\nThanks","response":"Alright. Do different llms have different input data format requirements?","category":"misc"}
{"instruction":"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","context":"\\*Update\\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.\n\nI think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.\n\ntl;dr Use the parallel scan\\[1\\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\\\sum\\_{i=0}\\^{j-1} k(i)\\_a\\^n q(j)\\_b\\^m v(i)$$ and the normalization $\\\\sum\\_{i=0}\\^{j-1} k(i)\\_a\\^n q(j)\\_b\\^m$. k(i)\\_a\\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\\_b\\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \\* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.\n\n# Background\n\nI was inspired to think about this because I was implementing MAMBA\\[2\\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \\[a\\_1, a\\_2, a\\_3, a\\_4, ...\\]. You can compute all partial sums by first adding a\\_i to a\\_{i -1}, where a\\_{-1} is zero, and generally a\\_{-n} is defined to be zero. Then take the result, call it r = \\[a\\_1, a\\_1+a\\_2, a\\_2 + a\\_3, ...\\], and compute r\\_i + r\\_{i-2}, which gives \\[a\\_1, a\\_1+a\\_2, a\\_1+a\\_2+a\\_3, ...\\]. The first 4 partial sums are already complete. The next step would be r\\_i + r\\_{i-2\\*\\*2}, and the next step, just increase the power of 2 until i-2\\*\\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.\n\n# The Meat of It\n\nIn the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.\n\nLet's assume we have a tensor x of shape (sequence\\_length, embedding\\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\\[:,i\\]\\*\\*n)\\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\\[y\\[0,:\\], y\\[0,:\\]+y\\[1,:\\], ...\\]. Now multiply the result by q\\[:,j\\]\\*\\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)\\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\\\sum\\_{n, m} A\\_{n, m} q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)\\*v).\n\nWhat is left is to find the Taylor series coefficients A\\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\\\cdot k$ in place of $q\\[:,j,:\\] \\\\cdot k\\[:,i,:\\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\\\cdot k) = 1 + (q \\\\cdot k) + (q \\\\cdot k)\\*\\*2 \/ 2! + ... + (q \\\\cdot k)\\*\\*n \/ n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\\\cdot k)\\*\\*n \/n! for every n. It can be done but I'm not going to do it. Just assume that A\\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\\\sum\\_{n, m} A\\_{n, m} x\\[:,j\\]\\*\\*m \\* ParallelPartialSum((x\\[:,i\\]\\*\\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:\n\n$$  \n(\\\\sum\\_{n, m} A\\_{n, m} q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)\\*v)) \/ (\\\\sum\\_{n, m} A\\_{n, m} q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)))  \n$$\n\nWhere again, A\\_{n, m} are the Taylor series coefficients for exp( q \\\\cdot k).\n\n# Take-Aways\n\nOne big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.\n\nNon-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.\n\n\\[1\\] [https:\/\/en.wikipedia.org\/wiki\/Prefix\\_sum](https:\/\/en.wikipedia.org\/wiki\/Prefix_sum)\n\n\\[2\\] [https:\/\/arxiv.org\/abs\/2312.00752](https:\/\/arxiv.org\/abs\/2312.00752)\n\n# Update\n\nActually there is a better algorithm for the parallel scan given in the wiki link above\\[1\\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.\n\n# Update 2\n\n@[Lajamerr\\_Mittesdine](https:\/\/www.reddit.com\/user\/Lajamerr_Mittesdine\/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\\_Mittesdine](https:\/\/www.reddit.com\/user\/Lajamerr_Mittesdine\/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.\n\n    import numpy as np\n    \n    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.\n    def parallel_partial_sum(arr): \n        \"\"\"Parallel scan (prefix sum) implementation.\"\"\"\n        n = len(arr)\n        steps = np.ceil(np.log2(n))\n        \n        for i in range(steps):\n            # check if this is the numerator or denominator\n            if len(arr.shape)==2:            \n                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)\n            else:\n                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)\n    \n        return arr\n    \n    def compute_taylor_basis_function(q, k, v, n, m, i, j):\n        \"\"\"Compute a Taylor basis function for given powers n and m.\"\"\"\n        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise\n        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise\n        if len(v.shape) == 2:\n            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast\n            q_power = np.expand_dims(q_power, axis=-1)\n        partial_sum_kv = parallel_partial_sum(k_power * v)\n        basis_function = q_power * partial_sum_kv\n        return basis_function\n    \n    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):\n        \"\"\"Compute the causal self-attention using Taylor series approximation.\"\"\"\n        attention_numerator = np.zeros_like(v)\n        attention_denominator = np.zeros_like(v[:,0])\n    \n        for n in range(max_n + 1):\n            for m in range(max_m + 1):\n                for j in range(q.shape[-1]):\n                    for i in range(k.shape[-1]):\n                        # note, either i or j loop can be removed because basis functions can be computed in parallel\n                        A_nmij = 1.0  # Simplified coefficient for illustration\n                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)\n                        attention_numerator += A_nmij * basis_function\n                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)\n                        attention_denominator += A_nmij * normalization_basis_function\n        \n        attention_denominator = np.expand_dims(attention_denominator, axis=-1)\n        attention = attention_numerator \/ attention_denominator\n        return attention\n    \n    # Example usage\n    sequence_length = 10\n    embedding_dim = 4\n    \n    # Randomly initialize q, k, v tensors\n    q = np.random.rand(sequence_length, embedding_dim)\n    k = np.random.rand(sequence_length, embedding_dim)\n    v = np.random.rand(sequence_length, embedding_dim)\n    \n    # Compute the causal self-attention\n    attention_output = compute_causal_self_attention(q, k, v)\n    \n    print(\"Causal Self-Attention Output:\")\n    print(attention_output)","response":"then show us your results. Also you probably mean 1B parameters as in 1 billion rather than 1M as in million","category":"misc"}
{"instruction":"[D] ACL 2024 Decisions","context":"Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? \ud83c\uddf9\ud83c\udded\ud83d\udc18","response":"Rejected. Is there anyone with an accept on 3?","category":"misc"}
{"instruction":"[P] DARWIN - open-sourced Devin alternative","context":"**\ud83d\ude80 Introducing DARWIN - Open Sourced, AI Software Engineer Intern! \ud83e\udd16**  \n DARWIN is an AI Software Intern at your command. It is equipped with capabilities to assist you in the way you build and deploy code. With internet access, DARWIN relies on updated knowledge to write codes and execute them. And if in case it gets stuck at an error, DARWIN tries to solve it by visiting discussions and forums. And what\u2019s better? Its open-sourced.\n\nDARWIN is also capable of training a machine learning model and solving GitHub issues.  \n Watch our video tutorials to witness DARWIN's features in action:  \n \ud83d\udcf9 Video 1: Discover how DARWIN can comprehend complex codebases, conduct thorough research, brainstorm innovative ideas, and proficiently write code in multiple languages. Watch here:[ Darwin Introduction](https:\/\/www.loom.com\/share\/25403d395ed7453998a08b780647acad?sid=64e16979-ae76-4dbc-8e07-3194b435f29f)  \n \ud83d\udcf9 Video 2: Watch DARWIN in action training a Machine Learning model here:[ Darwin ML Training](https:\/\/www.loom.com\/share\/bef27de4069742ec85d6f41c9e25a4a9?sid=6f35a284-fc6f-4c4f-ae61-c27e853cecc0)  \n \ud83d\udcf9 Video 3: Checkout how DARWIN is able to solve GitHub issues all by itself:[ Darwin Solves Github Issues](https:\/\/www.loom.com\/share\/f2c9344b9fc84af3b044f122db7db37f?sid=6426b836-b959-4182-9ead-3b5447eb062a)\n\nWe are launching Darwin as an open-sourced project. Although you cannot reproduce it for commercial purposes, you are free to use it for your personal use and in your daily job life.  \n[ Access Darwin](https:\/\/github.com\/Cognation\/darwin)\n\nJoin us, as we unveil DARWIN's full potential. From managing changes and bug fixes to training models with diverse datasets, DARWIN is going to be your ultimate partner in software development.\n\nShare your feedback, ideas, and suggestions to shape the future of AI in engineering. Let's code smarter, faster, and more innovatively with DARWIN!  \n Stay tuned for more updates and don't forget to check out the DARWIN README for installation instructions and a detailed list of key features.","response":"I did try out aider. However, Darwin, centralized around ML engineers, is not just limited to writing codes but tasks like going through research papers and keeping itself update to ever changing knowledge and documentation using the help of internet. \n\nBut there certain things that Darwin is currently missing like the knowledge of complete codebase which aider handles swiftly and can be referenced to improve Darwin.","category":"misc"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"In past I also worked in face antispoofing and I can confirm this. May be the reason is that these papers are not published in top tier conferences.","category":"general_qa"}
{"instruction":"[D] The usefulness of the last linear layer of each transformer layer","context":"This is a pretty obvious.\n\nI recently see that the last linear layer of transformer is kind of a waste of parameters.\n\nA transformer model is a stack of many transformer layers.\n\nThese layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\\_model \\* d\\_dim\\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.\n\nWe all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.\n\nSo why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","response":"I was intending to replace that particular linear layer with a super sparse linear layer; or something like a resemble convolution with stride, example:\n\nlinear = nn.Linear(4, 1)\n\ninput = torch.randn(b, n\\_s, n\\_d)\n\ninput = input.view(b, n\\_s, n\\_d \/\/ 4, 4)\n\ninput = linear(input).squeeze(-1)\n\nSo no extra parameter.","category":"misc"}
{"instruction":"[D] Those in the industry, how are you using open source LLMs?","context":"Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: \n\n- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of\n- Do you see any performance\/cost gains using fine tuned open source?\n- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?\n- Anything else that might be relevant to using your own LLMs\n\nI'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","response":"Sure, would appreciate a course you found useful. \n\nMy context is that I used to fine tune models and deploy them back when RoBERTa-large etc was considered a standard model size. With the rise of closed LLMs its way quicker to use prompting to ship stuff quickly. \n\nI wanted to know how people are leveraging open source in this world of openai APIs, and what benefits they give apart from the privacy, compliance aspects. \n\nWill definitely look more into AI ops!","category":"general_qa"}
{"instruction":"[D] Any reason not to submit to NeurIPS?","context":"As we all know, abstracts are due tomorrow. I'm on the fence on being able to finish a strong submission in a week. I know that I can always withdraw if reviews are bad (or if I don't feel like I have a strong submission in a week when it's due), but I'm worried that there might be a trace of the submission left online which future reviewers would be able to google. Can anyone confirm that this is only the case if you don't withdraw and instead submit a rebuttal that results in a rejection? If you withdraw from openreview, is any trace of it left online? Do you have to do some trick where you edit and scrub your submission before withdrawing? I know submission results are stochastic, so I'd like to know when, if ever, submitting is a strategic blunder.","response":"My worry is that if the submission is rejected\/withdrawn, subsequent reviewers will google the title or key words to see if the work was previously rejected\/withdrawn. Based on what I've found in this sub, this is common.","category":"misc"}
{"instruction":"[R] The Platonic Representation Hypothesis","context":"arxiv: [https:\/\/arxiv.org\/pdf\/2405.07987](https:\/\/arxiv.org\/pdf\/2405.07987)  \nProject page: [https:\/\/phillipi.github.io\/prh\/](https:\/\/phillipi.github.io\/prh\/)  \ngithub: [https:\/\/github.com\/minyoungg\/platonic-rep\/](https:\/\/github.com\/minyoungg\/platonic-rep\/)\n\nInteresting positional paper on the convergence of self-supervised, multi-modal representation learning.","response":"I wouldn\u2019t say that the identifiability of diffusion modes is that relevant here.  In diffusion models, you, the person training you model, effectively make an arbitrary choice of how to go from your data to a fixed prior (see the recent line of work on bridge matching for details).  Diffusion models identifiability just says that what you learn will be what you chose, which does not directly say anything about representations of data.\n\nQuick edit: also the \u201cidentifiability\u201d from that quote isn\u2019t the same as identifiability from ICA, which is much more relevant for representation learning","category":"misc"}
{"instruction":"[D] How do unets achieve spatial consistency?","context":"Hi,\nI have been reading through unet pytorch implementations here https:\/\/github.com\/lucidrains\/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever \u201eknows\u201c its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?\n\nSo when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?\n\nI think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels \u201enot enough\u201c. \n\nNeither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.\n\nSo How can the unet achieve spatial consistency\/spatial  auto-conditioning?\n\nThanks","response":"You need to think about the receptive field. Convolutional kernels are definitely enough to preserve spacial information.","category":"general_qa"}
{"instruction":"[Discussion] event sequence ORDER prediction","context":"I seem to have stumbled upon a problem that i can't google my way out of.\n\n**\\[MY TRAINING DATA\\]**  \nI have a dataset of bunch of sequential events. each event has 30-40 attributes, including the timestamp the event occured.\n\nuser 1: Event 1 > Event 2 > Event 3  \nuser 2: Event 1 > Event 2 > Event 3 > Event 4 > Event 5  \nuser 3: Event 1  \n....\n\n**\\[THE PROBLEM TO SOLVE\\]**  \nI have a dataset of events, but i do not know which events belongs to which users. those users are different than the training set users, but we are infering they behave the same.  \n   \nFor each event X, I need to solve for X. I need to figure out in what order that event occured. is it event 1? event 2? event 3?  \n\nif X > 1, then event X-1 is also present in the dataset, although i have no way of linking them.\n\n**\\[CURRENT APPROACH\\]**  \nmy manager is pushing to use LSTMs or transformers. I don't have much experience with them, but after doing some research i don't think its the correct approach. in fact, my research doesnt seem to have anything on this problem. am i the only one in the world who has it? ideas welcome. thanks (:","response":"Not sure I totally follow -- is it \"given some attributes of an event, infer whether this event was the first, second, ... for a given person\"?\n\nOr do you have data for a handful of events and you want to sort the events in terms of order?","category":"misc"}
{"instruction":"[R] Pretraining a byte-level 0.67B transformer on a single A100","context":"It feels so good, no multi-gpu crap, just a single powerful A100. Also, no tokenization business, just feeding it with plain UTF-8 bytes.  I've designed a positional encoding that does not rely on assigning increasingly lower scores the further apart two tokens are, so there is no such explicit bias like in RoPE or ALiBi. The hope is it will extrapolate to unlimited context length... probably not, but I will give it a try and waste some $$$. \n\n[Loss is going down!](https:\/\/preview.redd.it\/80qxqbzo1r0d1.png?width=550&format=png&auto=webp&s=58187427556268ecf6754d0339c7a0316dea443e)\n\n  \nIt's still very early in the training - a warmup phase - but this is what it's already capable of - e.g. completing the prompt \"Try our brand new Virtual Wallet services\":\n\n    Try our brand new Virtual Wallet services on a consistently low rate and with the same approach as a Virtual Wallet provider. We have recently emerged to install an outstanding Virtual Wallet service on an existing VW VW VW and VW Solenox VW exterior. We do it on all change dates, remain integrated into our service cycle company to ensure that the customer is happy and satisfied.\n    - Virtual Wallet solution to provide our customers the best and fastest alternative\n    - As soon as we have the premium virtual wallet, we can take it appropriately\n    - When it is needed to allow perishables to be changed\n    - On a demand scale that is extremely high and that is cost-effective\n    - All the necessary information about the products we offer the above\n    - All information required to contact our service providers and have permission\n    - We also deliver Virtual Wallet solutions to providers on a service website\n    - Support the customer and others as they are being contacted by the best customers and solutions\n    - Interested to have our services changed with our final call for recurring payments or any consequential monthly fees\n    - When it is needed to allow perishables to be changed\n    - We provide brand new as well as exclusive Virtual Wallet services\n    - All of our exceptional pricing and quality work\n    - Virtual Wallet solutions in regions of our selected customers\n    - Best example of our Virtual Wallet service\n    - Also integrated Virtual Wallet service\n    - Virtual Wallet service\n    - Fully equipped independent lifetime assistance\n    - Superior customer service\n    - Excellent advanced services\n    - Reputation as sales and advice\n    - An impressive range of products\n    - A concise service that meets your expectations and standards\n    - A commitment to our customers\u2019 expectations in corporate environment.\n    - Professional service to our customers and our community.\n    - All streamlined support on credit cards\n    - A customer-facing experience\n    With the right virtual wallet solution, you can offer a solution that fits your needs and satisfies your presence in the VW Virtual Wallet region. We will design an excellent solution and offer a professional service that is new or old.\n    The remarkable service that you offer meets all your virtual wallet needs. We can also do that to support our customers, their agents, the the manufacturer, all of your supported products\n    The VW Virtual Wallet is equipped with the speedy service in so many areas to produce and service solutions\n    To produce a service on your existing virtual wallet, we can develop and manufacture the VW Virtual Wallet which supports creating a virtual wallet in your operation and also provides virtual wallets to both provide our business solutions to be developed and to support our business solutions.\n    In case you are looking for a lasting change, we can offer this service in the lowest area of our service. For this reason, we can offer this service in case you choose to have a low additional track record and integrate the new VW Virtual Wallet with your virtual wallet. For this reason, we can offer this service in a number of locations that we throughout our service offer alternatives. For this reason, we do have our award-winning VW Virtual Wallet, which is designed to provide a superior service.\n    For more information, please contact the company directly at (202) 353-1465. You can reach the company at (202) 353-1465 to book the service by calling (320) 353-1475.","response":"Congratulations on actually doing the leg work, this is very valuable.\n\nI would not say that RoPE assigns lower scores the further apart tokens are, because it just rotates 2D vectors, which means that, due to that rotation, values can also increase with increasing distance. However it feels like sacrificing half of the usable embedding space in service of positional encoding. This feeling is probably wrong, but I am convinced there are better positional encoding methods possible.\n\nFeel free to share details of your positional encoding method, if you want!","category":"misc"}
{"instruction":"[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video)","context":"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties\/results of KANs like continual learning, sparsification, symbolic regression etc.\n\nLink here: [https:\/\/youtu.be\/7zpz\\_AlFW2w](https:\/\/youtu.be\/7zpz_AlFW2w)","response":"Vague? It's fairly straightforward and close to cases I saw in real life.\n\nThink of an embedding system for high end applications: Aerospace, nuclear power plants, telecommunications, etc. Many of the systems have hard coded functions for a few reasons 1. Power consumption. 2. Interpretability. Even if we had the capacity to store a MLP model in there, well, we don't actually trust how the thing will behave when the system operates in a weird regime, in case of space, that could be some sort of radiation.\n\nNow, my point about KANs was, what if by definition one is capable to - from start - define units that when put together form a close, tight, function, that I can safe derive limits of safety operation. That would be different than retrieving the learnt matrices and plugging in my system.","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"Maybe. I've also seen people saying coding performance is better. Just saying the initial numbers are maybe\/probably overestimated","category":"misc"}
{"instruction":"[D] Those in the industry, how are you using open source LLMs?","context":"Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: \n\n- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of\n- Do you see any performance\/cost gains using fine tuned open source?\n- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?\n- Anything else that might be relevant to using your own LLMs\n\nI'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","response":"It would be nice to have an industry survey paper with insights into the pitfalls discovered","category":"general_qa"}
{"instruction":"[D] Please consider signing this letter to open source AlphaFold3","context":"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g\/viewform\n\nGoogle DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.\n\nAF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.\n\nPlease sign the letter !\n\nAF3 : https:\/\/www.nature.com\/articles\/s41586-024-07487-w","response":"Also, for academic labs Nature requires open source code. It's double standards that they didn't for DeepMind.","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"how is the pokemon case an example for large scale implementation, outside of clickfarms?\n\nso far, every real world use case that i've been working on with my teams couldnt be implemented, while we're steadily getting closer, they didnt cross a qa threshold. but it totally depends on the industry.\n\nfor accessibility, any improvement on text2speech and speech2text is great and welcome. only, implementation costs to switch providers (from google to amazon to openai) every quarter are way too high. \nso we defined thresholds of significant quality improvement that need to be achieved. (as i'm working in the german market: self-detected pronounciation-switches between german and mixed-in english\/foreign words is what we're waiting for) \n\nfor customer care self-set ice, any improvement is also great, but hallucinations and prompt manipulations are terrible. so, there needs to be minimal risk.\n\nin education & journalism use cases, every mistake and hallucination in summarization a problem.","category":"misc"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"Damn, peer reviewed in the reddit comments.\n\nHonestly though it's pretty cool of you to go through it diligently and add these points. I'd be very happy if I was the researcher.","category":"misc"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"Good point, but I see this as a weakness because this is a tabular learning method compared to boosting frameworks, which naturally lend themselves to regression problems, and also other ones (e.g. ranking) via loss functions. And they are powerful at regression, e.g. for time series forecasting, so I see not supporting regression as a quite major limitation. However, the general idea does seem to be able to support regression in the future, so this is more of a implementation downside rather than general approach one.","category":"misc"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"Yes and no. As far as I can remember, none of the major ML conferences make submitting and or open sourcing code a strict requirement for acceptance. I think it should be, but as it stands you need to play by the rules and judge a paper fairly even without being able to check the code.","category":"general_qa"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"That big of a jump? Pretty impressive","category":"misc"}
{"instruction":"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","context":"Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I\u2019m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can\u2019t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?\n\nAre there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","response":"This is just a standard thing in functional analysis, which is the application of linear algebra to infinite dimensional vector spaces. It's also the traditional way of solving PDEs etc; if you read about e.g. finite element analysis, you'll see that the discretization is discussed entirely in terms of linear algebra as I have outlined above.\n\nEDIT: for something concrete to look at, read about the Galerkin method: https:\/\/en.wikipedia.org\/wiki\/Galerkin_method","category":"general_qa"}
{"instruction":"[D] Video analysis tools for detecting cheating in interviews","context":"Hi everyone,\n\nI am looking for any video analysis tools that I can use for the following usecase:\n\nI have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab\/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.\n\nThanks in advance for any inputs.","response":"This is a bit of a silly response as in interview settings we are assessing abilities via simple toy problems. A few minutes of \"planning\" on chat gpt can get you a general approach that is promising, obscuring most of what we're trying to infer in the first place.  I wouldn't care if someone used it for a longer take home (not that we give them), but for simple interview problems it's an issue.","category":"misc"}
{"instruction":"[Discussion] MICCAI 2024 decisions","context":"Hi all,\n\nI thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.\n\nI got a rebuttal invitation for an application paper and all the reviewers mentioned \"lack of technical novelty\" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","response":"I wonder the same thing here also as one of my reviewer explicitly asks for comparison to two specific other methods","category":"misc"}
{"instruction":"[D] Seminal papers list since 2018 that will be considered cannon in the future","context":"Hi there,\n\nA recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML\/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  \n\nWhere should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","response":"https:\/\/www.nature.com\/articles\/s42254-021-00314-5\n\nPhysics informed ml.","category":"misc"}
{"instruction":"[D] Running large models on Mac for prototype\/finetune","context":"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev\/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100\/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev\/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?","response":"For a lot of data scientists not having to use tools like ray or Kubernetes is actually a benefit as it's just another thing that can get in the way and make it harder to debug and fix problems.\n\nPersonally as a data scientist I'd rather just get a bare metal ok prem development machine for experimentation when working in a smaller team. If I'm moving fast it's easier to strip everything off the machine including any bit of IT management stuff, unnecessary packages, frameworks, orchestration stuff, etc. makes debugging stuff far easier and there's nothing using extra CPU processes so I can fully utilize the machine.","category":"misc"}
{"instruction":"[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video)","context":"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties\/results of KANs like continual learning, sparsification, symbolic regression etc.\n\nLink here: [https:\/\/youtu.be\/7zpz\\_AlFW2w](https:\/\/youtu.be\/7zpz_AlFW2w)","response":"In this regard, where\u2019s the key difference with all the interpolation and regression methods that predated neural networks?","category":"misc"}
{"instruction":"[D] Apriori Algorithm","context":"Do anyone still use Apriori in production use cases? There must be better algorithms available.","response":"Fpgrowth is what you want to use instead.","category":"misc"}
{"instruction":"[D] Those in the industry, how are you using open source LLMs?","context":"Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: \n\n- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of\n- Do you see any performance\/cost gains using fine tuned open source?\n- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?\n- Anything else that might be relevant to using your own LLMs\n\nI'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","response":"Damn that sounds so cool","category":"general_qa"}
{"instruction":"[D] Video analysis tools for detecting cheating in interviews","context":"Hi everyone,\n\nI am looking for any video analysis tools that I can use for the following usecase:\n\nI have screen recordings of candidates during interviews. I would like to detect if the user has switched from the interview test tab to a different tab\/application frequently. I know that there are browser APIs to detect tab switching but users can still switch to a different application on their system for which we would be need OS level access for detecting those.\n\nThanks in advance for any inputs.","response":"Pretty much 100% of candidates are using AI to outperform on online automated screening and assessments in the hiring process.\n\nThese are the candidates you should be hiring right now, not attempting to screen out.  Clearly, they are better at using AI than you.","category":"misc"}
{"instruction":"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","context":"Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I\u2019m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can\u2019t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?\n\nAre there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","response":"I agree, why would binning be considered linear?","category":"general_qa"}
{"instruction":"[P] Multivariate Traffic Flow Predictions","context":"Hi, i just started a new project, i have this traffic datasets with 5 Minutes time intervals , and flow and speed, now i tried using prediction models like Lstm and gru successfully with flow or speed but when i try to use the model on both speed and flow it doesn\u2019t work. Any inputs where can i learn this stuff ?","response":"Traffic flow predictions with KANs https:\/\/www.reddit.com\/r\/MachineLearning\/s\/w1cxgKH7zT","category":"misc"}
{"instruction":"[R] Marcus Hutter's work on Universal Artificial Intelligence","context":"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https:\/\/www.amazon.co.uk\/Universal-Artificial-Intelligence-Algorithmic-Probability\/dp\/3540221395) in 2005 and [one](https:\/\/www.amazon.co.uk\/Introduction-Universal-Artificial-Intelligence-Robotics\/dp\/1032607025\/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of \"universal intelligence\" in their paper [https:\/\/arxiv.org\/abs\/0712.3329](https:\/\/arxiv.org\/abs\/0712.3329)\n\nIn my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:\n\nhttps:\/\/preview.redd.it\/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49\n\nYoutube:\u00a0[https:\/\/www.youtube.com\/watch?v=7TgOwMW\\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https:\/\/www.youtube.com\/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)\n\n  \nOutline:\n\nI. Introduction\n\n* 00:38 : Biography\n* 01:45 : From Physics to AI\n* 03:05 : Hutter Prize\n* 06:25 : Overview of Universal Artificial Intelligence\n* 11:10 : Technical outline\n\nII. Universal Prediction\n\n* 18:27 : Laplace\u2019s Rule and Bayesian Sequence Prediction\n* 40:54 : Different priors: KT estimator\n* 44:39 : Sequence prediction for countable hypothesis class\n* 53:23 : Generalized Solomonoff Bound (GSB)\n* 57:56 : Example of GSB for uniform prior\n* 1:04:24 : GSB for continuous hypothesis classes\n* 1:08:28 : Context tree weighting\n* 1:12:31 : Kolmogorov complexity\n* 1:19:36 : Solomonoff Bound & Solomonoff Induction\n* 1:21:27 : Optimality of Solomonoff Induction\n* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines\n* 1:28:37 : Large Language Models (LLMs)\n* 1:37:07 : Using LLMs to emulate Solomonoff induction\n* 1:41:41 : Loss functions\n* 1:50:59 : Optimality of Solomonoff induction revisited\n* 1:51:51 : Marvin Minsky\n\nIII. Universal Agents\n\n* 1:52:42 : Recap and intro\n* 1:55:59 : Setup\n* 2:06:32 : Bayesian mixture environment\n* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy\n* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)\n* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence\n* 2:15:35 : AIXI explicit formula\n* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)\n* 2:33:09 : Multiagent setting\n* 2:39:38 : Grain of Truth problem\n* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria\n* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs\n* 2:56:13 : Outro: Brief philosophical remarks","response":"I will be messaging you in 1 month on [**2024-06-12 02:43:22 UTC**](http:\/\/www.wolframalpha.com\/input\/?i=2024-06-12%2002:43:22%20UTC%20To%20Local%20Time) to remind you of [**this link**](https:\/\/www.reddit.com\/r\/MachineLearning\/comments\/1cpcwuz\/r_marcus_hutters_work_on_universal_artificial\/l3nsv1e\/?context=3)\n\n[**1 OTHERS CLICKED THIS LINK**](https:\/\/www.reddit.com\/message\/compose\/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FMachineLearning%2Fcomments%2F1cpcwuz%2Fr_marcus_hutters_work_on_universal_artificial%2Fl3nsv1e%2F%5D%0A%0ARemindMe%21%202024-06-12%2002%3A43%3A22%20UTC) to send a PM to also be reminded and to reduce spam.\n\n^(Parent commenter can ) [^(delete this message to hide from others.)](https:\/\/www.reddit.com\/message\/compose\/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201cpcwuz)\n\n*****\n\n|[^(Info)](https:\/\/www.reddit.com\/r\/RemindMeBot\/comments\/e1bko7\/remindmebot_info_v21\/)|[^(Custom)](https:\/\/www.reddit.com\/message\/compose\/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https:\/\/www.reddit.com\/message\/compose\/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https:\/\/www.reddit.com\/message\/compose\/?to=Watchful1&subject=RemindMeBot%20Feedback)|\n|-|-|-|-|","category":"misc"}
{"instruction":"[R] Replace the object in the frame with a similar object","context":"Hello, I am doing research for my project and I need your help.  \nThere is a closed pizza box on the table in a 5 sec part of a movie, I want to replace this pizza box with Pizza H\\*t, \\*ominos, etc. There are only a few Pizza H\\*t boxes in my dataset, so the artificial intelligence model will need to use a technique similar to \"Few Shot Learn\". I thought I could do it using Generative Adversarial Networks (GAN) but I couldn't go further than turning a horse into a zebra. I have enough knowledge about AI and software development but new in GAN s.\n\nWhich model should I use, how should I research, which keywords should I use etc., where can i start to search for proper result I need your help, thanks.","response":"Greetings, I made progress in my research and found a model called \"Rewriting a Deep Generative Model\". I will try it and share the outputs.\n\n[https:\/\/rewriting.csail.mit.edu\/](https:\/\/rewriting.csail.mit.edu\/)","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"According to the blog post, they\u2019ve made major improvements to audio and image modalities. It was trained end-to-end on all three types of data, instead of stapling an image encoder to an LLM like GPT-4V did.","category":"misc"}
{"instruction":"[R] The Platonic Representation Hypothesis","context":"arxiv: [https:\/\/arxiv.org\/pdf\/2405.07987](https:\/\/arxiv.org\/pdf\/2405.07987)  \nProject page: [https:\/\/phillipi.github.io\/prh\/](https:\/\/phillipi.github.io\/prh\/)  \ngithub: [https:\/\/github.com\/minyoungg\/platonic-rep\/](https:\/\/github.com\/minyoungg\/platonic-rep\/)\n\nInteresting positional paper on the convergence of self-supervised, multi-modal representation learning.","response":"We have those similar representations because our brains constantly associate those different sensory impressions with each other and use all of them for learning. Thus it makes sense to have a \u201ecombined latent representation\u201c - a world model so to say. But you can\u2019t take only the eyes paired with a brain and then expect it to learn the same. This is fundamentally flawed.","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"The only audiovisual benchmark I see noted in their blog post is an Audio ASR beat over Whisper-3. Don't you think they'd show\/share more beats on multimodal benchmarks if they had them to show?","category":"misc"}
{"instruction":"[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video)","context":"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties\/results of KANs like continual learning, sparsification, symbolic regression etc.\n\nLink here: [https:\/\/youtu.be\/7zpz\\_AlFW2w](https:\/\/youtu.be\/7zpz_AlFW2w)","response":"There seems to be a bit too much hype around this paper despite not having been tested much. Any more promising results after further testing trying to scale it up?","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"Hmm, are you so sure ? Talking about phones, if the deal between OpenAI and Apple goes through, I can imagine Apple giving the ability to developers to make tools, shortcuts and actions from their app directly accessible to an API that the model could use. \nThe environment would be adapted for the model and I guess the model would also be finetuned to use the tools, docs provided by the developers but also the internal APIs of the iPhone. \nThat doesn\u2019t seem \u00ab\u00a0unbelievably far away\u00a0\u00bb, at least for having access to the internal APIs of iOS. This opens up A LOT of use-cases, since we can do almost anything with a smartphone.\nBeing so assertive and confident about limitations in this time of rapid progress is not a good idea!","category":"misc"}
{"instruction":"[D] How do you get better at reading proof in the ML papers, with background in CS only?","context":"Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!\n\nEdit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","response":"My favourite undergraduate book was Spivak's Calculus. I think it's very easy to fall in love with Analysis in that book.\n\nThat being said, don't be like me, try not to waste one day of your life in one problem. Look it up and move on.","category":"general_qa"}
{"instruction":"[D] Please consider signing this letter to open source AlphaFold3","context":"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g\/viewform\n\nGoogle DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.\n\nAF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.\n\nPlease sign the letter !\n\nAF3 : https:\/\/www.nature.com\/articles\/s41586-024-07487-w","response":"Let\u2019s be honest, even 20 millions people signed it, do you think they would fear the pressure and open source it? Has any close source become open source after public signing letter?","category":"misc"}
{"instruction":"[D] How do you get better at reading proof in the ML papers, with background in CS only?","context":"Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!\n\nEdit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","response":"Spivak's Calculus is amazing, so I second this.","category":"general_qa"}
{"instruction":"[D] Please consider signing this letter to open source AlphaFold3","context":"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g\/viewform\n\nGoogle DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.\n\nAF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.\n\nPlease sign the letter !\n\nAF3 : https:\/\/www.nature.com\/articles\/s41586-024-07487-w","response":"But how would anyone know that those things actually work really well, if no one is allowed to use them?","category":"misc"}
{"instruction":"[R] The Platonic Representation Hypothesis","context":"arxiv: [https:\/\/arxiv.org\/pdf\/2405.07987](https:\/\/arxiv.org\/pdf\/2405.07987)  \nProject page: [https:\/\/phillipi.github.io\/prh\/](https:\/\/phillipi.github.io\/prh\/)  \ngithub: [https:\/\/github.com\/minyoungg\/platonic-rep\/](https:\/\/github.com\/minyoungg\/platonic-rep\/)\n\nInteresting positional paper on the convergence of self-supervised, multi-modal representation learning.","response":"Sometimes, papers contain some aspects that are trivial facts and also some aspects that are obviously wrong.","category":"misc"}
{"instruction":"[D] Those in the industry, how are you using open source LLMs?","context":"Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: \n\n- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of\n- Do you see any performance\/cost gains using fine tuned open source?\n- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?\n- Anything else that might be relevant to using your own LLMs\n\nI'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","response":"I tried LLAVA for a commercial use case, I didn\u2019t find it worked well and went down a CNN route which worked.  I\u2019d be interested in how youve utilised it? If you\u2019re able to share.","category":"general_qa"}
{"instruction":"[D] ACL 2024 Decisions","context":"Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? \ud83c\uddf9\ud83c\udded\ud83d\udc18","response":"Main conference accept it is :)","category":"misc"}
{"instruction":"[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video)","context":"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties\/results of KANs like continual learning, sparsification, symbolic regression etc.\n\nLink here: [https:\/\/youtu.be\/7zpz\\_AlFW2w](https:\/\/youtu.be\/7zpz_AlFW2w)","response":"Thank you! Very good explaination","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"It was slow before because they used multiple models for speech to text and text to speech and thought inference .  For 4o they trained a single model to do all of it.  Less tokens because everything is \u201cpassed around\u201d less.","category":"misc"}
{"instruction":"[D] Correct interpretation of Model.predict output.","context":"Currently taking the FCC Machine Learning Course.\n\nI dont know how to correctly interpret the probabilities of the Model.predict function output. The CNN is meant to determine whether is an image of a cat or a dog. Some probabilities are negative and very low. I dont know how to interpret that data.  \nAlso mention the model achieves its goal with an aceptable margin.  \nHere an example:\n\nhttps:\/\/preview.redd.it\/yyss5v202y0d1.png?width=759&format=png&auto=webp&s=b1141a6e7150fcfdf0b65dc2119de956a9fc8ec8\n\nHere the code.\n\nLink of images:\n\n    wget https:\/\/cdn.freecodecamp.org\/project-data\/cats-and-dogs\/cats_and_dogs.zip\n\nCode:\n\n    # 3\n    train_image_generator = ImageDataGenerator(rescale=1.\/255)\n    validation_image_generator = ImageDataGenerator(rescale=1.\/255)\n    test_image_generator = ImageDataGenerator(rescale=1.\/255)\n    \n    train_data_gen = train_image_generator.flow_from_directory(\n    \u00a0 \u00a0 train_dir,\n    \u00a0 \u00a0 target_size=(IMG_HEIGHT,IMG_WIDTH),\n    \u00a0 \u00a0 batch_size = batch_size,\n    \u00a0 \u00a0 class_mode = 'binary')\n    val_data_gen = validation_image_generator.flow_from_directory(\n    \u00a0 \u00a0 directory = validation_dir,\n    \u00a0 \u00a0 target_size=(IMG_HEIGHT,IMG_WIDTH),\n    \u00a0 \u00a0 batch_size = batch_size,\n    \u00a0 \u00a0 class_mode = 'binary')\n    test_data_gen = test_image_generator.flow_from_directory(\n    \u00a0 \u00a0 directory=test_dir,\n    \u00a0 \u00a0 target_size=(IMG_HEIGHT,IMG_WIDTH),\n    \u00a0 \u00a0 batch_size = batch_size,\n    \u00a0 \u00a0 class_mode = 'binary',\n    \u00a0 \u00a0 shuffle=False)\n\n    # 4\n    def plotImages(images_arr, probabilities = False):\n    \u00a0 \u00a0 fig, axes = plt.subplots(len(images_arr), 1, figsize=(5,len(images_arr) * 3))\n    \u00a0 \u00a0 if probabilities is False:\n    \u00a0 \u00a0 \u00a0 for img, ax in zip( images_arr, axes):\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.imshow(img)\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.axis('off')\n    \u00a0 \u00a0 else:\n    \u00a0 \u00a0 \u00a0 for img, probability, ax in zip( images_arr, probabilities, axes):\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.imshow(img)\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.axis('off')\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if probability > 0.5:\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.set_title(\"%.2f\" % (probability*100) + \"% dog\")\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 else:\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.set_title(\"%.2f\" % ((1-probability)*100) + \"% cat\")\n    \u00a0 \u00a0 plt.show()\n    \n    sample_training_images, _ = next(train_data_gen)\n    plotImages(sample_training_images[:5])\n    \n    \n    \n\n    # 5\n    train_image_generator = train_image_generator = ImageDataGenerator(\n    \u00a0 \u00a0 rotation_range = 360,\n    \u00a0 \u00a0 horizontal_flip = True,\n    \u00a0 \u00a0 vertical_flip = True,\n    \u00a0 \u00a0 zoom_range = 0.2,\n    \u00a0 \u00a0 shear_range = 60,\n    \u00a0 \u00a0 rescale=1.\/255)\n    \n    \n\n    # 6\n    train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0directory=train_dir,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0target_size=(IMG_HEIGHT, IMG_WIDTH),\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0class_mode='binary')\n    \n    augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n    \n    plotImages(augmented_images)\n\n    # 7\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(Flatten())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(2))\n    \n    model.compile(optimizer='adam',\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 metrics=['accuracy'])\n    model.summary()\n\n    # 8\n    history = model.fit(x = train_data_gen, \n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 epochs = epochs,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 validation_data = val_data_gen)\n    acc = history.history['accuracy']\n    print(acc)\n\n    # 9\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    \n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs_range = range(epochs)\n    print(epochs_range)\n    print(acc)\n    \n    plt.figure(figsize=(8, 8))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.show()\n\n    #10\n    probabilities = model.predict(test_data_gen)\n    print(probabilities)\n    probabilities = np.argmax(probabilities, axis = 1)\n    sample_test_images, _ = next(test_data_gen)\n    plotImages(sample_test_images, probabilities=probabilities)\n\n    # 11\n    answers = \u00a0[1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0, 0, 0, 0, 0, 0]\n    \n    correct = 0\n    \n    for probability, answer in zip(probabilities, answers):\n    \u00a0 if round(probability) == answer:\n    \u00a0 \u00a0 correct +=1\n    \n    percentage_identified = (correct \/ len(answers)) * 100\n    \n    passed_challenge = percentage_identified >= 63\n    \n    print(f\"Your model correctly identified {round(percentage_identified, 2)}% of the images of cats and dogs.\")\n    \n    if passed_challenge:\n    \u00a0 print(\"You passed the challenge!\")\n    else:\n    \u00a0 print(\"You haven't passed yet. Your model should identify at least 63% of the images. Keep trying. You will get it!\")","response":"Since OP asked about the interpretation I would also add the following: Be careful when using these \"probabilities\". I prefer the term \"confidence\". The softmax output has all the properties of a probability distribution, but it is dangerous to treat these values as a probability of the prediction being correct. \n\nFor a well-calibrated model the confidence value and probability of the prediction being correct should be the same, but that does not need to be the case.\n\nIn case OP needs this (or is otherwise interested), you can read more about model calibration here: [https:\/\/scikit-learn.org\/stable\/modules\/calibration.html](https:\/\/scikit-learn.org\/stable\/modules\/calibration.html)","category":"misc"}
{"instruction":"What's your favorite paper at ICLR2024? [D]","context":"Way too much to keep in track..","response":"This is the paper for the curious: \"[Auto-Encoding Variational Bayes](https:\/\/ar5iv.labs.arxiv.org\/html\/1312.6114)\".\n\nThe [runner up](https:\/\/blog.iclr.cc\/2024\/05\/07\/iclr-2024-test-of-time-award\/) looks cool too: \"[Intriguing properties of neural networks](https:\/\/ar5iv.labs.arxiv.org\/html\/1312.6199)\".","category":"general_qa"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"When you are a researcher you are supposed to also set aside time for reviewing other people's work, this is true for all research fields. The issue is, you generally are already massively overworked and reviewing is something you do either as secondary or tertiary importance level in your job, or in your free time, and, of course, you're not paid to do it.\n\nAs you can imagine it is quite hard to find reviewers, so lately a lot of inexperienced people are asked and sometimes \"paid\" by coupons for reducing the publishing fee.","category":"general_qa"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"> In psych you might publish the questions asked in a survey and the overall result but not the raw data from the survey and not the web form used to ask the questions.\n\n  \nFYI, that is not true anymore. Because a lot of psych research turned out to be not replicable, people nowadays actually are expected to post the raw data. Basically no serious researcher believes a psych study that does not put the raw data and analysis code in a repo like osf.io.","category":"general_qa"}
{"instruction":"[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video)","context":"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties\/results of KANs like continual learning, sparsification, symbolic regression etc.\n\nLink here: [https:\/\/youtu.be\/7zpz\\_AlFW2w](https:\/\/youtu.be\/7zpz_AlFW2w)","response":"I came up with a fictitious scientific application.  I don't want to come on Reddit and write exactly what I do in real applications.","category":"misc"}
{"instruction":"[R] Marcus Hutter's work on Universal Artificial Intelligence","context":"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https:\/\/www.amazon.co.uk\/Universal-Artificial-Intelligence-Algorithmic-Probability\/dp\/3540221395) in 2005 and [one](https:\/\/www.amazon.co.uk\/Introduction-Universal-Artificial-Intelligence-Robotics\/dp\/1032607025\/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of \"universal intelligence\" in their paper [https:\/\/arxiv.org\/abs\/0712.3329](https:\/\/arxiv.org\/abs\/0712.3329)\n\nIn my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:\n\nhttps:\/\/preview.redd.it\/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49\n\nYoutube:\u00a0[https:\/\/www.youtube.com\/watch?v=7TgOwMW\\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https:\/\/www.youtube.com\/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)\n\n  \nOutline:\n\nI. Introduction\n\n* 00:38 : Biography\n* 01:45 : From Physics to AI\n* 03:05 : Hutter Prize\n* 06:25 : Overview of Universal Artificial Intelligence\n* 11:10 : Technical outline\n\nII. Universal Prediction\n\n* 18:27 : Laplace\u2019s Rule and Bayesian Sequence Prediction\n* 40:54 : Different priors: KT estimator\n* 44:39 : Sequence prediction for countable hypothesis class\n* 53:23 : Generalized Solomonoff Bound (GSB)\n* 57:56 : Example of GSB for uniform prior\n* 1:04:24 : GSB for continuous hypothesis classes\n* 1:08:28 : Context tree weighting\n* 1:12:31 : Kolmogorov complexity\n* 1:19:36 : Solomonoff Bound & Solomonoff Induction\n* 1:21:27 : Optimality of Solomonoff Induction\n* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines\n* 1:28:37 : Large Language Models (LLMs)\n* 1:37:07 : Using LLMs to emulate Solomonoff induction\n* 1:41:41 : Loss functions\n* 1:50:59 : Optimality of Solomonoff induction revisited\n* 1:51:51 : Marvin Minsky\n\nIII. Universal Agents\n\n* 1:52:42 : Recap and intro\n* 1:55:59 : Setup\n* 2:06:32 : Bayesian mixture environment\n* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy\n* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)\n* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence\n* 2:15:35 : AIXI explicit formula\n* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)\n* 2:33:09 : Multiagent setting\n* 2:39:38 : Grain of Truth problem\n* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria\n* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs\n* 2:56:13 : Outro: Brief philosophical remarks","response":"In my opinion that is not an approximation. It's sort of a solution to a problem, but it's not an approximation.\n\nI think the distinction you're drawing there is actually between *possible* and *impossible*. And for that reason I think that \"non computable\" is bad terminology given its definition, because colloquially it does seem to imply that a non computable function literally cannot ever be evaluated, which isn't true.\n\nLike, sure, the halting problem is non computable, but you always have the option of letting a program run until it stops, and sometimes it does indeed stop. You just can't know that it will beforehand, and (in general) you also can't know how close you are to a stopping point.\n\nIf I had my way we'd replace \"non computable\" with \"non approximable\", because that gives a very concrete and correct colloquial understanding of the meaning of the thing.","category":"misc"}
{"instruction":"[D] Neurips checklist Questions","context":"I had a doubt about the Neurips 2024 checklist questions. \n\nThe instructions relating to checklist says: \"All submissions must be in PDF format, and in a single PDF file include, in this order, (1) the submitted paper; (2) optional technical appendices that support the paper with additional proofs, derivations, or results; (3) the NeurIPS paper checklist. \"\n\nIt is also mentioned that \"The checklist is included in the LateX style file or the NeurIPS 2024 template on Overleaf.\"\n\nHowever, when I'm trying to open the link \"NeurIPS 2024 template on Overleaf\", it is showing as \"Project Not Found\" in the overleaf website. \n\nSo, how are folks accessing and adding the checklist that needs to added at the end of the paper? \n\nAlso, while submitting the abstract in openreview there was no checklist questions. I could submit the abstract and still can view and edit the submission. Hopefully the abstract submission has gone through correctly.","response":"Why does the [FAQ](https:\/\/neurips.cc\/Conferences\/2024\/PaperInformation\/NeurIPS-FAQ#) say: \"Do you need to include a paper checklist? No, there was no checklist in the PDF this year, so this is not needed.\"","category":"misc"}
{"instruction":"Integrating Multiple AIs for Single Purposes\u2014Seeking Research and Keywords [R]","context":"I'm currently conducting research on AI and am curious if there are any studies discussing the use of multiple AI systems collaboratively for a single purpose. For example, using a language AI to assist a voice recognition AI to more accurately determine what sounds correspond to specific words. Are there specific keywords or phrases I should use to search for research in this area?","response":"Try the application streams of AAMAS:https:\/\/www.southampton.ac.uk\/\\~eg\/AAMAS2023\/forms\/contents.htm#4F \n\nor have a look at Mike Wooldridge's book","category":"misc"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"wtffffff that's crazy. What do their reviewers do with their time then if GPT is used to do the reviewing work? They're just sitting all day twiddling their thumbs?","category":"general_qa"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"Well said.\nThe best some conferences do is to add a few silly checklists.\n\nApparently coming up with a basic template for coding (something that any Computer Science undergrad course requires for assignments) suddenly is too much for the people claiming to work on AGI and save the world hunger via universal basic income.\n\nThe way I see it: Conferences policy is biased and acting on bad faith (they don't want to cripple the business), and the researchers\/students cope with that because they also want to build their portfolio to get a \"FAANG\" position.\n\nAll of course, in detriment to science: How many \"truths\" are out there and keep being repeated just because...someone said so and it's almost impossible to verify? How many researchers in the next decade have to waste time and resources just because someone simply didn't do their jobs in properly sharing their work?\n\nIt's infuriating...","category":"general_qa"}
{"instruction":"[D] Please consider signing this letter to open source AlphaFold3","context":"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g\/viewform\n\nGoogle DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.\n\nAF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.\n\nPlease sign the letter !\n\nAF3 : https:\/\/www.nature.com\/articles\/s41586-024-07487-w","response":"Not gonna happen.","category":"misc"}
{"instruction":"[D] Those in the industry, how are you using open source LLMs?","context":"Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: \n\n- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of\n- Do you see any performance\/cost gains using fine tuned open source?\n- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?\n- Anything else that might be relevant to using your own LLMs\n\nI'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","response":"We used completely synthetic dataset generated by gpt-4 for an NLP task trained on a smaller and faster model. It really works well; although I advise involving humans to review the datasets.","category":"general_qa"}
{"instruction":"[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?","context":"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.\n\nEDIT: I am mainly working with regression problems.\n\nThanks in advance! :)","response":"I've not heard of these, post a link and I'll have a look. I guess it's reliant on the volume and quality of the data and the degree of drop out applied to some extent. In practice however I've found it works very well.","category":"general_qa"}
{"instruction":"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","context":"Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I\u2019m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can\u2019t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?\n\nAre there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","response":"To be clear, x here does not represent a function, it's a state vector. Functions are linear operators like matrices.\n\nAnd I think you're implicitly assuming that x+y isn't a valid state vector, but that's not so. You can interpret x+y as a statistical ensemble; the output of any function (i.e. matrix multiplication) is then an ensemble of possible outputs, represented as a vector.\n\nThis also how quantum mechanics works; in QM the entire universe and everything in it is a linear equation, and vectors like x+y are superpositions. Indeed that's a simple and reductive way of seeing how nonlinearity is unnecessary.\n\nIn order to use linear algebra you do have to choose a basis set, of course, but it's very weird to think of \"choosing a basis set\" as a nonlinear operation. I think it's better framed as an axiom.","category":"general_qa"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"[AudioPaLM](https:\/\/google-research.github.io\/seanet\/audiopalm\/examples\/) did text + audio to text + audio in one LLM","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"crazy cool and definitely eliminating many startup ideas","category":"misc"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"I see you've posted a GitHub link to a Jupyter Notebook! GitHub doesn't \nrender large Jupyter Notebooks, so just in case, here is an \n[nbviewer](https:\/\/nbviewer.jupyter.org\/) link to the notebook:\n\nhttps:\/\/nbviewer.jupyter.org\/url\/github.com\/catboost\/catboost\/blob\/master\/catboost\/tutorials\/categorical_features\/categorical_features_parameters.ipynb\n\nWant to run the code yourself? Here is a [binder](https:\/\/mybinder.org\/) \nlink to start your own Jupyter server and try it out!\n\nhttps:\/\/mybinder.org\/v2\/gh\/catboost\/catboost\/master?filepath=catboost%2Ftutorials%2Fcategorical_features%2Fcategorical_features_parameters.ipynb\n\n\n\n------\n\n^(I am a bot.) \n[^(Feedback)](https:\/\/www.reddit.com\/message\/compose\/?to=jd_paton) ^(|) \n[^(GitHub)](https:\/\/github.com\/JohnPaton\/nbviewerbot) ^(|) \n[^(Author)](https:\/\/johnpaton.net\/)","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"The problem is that LLMs have different style, so it is relatively easy to discern the families once you play with them awhile. (OpenAI uses Latex, llama always tells you that you've raised a great question, etc.), so that introduces some level of bias.\n\nThere's a risk that LMSys corrupted data by removing the experimental models from direct chat, but permitted them to still be in area (with follow-up). Encouraged gaming to \"find gpt-4\".","category":"misc"}
{"instruction":"[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?","context":"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.\n\nEDIT: I am mainly working with regression problems.\n\nThanks in advance! :)","response":"You should look at this recent (2024) paper where we benchmark various Bayesian neural network methods specifically for uncertainty quantification in regression tasks:  \nPaper: [https:\/\/doi.org\/10.1016\/j.neucom.2023.127183](https:\/\/doi.org\/10.1016\/j.neucom.2023.127183)  \nPreprint: [http:\/\/profs.polymtl.ca\/jagoulet\/Site\/Papers\/Deka\\_TAGIV\\_2024\\_preprint.pdf](http:\/\/profs.polymtl.ca\/jagoulet\/Site\/Papers\/Deka_TAGIV_2024_preprint.pdf)","category":"general_qa"}
{"instruction":"Nanogpt alternative [D]","context":"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model\n\nAlso, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? \n\nThanks","response":"[Nanogpt](https:\/\/github.com\/karpathy\/nanoGPT) is not an LLM. It is a program for training LLMs. \n\nYou would create a different LLM by training on different data.","category":"misc"}
{"instruction":"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","context":"Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I\u2019m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can\u2019t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?\n\nAre there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","response":"the intuitive explanation is that, for instance in the case of the attention mechanism, its the softmax part that makes it non linear and makes inference so slow. by getting rid of the softmax (techniques such as linear attention and also rkwv etc) you can do inference as if it is an rnn\n\n\non the opposite side, what prevents RNN from beeing trained on all of a sequence at the same time is the non linear elements: by making an rnn linear it can be trained on the whole sequence at once like a transformer\u00a0\n\n\nso linear rnn and linear attention ends up being the same thing.\n\n\nF(a + b) = f(a) + f(b) makes it so you can have both parallel training and RNN like inference","category":"general_qa"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"Linear from SERF stands for linear time complexity but not being a linear model, right? Personally, I find the naming confusing and thought it might be something like boosting linear models (of which one should note that a straightforward ensemble of linear models is still a linear model)","category":"misc"}
{"instruction":"[D] Correct interpretation of Model.predict output.","context":"Currently taking the FCC Machine Learning Course.\n\nI dont know how to correctly interpret the probabilities of the Model.predict function output. The CNN is meant to determine whether is an image of a cat or a dog. Some probabilities are negative and very low. I dont know how to interpret that data.  \nAlso mention the model achieves its goal with an aceptable margin.  \nHere an example:\n\nhttps:\/\/preview.redd.it\/yyss5v202y0d1.png?width=759&format=png&auto=webp&s=b1141a6e7150fcfdf0b65dc2119de956a9fc8ec8\n\nHere the code.\n\nLink of images:\n\n    wget https:\/\/cdn.freecodecamp.org\/project-data\/cats-and-dogs\/cats_and_dogs.zip\n\nCode:\n\n    # 3\n    train_image_generator = ImageDataGenerator(rescale=1.\/255)\n    validation_image_generator = ImageDataGenerator(rescale=1.\/255)\n    test_image_generator = ImageDataGenerator(rescale=1.\/255)\n    \n    train_data_gen = train_image_generator.flow_from_directory(\n    \u00a0 \u00a0 train_dir,\n    \u00a0 \u00a0 target_size=(IMG_HEIGHT,IMG_WIDTH),\n    \u00a0 \u00a0 batch_size = batch_size,\n    \u00a0 \u00a0 class_mode = 'binary')\n    val_data_gen = validation_image_generator.flow_from_directory(\n    \u00a0 \u00a0 directory = validation_dir,\n    \u00a0 \u00a0 target_size=(IMG_HEIGHT,IMG_WIDTH),\n    \u00a0 \u00a0 batch_size = batch_size,\n    \u00a0 \u00a0 class_mode = 'binary')\n    test_data_gen = test_image_generator.flow_from_directory(\n    \u00a0 \u00a0 directory=test_dir,\n    \u00a0 \u00a0 target_size=(IMG_HEIGHT,IMG_WIDTH),\n    \u00a0 \u00a0 batch_size = batch_size,\n    \u00a0 \u00a0 class_mode = 'binary',\n    \u00a0 \u00a0 shuffle=False)\n\n    # 4\n    def plotImages(images_arr, probabilities = False):\n    \u00a0 \u00a0 fig, axes = plt.subplots(len(images_arr), 1, figsize=(5,len(images_arr) * 3))\n    \u00a0 \u00a0 if probabilities is False:\n    \u00a0 \u00a0 \u00a0 for img, ax in zip( images_arr, axes):\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.imshow(img)\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.axis('off')\n    \u00a0 \u00a0 else:\n    \u00a0 \u00a0 \u00a0 for img, probability, ax in zip( images_arr, probabilities, axes):\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.imshow(img)\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.axis('off')\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if probability > 0.5:\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.set_title(\"%.2f\" % (probability*100) + \"% dog\")\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 else:\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.set_title(\"%.2f\" % ((1-probability)*100) + \"% cat\")\n    \u00a0 \u00a0 plt.show()\n    \n    sample_training_images, _ = next(train_data_gen)\n    plotImages(sample_training_images[:5])\n    \n    \n    \n\n    # 5\n    train_image_generator = train_image_generator = ImageDataGenerator(\n    \u00a0 \u00a0 rotation_range = 360,\n    \u00a0 \u00a0 horizontal_flip = True,\n    \u00a0 \u00a0 vertical_flip = True,\n    \u00a0 \u00a0 zoom_range = 0.2,\n    \u00a0 \u00a0 shear_range = 60,\n    \u00a0 \u00a0 rescale=1.\/255)\n    \n    \n\n    # 6\n    train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0directory=train_dir,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0target_size=(IMG_HEIGHT, IMG_WIDTH),\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0class_mode='binary')\n    \n    augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n    \n    plotImages(augmented_images)\n\n    # 7\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(Flatten())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(2))\n    \n    model.compile(optimizer='adam',\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 metrics=['accuracy'])\n    model.summary()\n\n    # 8\n    history = model.fit(x = train_data_gen, \n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 epochs = epochs,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 validation_data = val_data_gen)\n    acc = history.history['accuracy']\n    print(acc)\n\n    # 9\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    \n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs_range = range(epochs)\n    print(epochs_range)\n    print(acc)\n    \n    plt.figure(figsize=(8, 8))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.show()\n\n    #10\n    probabilities = model.predict(test_data_gen)\n    print(probabilities)\n    probabilities = np.argmax(probabilities, axis = 1)\n    sample_test_images, _ = next(test_data_gen)\n    plotImages(sample_test_images, probabilities=probabilities)\n\n    # 11\n    answers = \u00a0[1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0, 0, 0, 0, 0, 0]\n    \n    correct = 0\n    \n    for probability, answer in zip(probabilities, answers):\n    \u00a0 if round(probability) == answer:\n    \u00a0 \u00a0 correct +=1\n    \n    percentage_identified = (correct \/ len(answers)) * 100\n    \n    passed_challenge = percentage_identified >= 63\n    \n    print(f\"Your model correctly identified {round(percentage_identified, 2)}% of the images of cats and dogs.\")\n    \n    if passed_challenge:\n    \u00a0 print(\"You passed the challenge!\")\n    else:\n    \u00a0 print(\"You haven't passed yet. Your model should identify at least 63% of the images. Keep trying. You will get it!\")","response":"Your loss is computed from logits:\n\n    from_logits=True\n\nTherefore, your model is trained to output logits and not probabilities.\n\nA logit for a given class is a logarithm of ratio of probability of being positive class (1) to probability of being negative class (0). As such, a logit isn't directly intrepretable as a certain probability in a binary\/multi-class setting but shows the model's uncertainty with regards to a single given class. Higher magnitude of a logit corresponds to higher certainty for a given class. Logits can be any real number.\n\nPeople already suggested to use softmax to convert logits to probabilities.","category":"misc"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"SEFR was originally designed to be extremely time and resource-efficient. Because of that, it has been implemented in numerous microcontroller applications. But apart from that, SEFR is also a good weak learner for boosting. It is a minimalistic building block, and by future improvements, it can handle interactions as well.","category":"misc"}
{"instruction":"[D] Seminal papers list since 2018 that will be considered cannon in the future","context":"Hi there,\n\nA recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML\/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  \n\nWhere should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","response":"ULMFIT was seminal in bringing transfer learning to NLP. Happened right before Bert and friends iirc","category":"misc"}
{"instruction":"[D] The usefulness of the last linear layer of each transformer layer","context":"This is a pretty obvious.\n\nI recently see that the last linear layer of transformer is kind of a waste of parameters.\n\nA transformer model is a stack of many transformer layers.\n\nThese layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\\_model \\* d\\_dim\\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.\n\nWe all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.\n\nSo why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","response":"How does it fix that?","category":"misc"}
{"instruction":"[D] What\u2019s the best cloud compute service for hobby projects?","context":"Hi everyone!\n\nI\u2019m a research engineer working mainly on Computer Vision applications. I want to start experimenting with models or tasks I\u2019m not an expert in as a side project, but I don\u2019t have a GPU on my personal laptop, and I\u2019d like to perform some small-to-medium training experiments at least. Just to give you an idea of the models I want to train:\n\n- NeRFs and Gaussian Splats\n- Diffusion models\n- Some small transformer models (Think Llama-3 8b and less).\n\nConsidering the scale of the projects I have in mind, anything above an A100 is probably an overkill.\n\nUntil a few weeks ago I was using colab pro, but I didn\u2019t really like the fact that I had to store stuff on my google drive and I\u2019d like to have something where I can at least access the terminal and not being limited just to jupyter notebooks.\n\nIn your opinion, what\u2019s a good cloud provider at a good cost for these sort of projects?","response":"Tensordock is a good choice to get cheap GPUs but sometimes availability is an issue","category":"general_qa"}
{"instruction":"[D] Please consider signing this letter to open source AlphaFold3","context":"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g\/viewform\n\nGoogle DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.\n\nAF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.\n\nPlease sign the letter !\n\nAF3 : https:\/\/www.nature.com\/articles\/s41586-024-07487-w","response":"Thanks for raising this issue, it is important, even if open-source ideologues would prefer to plant their heads in the sand and pretend that if they don't talk about something that it isn't an issue.","category":"misc"}
{"instruction":"[D] Dealing with conflicting training configurations in reference works.","context":"I am working on active learning for object detection, and I am at the stage where I need to setup my training configuration to run the experiments. I am not planning on rerunning the experiments of the other works because I don't have the compute, nor time. But I will still be comparing my results with theirs, and for that I will have to follow the training configurations used in those works.\n\nThe problem is different papers report different configurations, although they are comparing their results with each other. The paper that other methods usually compare themselves with is the [MI-AOD - CVPR21](https:\/\/github.com\/yuantn\/MI-AOD) paper, since it is the first AL method for object detection in CVPR. For RetinaNet, they train 26 epochs with LR of 0.001, stepping by 0.1 at epoch 20.\n\nThen comes the [CVPR22 ](https:\/\/arxiv.org\/abs\/2204.07965)paper which uses the standard 1x schedule for RetinaNet training (12 epochs, 0.02 LR, and steps at epoch 8 and 11). Yet, they're comparing their results with the MI-AOD paper and it doesn't seem like they rerun the experiments with their settings because the mAP looks exactly the same as the one reported in the original. I can only judge it by looks because they only show the comparison as plots of mAP in each AL cycle and don't write down the values in the table. They also don't have the code published.\n\nThen you have [PPAL - CVPR24](https:\/\/arxiv.org\/abs\/2211.11612) that claims to use the same config as MI-AOD, [but in their code](https:\/\/github.com\/ChenhongyiYang\/PPAL\/blob\/15875ed7a524675bc6daeba79b3716a0abca2b64\/configs\/voc_active_learning\/al_train\/retinanet_26e.py#L22) they're using an LR of 0.002 instead of 0.001 like they claim in the paper. And they also compare their results with the last two, despite differing configs and it doesn't seem like they rerun the experiments here either (again plots only, no table).\n\nThere are also several other works outside of CVPR, and they usually tend to follow the MI-AOD settings.\n\nMy question is, since the above three are all in CVPR, I would be required to at least compare my method with theirs, but how do I decide what config to use? Do I just follow the latest CVPR one as reported in their paper and use their reported results for the previous works for comparison?","response":"I have read it and referenced it in my literature review. It's a nice paper. But they don't test it on RetinaNet, just SSD, so no reference configs. I want to test on RetinaNet. Might try SSD too if I have time later.","category":"misc"}
{"instruction":"[D] Dealing with conflicting training configurations in reference works.","context":"I am working on active learning for object detection, and I am at the stage where I need to setup my training configuration to run the experiments. I am not planning on rerunning the experiments of the other works because I don't have the compute, nor time. But I will still be comparing my results with theirs, and for that I will have to follow the training configurations used in those works.\n\nThe problem is different papers report different configurations, although they are comparing their results with each other. The paper that other methods usually compare themselves with is the [MI-AOD - CVPR21](https:\/\/github.com\/yuantn\/MI-AOD) paper, since it is the first AL method for object detection in CVPR. For RetinaNet, they train 26 epochs with LR of 0.001, stepping by 0.1 at epoch 20.\n\nThen comes the [CVPR22 ](https:\/\/arxiv.org\/abs\/2204.07965)paper which uses the standard 1x schedule for RetinaNet training (12 epochs, 0.02 LR, and steps at epoch 8 and 11). Yet, they're comparing their results with the MI-AOD paper and it doesn't seem like they rerun the experiments with their settings because the mAP looks exactly the same as the one reported in the original. I can only judge it by looks because they only show the comparison as plots of mAP in each AL cycle and don't write down the values in the table. They also don't have the code published.\n\nThen you have [PPAL - CVPR24](https:\/\/arxiv.org\/abs\/2211.11612) that claims to use the same config as MI-AOD, [but in their code](https:\/\/github.com\/ChenhongyiYang\/PPAL\/blob\/15875ed7a524675bc6daeba79b3716a0abca2b64\/configs\/voc_active_learning\/al_train\/retinanet_26e.py#L22) they're using an LR of 0.002 instead of 0.001 like they claim in the paper. And they also compare their results with the last two, despite differing configs and it doesn't seem like they rerun the experiments here either (again plots only, no table).\n\nThere are also several other works outside of CVPR, and they usually tend to follow the MI-AOD settings.\n\nMy question is, since the above three are all in CVPR, I would be required to at least compare my method with theirs, but how do I decide what config to use? Do I just follow the latest CVPR one as reported in their paper and use their reported results for the previous works for comparison?","response":"I mean that's the same effective batch size so I think that's totally fine. Yeah for one method you'll update the batch norm mean\/std twice as often (with half the sample count each) but I cannot imagine that being too much of an issue. I gues you can't just switch to layer norm?","category":"misc"}
{"instruction":"[D] ACL 2024 Decisions","context":"Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? \ud83c\uddf9\ud83c\udded\ud83d\udc18","response":"Congrats, same here. I had a question, do they reveal about the best and outstanding papers now or later?","category":"misc"}
{"instruction":"[D] Is BERT still relevant in 2024 for an EMNLP submission?","context":"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being \"out of date\"?\n\nMy idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","response":"Because BERT has less than a bajillion parameters","category":"misc"}
{"instruction":"[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?","context":"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.\n\nEDIT: I am mainly working with regression problems.\n\nThanks in advance! :)","response":"I liked [https:\/\/arxiv.org\/abs\/2402.19460](https:\/\/arxiv.org\/abs\/2402.19460)","category":"general_qa"}
{"instruction":"[D] GPT-4o \"natively\" multi-modal, what does this actually mean?","context":"What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?\n\nE.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system \"self-select\" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","response":"They use VQVAE. It puts it into tokens then they reserve a space for their embedding to register the tokens.Which means they trained it on these tokens instead of using something like a text captioning model.","category":"general_qa"}
{"instruction":"[D] Seminal papers list since 2018 that will be considered cannon in the future","context":"Hi there,\n\nA recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML\/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  \n\nWhere should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","response":"Why would GPT 2 and 3 be seminal? The only real change from the original GPT was scale iirc.","category":"misc"}
{"instruction":"[D] Dealing with conflicting training configurations in reference works.","context":"I am working on active learning for object detection, and I am at the stage where I need to setup my training configuration to run the experiments. I am not planning on rerunning the experiments of the other works because I don't have the compute, nor time. But I will still be comparing my results with theirs, and for that I will have to follow the training configurations used in those works.\n\nThe problem is different papers report different configurations, although they are comparing their results with each other. The paper that other methods usually compare themselves with is the [MI-AOD - CVPR21](https:\/\/github.com\/yuantn\/MI-AOD) paper, since it is the first AL method for object detection in CVPR. For RetinaNet, they train 26 epochs with LR of 0.001, stepping by 0.1 at epoch 20.\n\nThen comes the [CVPR22 ](https:\/\/arxiv.org\/abs\/2204.07965)paper which uses the standard 1x schedule for RetinaNet training (12 epochs, 0.02 LR, and steps at epoch 8 and 11). Yet, they're comparing their results with the MI-AOD paper and it doesn't seem like they rerun the experiments with their settings because the mAP looks exactly the same as the one reported in the original. I can only judge it by looks because they only show the comparison as plots of mAP in each AL cycle and don't write down the values in the table. They also don't have the code published.\n\nThen you have [PPAL - CVPR24](https:\/\/arxiv.org\/abs\/2211.11612) that claims to use the same config as MI-AOD, [but in their code](https:\/\/github.com\/ChenhongyiYang\/PPAL\/blob\/15875ed7a524675bc6daeba79b3716a0abca2b64\/configs\/voc_active_learning\/al_train\/retinanet_26e.py#L22) they're using an LR of 0.002 instead of 0.001 like they claim in the paper. And they also compare their results with the last two, despite differing configs and it doesn't seem like they rerun the experiments here either (again plots only, no table).\n\nThere are also several other works outside of CVPR, and they usually tend to follow the MI-AOD settings.\n\nMy question is, since the above three are all in CVPR, I would be required to at least compare my method with theirs, but how do I decide what config to use? Do I just follow the latest CVPR one as reported in their paper and use their reported results for the previous works for comparison?","response":"What do you think about batch size? Can I compare my method that I ran with a batch size of 8 with gradient accumulation of 2, with the other works that use batch size of 16 (usually 2 distributed over 8 GPUs)? RetinaNet has batch norm layers, so it is affected by batch size. Although it might be negligible.\n\nI am having trouble with VRAM too (I have really limited compute, self-funded, and didn't write my method to be compatible with multi-GPU training), so I am wondering if this is possible.","category":"misc"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"Except it isn't the same as label encoding. In fact, none of the three major boosting implementations use one-hot encoding style of handling categorical variables.\n\nLightGBM uses partition split, which for regression trees can efficiently check the partition of the set into two maximum homogeneity subsets, see [the docs](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Advanced-Topics.html#categorical-feature-support) and the original paper: \"On Grouping for Maximum Homogeneity\" W. Fisher. XGBoost [also offers](https:\/\/xgboost.readthedocs.io\/en\/stable\/tutorials\/categorical.html#optimal-partitioning) partition split for categorical variables, with the same algorithm.\n\nYou could use one-hot encoding, but then to represent \"variable has value A or B, and not C\" you would have to use 2 or 3 splits, whereas with partition split you only use one.\n\nCatBoost, on the other hand, uses [Ordered Target Encoding](https:\/\/github.com\/catboost\/catboost\/blob\/master\/catboost\/tutorials\/categorical_features\/categorical_features_parameters.ipynb) instead, described in the linked notebook. It can also combine them during learning, but I don't know the details.","category":"misc"}
{"instruction":"[D] GPT-4o \"natively\" multi-modal, what does this actually mean?","context":"What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?\n\nE.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system \"self-select\" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","response":"How does it know to output e.g. only text tokens?","category":"general_qa"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"One component would be the new tokenizer (more for languages other than English). Less tokens per string means faster generation.","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"I doubt people are doing this enough to mess up the rankings lol","category":"misc"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"Which conferences or journals have this as a hard requirement?","category":"general_qa"}
{"instruction":"[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?","context":"Hi all,\n\nWhile reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.\n\n\"Widespread voltage control problems and protective system problems can occur,\" NOAA warns. \"Some grid systems may experience complete collapse or blackouts. Transformers may experience damage.\"\u00a0\n\nI'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","response":"This reminds me of the scifi novelette, 'For a Breath I Tarry', where the protagonist AI is described in the beginning as being an anomaly due to construction during a solar storm or flare or something like that.","category":"misc"}
{"instruction":"ML Feature Compression [D]","context":"Hey All,\n\nWe know that feature reduction\/Compression can be used via AutoEncoders, SVD, PCA, etc. \n\n- Are there any methods that anyone can think of other than these that have worked for them?\n- When using feature reduction, are there any techniques\/gotcha\u2019s that you\u2019ve learned over the years that you\u2019d want to share?","response":"One thing that I have found to help with dimensionality in Neural Networks is semi supervision or self supervision. You essentially put your inputs in, reduce dimensionality while corrupting \/ dropping information. Then use the reduce composition to try and recreate the inputs in a decoder and use some sort of distance as your loss (MSE, cosine, ect..). I like to warm up the network with self supervision then move to a semi supervision model to get really strong features for other algorithms.","category":"misc"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"What are your thoughts on obfuscating the public releases of your code? I had a collaborator recommend this and tbh it felt weird to me. I'm all for FOSS but at the same time agree with your point that as a PhD student I can undermine myself by publicizing code that I'm going to use for future papers.","category":"general_qa"}
{"instruction":"[D] How do unets achieve spatial consistency?","context":"Hi,\nI have been reading through unet pytorch implementations here https:\/\/github.com\/lucidrains\/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever \u201eknows\u201c its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?\n\nSo when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?\n\nI think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels \u201enot enough\u201c. \n\nNeither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.\n\nSo How can the unet achieve spatial consistency\/spatial  auto-conditioning?\n\nThanks","response":"There is also downsampling, which doubles the width of subsequent convolutional receptive fields.","category":"general_qa"}
{"instruction":"[D] ACL 2024 Decisions","context":"Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? \ud83c\uddf9\ud83c\udded\ud83d\udc18","response":"I see, yes the meta reviewer gave the nomination, although I am not the first author. Thanks for the info!","category":"misc"}
{"instruction":"[R] Marcus Hutter's work on Universal Artificial Intelligence","context":"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https:\/\/www.amazon.co.uk\/Universal-Artificial-Intelligence-Algorithmic-Probability\/dp\/3540221395) in 2005 and [one](https:\/\/www.amazon.co.uk\/Introduction-Universal-Artificial-Intelligence-Robotics\/dp\/1032607025\/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of \"universal intelligence\" in their paper [https:\/\/arxiv.org\/abs\/0712.3329](https:\/\/arxiv.org\/abs\/0712.3329)\n\nIn my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:\n\nhttps:\/\/preview.redd.it\/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49\n\nYoutube:\u00a0[https:\/\/www.youtube.com\/watch?v=7TgOwMW\\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https:\/\/www.youtube.com\/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)\n\n  \nOutline:\n\nI. Introduction\n\n* 00:38 : Biography\n* 01:45 : From Physics to AI\n* 03:05 : Hutter Prize\n* 06:25 : Overview of Universal Artificial Intelligence\n* 11:10 : Technical outline\n\nII. Universal Prediction\n\n* 18:27 : Laplace\u2019s Rule and Bayesian Sequence Prediction\n* 40:54 : Different priors: KT estimator\n* 44:39 : Sequence prediction for countable hypothesis class\n* 53:23 : Generalized Solomonoff Bound (GSB)\n* 57:56 : Example of GSB for uniform prior\n* 1:04:24 : GSB for continuous hypothesis classes\n* 1:08:28 : Context tree weighting\n* 1:12:31 : Kolmogorov complexity\n* 1:19:36 : Solomonoff Bound & Solomonoff Induction\n* 1:21:27 : Optimality of Solomonoff Induction\n* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines\n* 1:28:37 : Large Language Models (LLMs)\n* 1:37:07 : Using LLMs to emulate Solomonoff induction\n* 1:41:41 : Loss functions\n* 1:50:59 : Optimality of Solomonoff induction revisited\n* 1:51:51 : Marvin Minsky\n\nIII. Universal Agents\n\n* 1:52:42 : Recap and intro\n* 1:55:59 : Setup\n* 2:06:32 : Bayesian mixture environment\n* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy\n* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)\n* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence\n* 2:15:35 : AIXI explicit formula\n* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)\n* 2:33:09 : Multiagent setting\n* 2:39:38 : Grain of Truth problem\n* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria\n* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs\n* 2:56:13 : Outro: Brief philosophical remarks","response":"I mean this is not that uncommon... I personally know many professors who wrote their own Wikipedia article","category":"misc"}
{"instruction":"[D] Those in the industry, how are you using open source LLMs?","context":"Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: \n\n- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of\n- Do you see any performance\/cost gains using fine tuned open source?\n- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?\n- Anything else that might be relevant to using your own LLMs\n\nI'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","response":"Oper ai","category":"general_qa"}
{"instruction":"[D] Thoughts on DSPy","context":"I have been tinkering with DSPy and thought I will share my 2 cents here for anyone who is planning to explore it:\n\nThe core idea behind DSPy are two things:\n\n1.\t\u2060Separate programming from prompting\n2.\t\u2060incorporate some of the best practice prompting techniques under the hood and expose it as a \u201csignature\u201d\n\nImagine working on a RAG. Today, the typical approach is to write some retrieval and pass the results to a language model for natural language generation. But, after the first pass, you realize it\u2019s not perfect and you need to iterate and improve it. Typically, there are 2 levers to pull:\n\n1.\t\u2060Document Chunking, insertion and Retrieval strategy\n2.\t\u2060Language model settings and prompt engineering\n\nNow, you try a few things, maybe document the performance in a google sheet, iterate and arrive at an ideal set of variables that gives max accuracy.\n\nNow, let\u2019s say after a month, model upgrades, and all of a sudden the accuracy of your RAG regresses. Again you are back to square one, cos you don\u2019t know what to optimize now - retrieval or model? You see what the problem is with this approach? This is a very open ended, monolithic, brittle and unstructured way to optimize and build language model based applications.\n\nThis is precisely the problem DSPy is trying to solve. Whatever you can achieve with DSPy can be achieved with native prompt engineering and program composition techniques but it is purely dependent on the programmers skill. But DSPy provides native constructs which anyone can learn and use for trying different techniques in a systematic manner.\n\nDSPy the concept:\n\nSeparate prompting from programming and signatures\n\nDSPy does not do any magic with the language model. It just uses a bunch of prompt templates behind the scenes and exposes them as signatures. Ex: when you write a signature like \u2018context, question -> answer\u2019, DSPy adds a typical RAG prompt before it makes the call to the LLM. But DSPy also gives you nice features like module settings, assertion based backtracking and automatic prompt optimization.\n\nBasically, you can do something like this with DSPy,\n\n\u201cGiven a context and question, answer the following question. Make sure the answer is only \u201cyes\u201d or \u201cno\u201d\u201d. If the language model responds with anything else, traditionally we prompt engineer our way to fix it. In DSPy, you can assert the answer for \u201cyes\u201d or \u201cno\u201d and if the assertion fails, DSPy will backtrack automatically, update the prompt to say something like, \u201cthis is not a correct answer- {previous_answer} and always only respond with a \u201cyes\u201d or \u201cno\u201d\u201d and makes another language model call which improves the LLMs response because of this newly optimized prompt. In addition, you can also incorporate things like multi hops in your retrieval where you can do something like \u201cretrieve -> generate queries and then retrieve again using the generated queries\u201d for n times and build up a larger context to answer the original question.\n\nObviously, this can also be done using usual prompt engineering and programming techniques, but the framework exposes native easy to use settings and constructs to do these things more naturally. DSPy as a concept really shines when you are composing a pipeline of language model calls where prompt engineering the entire pipeline or even module wise can lead to a brittle Pipeline.\n\nDSPy the Framework:\n\nNow coming to the framework which is built in python, I think the framework as it stands today is\n\n1.\t\u2060Not production ready\n2.\t\u2060Lacks clear documentation\n3.\t\u2060Poorly designed with not so clean interfaces and abstractions\n\nTo me it felt like a rushed implementation with little thought for design thinking, testing and programming principles. The framework code is very hard to understand with a lot of meta programming and data structure parsing and construction going behind the scenes that are scary to run in production.\n\nThis is a huge deterrent for anyone trying to learn and use this framework. But, I am sure the creators are thinking about all this and are working to reengineer the framework. There\u2019s also a typescript implementation of this framework that is fairly less popular but has a much better and cleaner design and codebase:\n\nhttps:\/\/github.com\/dosco\/llm-client\/\n\nMy final thought about this framework is, it\u2019s a promising concept, but it does not change anything about what we already know about LLMs. Also, hiding prompts as templates does not mean prompt engineering is going away, someone still needs to \u201cengineer\u201d the prompts the framework uses and imo the framework should expose these templates and give control back to the developers that way, the vision of separate programming and prompting co exists with giving control not only to program but also to prompt.\n\nFinally, I was able to understand all this by running DSPy programs and visualizing the LLM calls and what prompts it\u2019s adding using my open source tool - https:\/\/github.com\/Scale3-Labs\/langtrace . Do check it out and let me know if you have any feedback.","response":"Nice write up. I did a presentation on DSPy for my coworkers a while back. It is very interesting.\n\n\nEdit:\nI kind of understand your criticism of the documentation, however, it is a product of a The\u00a0 Stanford NLP lab, which is impressive for how much the grad students have done on it. They have made tremendous improvements by leaps and bounds since their DSP paper. hopefully the PI has students\/postdocs who continue to work on it after the lead author moves on.","category":"misc"}
{"instruction":"[D] Seminal papers list since 2018 that will be considered cannon in the future","context":"Hi there,\n\nA recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML\/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  \n\nWhere should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","response":"If anyone tries to open it, remove the column at the end if it doesn't work","category":"misc"}
{"instruction":"ML Feature Compression [D]","context":"Hey All,\n\nWe know that feature reduction\/Compression can be used via AutoEncoders, SVD, PCA, etc. \n\n- Are there any methods that anyone can think of other than these that have worked for them?\n- When using feature reduction, are there any techniques\/gotcha\u2019s that you\u2019ve learned over the years that you\u2019d want to share?","response":"Check out some spectral \u201cclustering\u201d methods!\n\nThese methods (Laplacian Eigenmaps, Diffusion Maps) are more or less based in the following steps-\n\n1) build a graph on the data (typically by taking a Gaussian kernel over pairs of points, but there are many variations)\n\n2) compute the graph Laplacian (or some normalized Laplacian, or a normalized transition markov matrix)\n\n3) perform PCA (or SVD when applicable) to obtain eigenvectors, which contain the new features.\n\nThey\u2019re called clustering methods, but in reality the  graph laplacian is an extremely powerful object, and its spectra describes various aspects of the geometry of a dataset. These methods are highly utilized on lots of datasets, such as single-cell rna sequencing data, financial data, seismic data, medical images & video & more. In fact word2vec  (and some variants) which is widely used for text data is prove-ably a spectral method!\n\nThese are very cool from a theoretical standpoint- especially Diffusion Maps, which learns features of the geometry of how the data is organized by relating a diffusion and markov operator on the data, and therefore organizes the data by asking the question- how would heat propagate through the graph of this data? (It actually models solutions to the heat equation on the \u201cintrinsic manifold\u201d that the data is \u201csampled\u201d from). The nice thing about diffusion maps is that it preserves a metric on the data.\n\nThis all leads into manifold learning methods (of which there are many), there are lots of cool variants of all these methods that have been extended.\n\nHere are some sources-\n\n[nice tutorial](https:\/\/towardsdatascience.com\/spectral-clustering-for-beginners-d08b7d25b4d8)\n\n[Diffusion Maps](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1063520306000546)\n\n[Laplacian eigenmaps](https:\/\/www2.imm.dtu.dk\/projects\/manifold\/Papers\/Laplacian.pdf)\n\n[Short paper on local vs global feature embedding](https:\/\/www.pnas.org\/doi\/epdf\/10.1073\/pnas.0709842104)\n\n[word2vec uses the graph spectra](https:\/\/arxiv.org\/pdf\/2002.12317)","category":"misc"}
{"instruction":"[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?","context":"Hi all,\n\nWhile reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.\n\n\"Widespread voltage control problems and protective system problems can occur,\" NOAA warns. \"Some grid systems may experience complete collapse or blackouts. Transformers may experience damage.\"\u00a0\n\nI'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","response":"That was a nice laugh. \n\nI miss what this sub used to be.","category":"misc"}
{"instruction":"[D] Correct interpretation of Model.predict output.","context":"Currently taking the FCC Machine Learning Course.\n\nI dont know how to correctly interpret the probabilities of the Model.predict function output. The CNN is meant to determine whether is an image of a cat or a dog. Some probabilities are negative and very low. I dont know how to interpret that data.  \nAlso mention the model achieves its goal with an aceptable margin.  \nHere an example:\n\nhttps:\/\/preview.redd.it\/yyss5v202y0d1.png?width=759&format=png&auto=webp&s=b1141a6e7150fcfdf0b65dc2119de956a9fc8ec8\n\nHere the code.\n\nLink of images:\n\n    wget https:\/\/cdn.freecodecamp.org\/project-data\/cats-and-dogs\/cats_and_dogs.zip\n\nCode:\n\n    # 3\n    train_image_generator = ImageDataGenerator(rescale=1.\/255)\n    validation_image_generator = ImageDataGenerator(rescale=1.\/255)\n    test_image_generator = ImageDataGenerator(rescale=1.\/255)\n    \n    train_data_gen = train_image_generator.flow_from_directory(\n    \u00a0 \u00a0 train_dir,\n    \u00a0 \u00a0 target_size=(IMG_HEIGHT,IMG_WIDTH),\n    \u00a0 \u00a0 batch_size = batch_size,\n    \u00a0 \u00a0 class_mode = 'binary')\n    val_data_gen = validation_image_generator.flow_from_directory(\n    \u00a0 \u00a0 directory = validation_dir,\n    \u00a0 \u00a0 target_size=(IMG_HEIGHT,IMG_WIDTH),\n    \u00a0 \u00a0 batch_size = batch_size,\n    \u00a0 \u00a0 class_mode = 'binary')\n    test_data_gen = test_image_generator.flow_from_directory(\n    \u00a0 \u00a0 directory=test_dir,\n    \u00a0 \u00a0 target_size=(IMG_HEIGHT,IMG_WIDTH),\n    \u00a0 \u00a0 batch_size = batch_size,\n    \u00a0 \u00a0 class_mode = 'binary',\n    \u00a0 \u00a0 shuffle=False)\n\n    # 4\n    def plotImages(images_arr, probabilities = False):\n    \u00a0 \u00a0 fig, axes = plt.subplots(len(images_arr), 1, figsize=(5,len(images_arr) * 3))\n    \u00a0 \u00a0 if probabilities is False:\n    \u00a0 \u00a0 \u00a0 for img, ax in zip( images_arr, axes):\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.imshow(img)\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.axis('off')\n    \u00a0 \u00a0 else:\n    \u00a0 \u00a0 \u00a0 for img, probability, ax in zip( images_arr, probabilities, axes):\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.imshow(img)\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.axis('off')\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 if probability > 0.5:\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.set_title(\"%.2f\" % (probability*100) + \"% dog\")\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 else:\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 ax.set_title(\"%.2f\" % ((1-probability)*100) + \"% cat\")\n    \u00a0 \u00a0 plt.show()\n    \n    sample_training_images, _ = next(train_data_gen)\n    plotImages(sample_training_images[:5])\n    \n    \n    \n\n    # 5\n    train_image_generator = train_image_generator = ImageDataGenerator(\n    \u00a0 \u00a0 rotation_range = 360,\n    \u00a0 \u00a0 horizontal_flip = True,\n    \u00a0 \u00a0 vertical_flip = True,\n    \u00a0 \u00a0 zoom_range = 0.2,\n    \u00a0 \u00a0 shear_range = 60,\n    \u00a0 \u00a0 rescale=1.\/255)\n    \n    \n\n    # 6\n    train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0directory=train_dir,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0target_size=(IMG_HEIGHT, IMG_WIDTH),\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0class_mode='binary')\n    \n    augmented_images = [train_data_gen[0][0][0] for i in range(5)]\n    \n    plotImages(augmented_images)\n\n    # 7\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(Flatten())\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(2))\n    \n    model.compile(optimizer='adam',\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 metrics=['accuracy'])\n    model.summary()\n\n    # 8\n    history = model.fit(x = train_data_gen, \n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 epochs = epochs,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 validation_data = val_data_gen)\n    acc = history.history['accuracy']\n    print(acc)\n\n    # 9\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    \n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    \n    epochs_range = range(epochs)\n    print(epochs_range)\n    print(acc)\n    \n    plt.figure(figsize=(8, 8))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, acc, label='Training Accuracy')\n    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.show()\n\n    #10\n    probabilities = model.predict(test_data_gen)\n    print(probabilities)\n    probabilities = np.argmax(probabilities, axis = 1)\n    sample_test_images, _ = next(test_data_gen)\n    plotImages(sample_test_images, probabilities=probabilities)\n\n    # 11\n    answers = \u00a0[1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n    \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0, 0, 0, 0, 0, 0]\n    \n    correct = 0\n    \n    for probability, answer in zip(probabilities, answers):\n    \u00a0 if round(probability) == answer:\n    \u00a0 \u00a0 correct +=1\n    \n    percentage_identified = (correct \/ len(answers)) * 100\n    \n    passed_challenge = percentage_identified >= 63\n    \n    print(f\"Your model correctly identified {round(percentage_identified, 2)}% of the images of cats and dogs.\")\n    \n    if passed_challenge:\n    \u00a0 print(\"You passed the challenge!\")\n    else:\n    \u00a0 print(\"You haven't passed yet. Your model should identify at least 63% of the images. Keep trying. You will get it!\")","response":"Apply a softmax. For instance, in the first entry of your probabilities:\n\n    # Create tensor [-2.02e-01 -5.5332e-01]\n    x = torch.tensor([-2.0200e-01, -5.5332e-01])\n    # Softmax\n    softmax = nn.Softmax(dim=0)\n    softmax(x) # tensor([0.5869, 0.4131])\n\nOnly after the softmax can they be considered as probabilities, allowing you to do the argmax to get the classification.","category":"misc"}
{"instruction":"[D] Seminal papers list since 2018 that will be considered cannon in the future","context":"Hi there,\n\nA recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML\/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  \n\nWhere should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","response":"I remember when considerable parts of the big conferences were committed to the field of meta-learning.\n\nWell, since the \"Language Models are Few-Shot Learners\", that is completely gone. Solved problem. The title is one of those things that seems obvious in hindsight, but it wasn't in 2020.","category":"misc"}
{"instruction":"Does every TTS tool need reference voice along with model to run ? [D]","context":"Does every TTS tool need reference voice along with model to run ?\n\ndoes every text to speech tool need reference voice along with model to work  \neven if you have model for that voice u still  need the reference voice of it   or just the model is enough ?","response":"thank bro  , you explained it to me better than most youtube video i saw","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"Sure, I agree. Just saying we should be sceptical about the increase in performance. It is way faster though (which is not very important to me at least).","category":"misc"}
{"instruction":"[D] ACL 2024 Decisions","context":"Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? \ud83c\uddf9\ud83c\udded\ud83d\udc18","response":"If you submitted through ARR I believe you can resubmit it to EMNLP without any changes, keeping the previews reviews and meta-review. I would a assume a 4 is worth keeping unless the paper is exceptional (and even then it can be kind of random).","category":"misc"}
{"instruction":"[D] Neurips 2024 submissions","context":"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it\u2019s not the case! 7000 submissions already?!","response":"Damn, 12:07AM EST, May15. It's apparently over 14000.","category":"misc"}
{"instruction":"[D] Running large models on Mac for prototype\/finetune","context":"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev\/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100\/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev\/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?","response":"There is also MLX, which Apple seems to be working on for their hardware specifically for ML, and it seems to be an alternative to Pytorch. Have they stopped putting effort into the Apple silicon backend in Pytorch?","category":"misc"}
{"instruction":"[D] Any reason not to submit to NeurIPS?","context":"As we all know, abstracts are due tomorrow. I'm on the fence on being able to finish a strong submission in a week. I know that I can always withdraw if reviews are bad (or if I don't feel like I have a strong submission in a week when it's due), but I'm worried that there might be a trace of the submission left online which future reviewers would be able to google. Can anyone confirm that this is only the case if you don't withdraw and instead submit a rebuttal that results in a rejection? If you withdraw from openreview, is any trace of it left online? Do you have to do some trick where you edit and scrub your submission before withdrawing? I know submission results are stochastic, so I'd like to know when, if ever, submitting is a strategic blunder.","response":"Yes it happens, this is a high volume field and not all reviewers really put in the effort, or follow the rules. However, I think it is a bit of an irrational fear. Plus, if you do end up rejected\/withdrawn and come back with a significantly improved version of your paper it should hopefully sway all but the absolute laziest of reviewers, even if they somehow saw your last submission's reviews.\n\nOn the other hand I think it is fine to hold off if you think the chance of acceptance is very low, or if you think there is a decent chance of it being accepted and you would like to improve it for personal satisfaction.","category":"misc"}
{"instruction":"What's your favorite paper at ICLR2024? [D]","context":"Way too much to keep in track..","response":"The test of time award winner - variational bayes auto encoder","category":"general_qa"}
{"instruction":"[D] Please consider signing this letter to open source AlphaFold3","context":"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g\/viewform\n\nGoogle DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.\n\nAF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.\n\nPlease sign the letter !\n\nAF3 : https:\/\/www.nature.com\/articles\/s41586-024-07487-w","response":"Could that maybe go both ways? One reason to make it inaccessible is because it works too well. Another reason to make it inaccessible is because it doesn't work very well at all.\n\nI don't know anything about Isomorphic specifically, but that's a pretty common trick among tech startups generally: claim to have mind-blowing technology in order to build hype and get investor money, but also claim that the technology is too powerful \/ you're still working on patents \/ whatever as a stopgap to prevent people from finding out that your tech doesn't actually work yet, or only works in a prohibitively restrictive subset of applications.","category":"misc"}
{"instruction":"[D] How do you get better at reading proof in the ML papers, with background in CS only?","context":"Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!\n\nEdit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","response":"Umm\u2026 no.  Domestically educated undergrads have barely enough math to identify the branch of math they\u2019re being shown.","category":"general_qa"}
{"instruction":"Tips for improving my VAE [Project]","context":"Hi everyone,\n\nI'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?\n\nAlso my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.\n\nhttps:\/\/preview.redd.it\/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557\n\nFor further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.\n\nI appreciate any suggestions!","response":"try different ways of disentanglement\n\nLike, Factor VAE, Beta TC VAE, etc..\n\nI literally have nausea even thinking of reading and understanding those loss functions in all those VAE methods...","category":"misc"}
{"instruction":"[D] ACL 2024 Decisions","context":"Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? \ud83c\uddf9\ud83c\udded\ud83d\udc18","response":"Taking my chances with meta-review of 3.","category":"misc"}
{"instruction":"[R] Marcus Hutter's work on Universal Artificial Intelligence","context":"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https:\/\/www.amazon.co.uk\/Universal-Artificial-Intelligence-Algorithmic-Probability\/dp\/3540221395) in 2005 and [one](https:\/\/www.amazon.co.uk\/Introduction-Universal-Artificial-Intelligence-Robotics\/dp\/1032607025\/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of \"universal intelligence\" in their paper [https:\/\/arxiv.org\/abs\/0712.3329](https:\/\/arxiv.org\/abs\/0712.3329)\n\nIn my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:\n\nhttps:\/\/preview.redd.it\/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49\n\nYoutube:\u00a0[https:\/\/www.youtube.com\/watch?v=7TgOwMW\\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https:\/\/www.youtube.com\/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)\n\n  \nOutline:\n\nI. Introduction\n\n* 00:38 : Biography\n* 01:45 : From Physics to AI\n* 03:05 : Hutter Prize\n* 06:25 : Overview of Universal Artificial Intelligence\n* 11:10 : Technical outline\n\nII. Universal Prediction\n\n* 18:27 : Laplace\u2019s Rule and Bayesian Sequence Prediction\n* 40:54 : Different priors: KT estimator\n* 44:39 : Sequence prediction for countable hypothesis class\n* 53:23 : Generalized Solomonoff Bound (GSB)\n* 57:56 : Example of GSB for uniform prior\n* 1:04:24 : GSB for continuous hypothesis classes\n* 1:08:28 : Context tree weighting\n* 1:12:31 : Kolmogorov complexity\n* 1:19:36 : Solomonoff Bound & Solomonoff Induction\n* 1:21:27 : Optimality of Solomonoff Induction\n* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines\n* 1:28:37 : Large Language Models (LLMs)\n* 1:37:07 : Using LLMs to emulate Solomonoff induction\n* 1:41:41 : Loss functions\n* 1:50:59 : Optimality of Solomonoff induction revisited\n* 1:51:51 : Marvin Minsky\n\nIII. Universal Agents\n\n* 1:52:42 : Recap and intro\n* 1:55:59 : Setup\n* 2:06:32 : Bayesian mixture environment\n* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy\n* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)\n* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence\n* 2:15:35 : AIXI explicit formula\n* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)\n* 2:33:09 : Multiagent setting\n* 2:39:38 : Grain of Truth problem\n* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria\n* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs\n* 2:56:13 : Outro: Brief philosophical remarks","response":",Personally I like \"An inexact result adequate for a given purpose\" as a definition for approximation (American Heritage Dictionary).\n\n[Here](https:\/\/arxiv.org\/pdf\/0909.0801) is Hutter and colleagues on approximating AIXI (Hutter's AI framework built around Solomonoff induction):\n\n> As the AIXI agent is only asymptotically computable, it is by no means an algorithmic solution to the general reinforcement learning problem. Rather it is best understood as a Bayesian optimality notion for decision making in general unknown environments. As such, its role in general AI research should be viewed in, for example, the same way the minimax and empirical risk minimisation principles are viewed in decision theory and statistical machine learning research. These principles define what is optimal behaviour if computational complexity is not an issue, and can provide important theoretical guidance in the design of practical algorithms.\n\nYou are certainly correct that we have desirable guarantees about algorithms that approximate the Mandelbrot set, but this doesn't mean it's impossible to usefully approximate things where such guarantees are harder to come by.","category":"misc"}
{"instruction":"[P] Time series forecasting","context":"Time series Forecasting\n\nHi everyone I am trying first forecasting project. \n\nI have a time series over 1 year which is made by users check-ins everyday in a physical center located on a single country\/nation.\nI want to produce synthetic data  to do forecasting and simulations.\n\nNow I would like to understand if I need to use ML  algorithm or just pick up uniformly random time and places. My understanding tells me that doing so I would lose any correlation between users-time-center location.\n\nSo I was naturally leaning towards ML.. which frameworks should I study for this?","response":"I see. I think you gave me a HUGE help with this really","category":"misc"}
{"instruction":"[D] The usefulness of the last linear layer of each transformer layer","context":"This is a pretty obvious.\n\nI recently see that the last linear layer of transformer is kind of a waste of parameters.\n\nA transformer model is a stack of many transformer layers.\n\nThese layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\\_model \\* d\\_dim\\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.\n\nWe all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.\n\nSo why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","response":"Looks like I hallucinate things;\n\nI thought about permutations and things. The first linear layer can make all sorts of permutations while the value embeddings also can makes all sort of permutations.\n\nHere is a test of approximation-ability; The MSE does not reduce to machine precision, only to 0.002.\n\n[https:\/\/drive.google.com\/file\/d\/153UGUR8Mn\\_rrtCqoMDi74EFCPzk6R\\_TN\/view?usp=sharing](https:\/\/drive.google.com\/file\/d\/153UGUR8Mn_rrtCqoMDi74EFCPzk6R_TN\/view?usp=sharing)\n\nThe implement is not optimal; just a proof of concept.","category":"misc"}
{"instruction":"[Discussion] MICCAI 2024 decisions","context":"Hi all,\n\nI thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.\n\nI got a rebuttal invitation for an application paper and all the reviewers mentioned \"lack of technical novelty\" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","response":"The early accepted papers account for 11% of all submissions, and 54% enter rebuttal. These numbers come from their emails. There should be around 30% of papers getting accepted, similar to previous conferences, so they need 19% more.","category":"misc"}
{"instruction":"[D] Please consider signing this letter to open source AlphaFold3","context":"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g\/viewform\n\nGoogle DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.\n\nAF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.\n\nPlease sign the letter !\n\nAF3 : https:\/\/www.nature.com\/articles\/s41586-024-07487-w","response":"Looks like they caved (sort of).\n\nhttps:\/\/twitter.com\/pushmeet\/status\/1790086453520691657\n\nhttps:\/\/twitter.com\/maxjaderberg\/status\/1790086549205401947\n\nThey\u2019re releasing the code and weights for *academic* use within 6 months (we\u2019ll see if this will actually materialize).\n\nMy guess is it will be per research group licensing, so not completely free to the public at large.","category":"misc"}
{"instruction":"[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?","context":"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.\n\nEDIT: I am mainly working with regression problems.\n\nThanks in advance! :)","response":"I would say deep ensembles most likely, especially if you factor in implementation complexity. This has driven the Bayesian neural network community a bit mad. \n\nI would also recommend https:\/\/arxiv.org\/abs\/2110.13572 as a possible fancier alternative","category":"general_qa"}
{"instruction":"[D] Kolmogorov Arnold Networks: A visual paper breakdown (Video)","context":"Sharing a video from my YT channel that breaks down the new KAN paper. It goes into all the core concepts required to understand the paper - the Kolmogorov Arnold Representation Theorem, Splines, MLPs, comparisons between MLPs and KANs, challenges ahead, and highlights some of the amazing properties\/results of KANs like continual learning, sparsification, symbolic regression etc.\n\nLink here: [https:\/\/youtu.be\/7zpz\\_AlFW2w](https:\/\/youtu.be\/7zpz_AlFW2w)","response":"I think being able to do symbolic regression reliably with backprop rather than genetic algorithm is in itself a great achievement, but maybe the paper wouldn't sell if framed that way...","category":"misc"}
{"instruction":"[D] GPT-4o \"natively\" multi-modal, what does this actually mean?","context":"What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?\n\nE.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system \"self-select\" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","response":"Watch [How AI 'Understands' Images (CLIP) - Computerphile](https:\/\/www.youtube.com\/watch?v=KcSXcpluDe4) and include other mediums in your thoughts.","category":"general_qa"}
{"instruction":"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","context":"Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I\u2019m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can\u2019t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?\n\nAre there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","response":"It's a consequence of agreement with experiment;  it's asymptotically equivalent to classical mechanics as planck's constant goes to zero; and it is a superset all of classical probability theory, including markovian systems.","category":"general_qa"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"Dola contrastive decoding, AnyMal, LayerSkip, H-JEPA, Rho-1, Megaladon, MixtureOfAttention, V-Jepa, Codefusion, Phi-3, Better and faster language models paper by Meta, llava-interactive, MiniCPM, Jamba, Medusa-V2, Megabyte, IWM Jepa.\n\nThat\u2019s just scratching the surface of potential directions of innovation known in the open source, over half of which have already been successfully applied and working on some commercially usable scale.","category":"misc"}
{"instruction":"[D] Are PyTorch high-level frameworks worth using?","context":"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.\n\nWould these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?\n\nIf you think they're useful, let me know which one you'd recommend.","response":"Do you use it paired with any PyTorch wrapper as well for Trainers? Just out of curiosity.","category":"misc"}
{"instruction":"[R] The Platonic Representation Hypothesis","context":"arxiv: [https:\/\/arxiv.org\/pdf\/2405.07987](https:\/\/arxiv.org\/pdf\/2405.07987)  \nProject page: [https:\/\/phillipi.github.io\/prh\/](https:\/\/phillipi.github.io\/prh\/)  \ngithub: [https:\/\/github.com\/minyoungg\/platonic-rep\/](https:\/\/github.com\/minyoungg\/platonic-rep\/)\n\nInteresting positional paper on the convergence of self-supervised, multi-modal representation learning.","response":"My understanding is their argument depends on using multiple modalities, so I wouldn't say so at all","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"Why are you comparing to GPT4-v?  The latest release is GPT-4-turbo-2024-04-09.  \n\nThe gains of gpt-4o are on par with to smaller than GPT-4-turbo-2024-04-09 compared to gpt-4-0125.","category":"misc"}
{"instruction":"Nanogpt alternative [D]","context":"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model\n\nAlso, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? \n\nThanks","response":"There's still nothing wrong with nanogpt.","category":"misc"}
{"instruction":"[Discussion] MICCAI 2024 decisions","context":"Hi all,\n\nI thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.\n\nI got a rebuttal invitation for an application paper and all the reviewers mentioned \"lack of technical novelty\" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","response":"Finally got a look at the reviews. \n\n4, 4, 1, 3\n\nI'm very new to this stuff and will let the main author deal with it. Kind of sad looking at the rejection reviews.","category":"misc"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"Yeah, junk research is a lot faster and easier than real research.","category":"general_qa"}
{"instruction":"[D] How do you get better at reading proof in the ML papers, with background in CS only?","context":"Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!\n\nEdit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","response":"100% this. Proofs the way they are written in the big ML conferences are mostly to embellish and provide a false sense of correctness\/mastery. \n\nFar too often we see a 3 page proof, just to go over the experimental session and see experiments without replications, without confidence intervals, without reproducible code. \n\nSure, in theory you just proved to me these embeddings are bounded by some epsilon in RKHS. I followed it, I'm also a mathematician. But then I ran a test with an heuristic MSE and your method isn't statistically different than another method that is simpler and easier to understand. \n\nNow what? Now the paper is getting dust and it's so arcane to understand and honest future researchers will waste their time and discover it the worst way a. Because some people wanted to be fancy and publish at \"top conferences\" b. But weren't rigorous all the way down in communicating results and experimentation.\n\nVery frustrating....","category":"general_qa"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"Not on Twitter so did not see that. I guess they are highlighting the UX\/UI components on the main page. The ELO gain is impressive if as you said no issues. But overall across all performance metrics, nothing to brag about it seems. This is the reason they are not calling this GPT-5.","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"Why do you think that? Have you seen any data supporting your claim? What an odd comment to see at the top of a MachineLearning post.","category":"misc"}
{"instruction":"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","context":"Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I\u2019m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can\u2019t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?\n\nAre there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","response":"From what I understand, the general motivation of these models is to use Linear RNN to *memorise* past inputs and still use nonlinear layers in between \/ afterwards to further process them. It turns out that linear transformations are often enough for reconstructing past inputs.","category":"general_qa"}
{"instruction":"[D] Impact of solar storm on QLORA + RLHF of Llama3 8B?","context":"Hi all,\n\nWhile reading an article on the current solar storm I came across a warning from NOAA about the impact of the storm on transformers.\n\n\"Widespread voltage control problems and protective system problems can occur,\" NOAA warns. \"Some grid systems may experience complete collapse or blackouts. Transformers may experience damage.\"\u00a0\n\nI'm currently in the process of a QLORA + RLHF sequence on Llama3 8B (we're trying to make a model that creates more efficient SQL queries from a prompt) and I was wondering what these impacts are on models like Llama3 8B. Have any of you experienced damage? What were the performance implications?","response":"I'll reply with an earnest answer because it's fun to talk about.\n\nLLMs are generally trained on supercomputers which, like any computer, [are susceptible to cosmic radiation interference](https:\/\/physicsworld.com\/a\/cosmic-challenge-protecting-supercomputers-from-an-extraterrestrial-threat\/). However, as the article mentions, supercomputers are more likely to experience ill effects from cosmic radiation given their much larger surface area.\n\nAs an administrator of several such systems, I can say that the errors that result from such radiation are either virtually non-existent, or impossible to discern. Hypothetically, if one did occur, it would likely manifest as a bit flip on a specific piece of hardware.\n\nIf the bit flip occurred in RAM, for example, it could register as an ECC error, which may or may not cause a kernel panic. If the node panics, your training job would fail and requeue, with the node being set to `down` in whichever scheduler you use. Your team would go through running diagnostics on the node, they would all pass, and you would return the node to the production queue, unable to replicate the issue.\n\nIt is far more likely that training performance will suffer from a piece of hardware that fails due to a manufacturer defect than from cosmic radiation. Eg, that ECC error you just experienced is much more likely a bad DIMM than a bit flip from space.\n\nThat being said, due to the recency of the solar storm, this was actually a topic of conversation that was brought up, albeit in jest, during a couple calls and threads at work. It was interesting to talk about and revisit some of the examples laid out in the article.\n\nTL;DR: cosmic rays caused by solar storms are a minimal, virtually imperceptible risk to LLM training supercomputers","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"When they gonna develop an AI that knows when to shut up","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"It wouldn't surprise me if they went for a set of specialized models in a Mixture of Experts (MoE) setup. It makes sense, they had a lot of data when they trained GPT 3 and 4, but they've gained one very important dataset: how people interact with LLMs. That additional value could be utilized best, I believe, in a MoE architecture, because neural nets would be able find a setup that is most efficient at splitting up the different type of tasks LLMs are used for. It's also been a trend with open-source models lately.","category":"misc"}
{"instruction":"[D] Please consider signing this letter to open source AlphaFold3","context":"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g\/viewform\n\nGoogle DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.\n\nAF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.\n\nPlease sign the letter !\n\nAF3 : https:\/\/www.nature.com\/articles\/s41586-024-07487-w","response":"I would be very surprised if it were smoke and mirrors. They have a glowing endorsement from some of the members of Rosetta, the best physics based alternative for MDM. AF has a good track record and previous models have long been known to be able to perform blind docking. If AF2 can do it the chances their new model does better isn't unlikely. The proof will be in the patents we see a few years from now.","category":"misc"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"Yes, this is in our plans!","category":"misc"}
{"instruction":"[D] We are forming a study group!","context":"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. \n\nOur aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.\n\nSo if anybody interested you can approach us.\n\nThis group is not only for beginners it's also for pro as well as they can mentor others.\n\nDM me if u want to join!","response":"Absolutely, I\u2019d like to join too. Could you send the link?","category":"misc"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"Release it when you're done milking it, imo.\nSwitching topics? Already have that next paper finished? Graduating next week? Let it loose","category":"general_qa"}
{"instruction":"Can one use squared inverse of KL divergence as another divergence metric? [D]","context":"I came across this doubt (might be dumb), but it would be great if someone can throw some light on this:\n\nThe KL Divergence between two distributions p and q is defined as : `$D_{KL}(p || q) = E_{p}[\\log \\frac{p}{q}]$`\n\ndepending on the order of p and q, the divergence is mode seeking or mode covering.\n\nHowever, can one use `$\\frac{-1}{D_{KL}(p || q)}$` as a divergence metric?\n\nOr maybe not a divergence metric (strictly speaking), but something to measure similarity\/dissimilarity between the two distributions?\n\n**Edit**:\n\nit is definitely not a divergence as `-1\/KL(p,q) <= 0` also as pointed in the discussion, `1\/KL(p,p) = +oo`.\n\nHowever, I am thinking it from this point: if `KL(p,q)` is decreasing `=>` `1\/KL(p,q)` is increasing `=>` `-1\/KL(p,q)` is decreasing. Although, `-1\/KL(p,q)` is unbounded from below hence can reach `-oo`. Question is, does the above equivalence, make `-1\/KL(p,q)` useful as a metric for any application. Or is it considered somewhere in any literature.","response":"I don't think this behavior is particularly useful. It is actually very important that the value which represents absolute similarity is pinned to some real number. Without this it means that it is difficult to say how different two distributions actually are. You would always need a third distribution to measure relative to. Think about it like this, if I gave you two distributions that are highly similar, your equation would output a very large positive number. But how far is that number from absolute similarity? Well it is infinitely far away, same as if I gave you two completely different distributions. \n\nThis also has terrible effects on programmability. It is very easy to check if something is equal to 0, it is not as easy to check if a function is going infinite. Additionally you will have big issues doing nearest neighbor search without defining some sort of scaling factor specific to your domain that prevents the distances from getting too large.","category":"misc"}
{"instruction":"[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?","context":"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.\n\nEDIT: I am mainly working with regression problems.\n\nThanks in advance! :)","response":"That repo\/code should be illegal lmfao.","category":"general_qa"}
{"instruction":"[D] ACL 2024 Decisions","context":"Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? \ud83c\uddf9\ud83c\udded\ud83d\udc18","response":"Meta review of 4 got rejected. Disappointed to not even get findings. Reviews were 4, 3.5 and 2.5, with the 2.5 being quite ridiculous (what else is new). Resubmitting to EMNLP it is.","category":"misc"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"We tested SEFR on numerous datasets with grid search on hyperparameters to find the optimal results of them. We reported some of them in the paper in arXiv, but it is consistently more accurate than the other simple algorithms.","category":"misc"}
{"instruction":"[D] Are PyTorch high-level frameworks worth using?","context":"In an attempt to better track experiment results and hyperparameters, not only did I learn about the Weights and Biases library but also ended up finding out about frameworks such as PyTorch Lightning and Ignite. I've always used raw PyTorch, so I'm not sure if these frameworks are really useful. I mostly work with academic research, right now I also need to keep track of the MAE since it's a regression problem and I don't know if these frameworks support this or let me define a custom metric.\n\nWould these frameworks be useful for me? Could it speed up the process when experimenting with different architectures?\n\nIf you think they're useful, let me know which one you'd recommend.","response":"`accelerate` and `deepspeed` are definitely good to know if you have access to distributed hardware","category":"misc"}
{"instruction":"[D] Please consider signing this letter to open source AlphaFold3","context":"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g\/viewform\n\nGoogle DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.\n\nAF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.\n\nPlease sign the letter !\n\nAF3 : https:\/\/www.nature.com\/articles\/s41586-024-07487-w","response":"I have no idea if that's true but it wouldn't be even remotely surprising if it were - the most popular publications are the ones with the most surprising results, and surprising results are also the most likely ones to be wrong.","category":"misc"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"The numbers are reported in the README of GitHub Repo. The SAMME version is very fast and it can be trained with LOOCV on many datasets. Also when the number of records\/features are not too high, SAMME.R also can be used for LOOCV. For setting these parameters, please see here:  \n[https:\/\/linearboost.readthedocs.io\/en\/latest\/usage.html#parameters](https:\/\/linearboost.readthedocs.io\/en\/latest\/usage.html#parameters)","category":"misc"}
{"instruction":"[R] The Platonic Representation Hypothesis","context":"arxiv: [https:\/\/arxiv.org\/pdf\/2405.07987](https:\/\/arxiv.org\/pdf\/2405.07987)  \nProject page: [https:\/\/phillipi.github.io\/prh\/](https:\/\/phillipi.github.io\/prh\/)  \ngithub: [https:\/\/github.com\/minyoungg\/platonic-rep\/](https:\/\/github.com\/minyoungg\/platonic-rep\/)\n\nInteresting positional paper on the convergence of self-supervised, multi-modal representation learning.","response":"Yep: https:\/\/icml.cc\/virtual\/2024\/poster\/34734","category":"misc"}
{"instruction":"[D] Seminal papers list since 2018 that will be considered cannon in the future","context":"Hi there,\n\nA recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML\/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  \n\nWhere should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","response":"\"only\" scale, as if scale hasn't been the most important idea of the decade.\n\nYou're either blind or lying to yourself if you don't see GPT-3 as a seminal paper. It kicked off the current era of hyperscaling LLMs and billion-parameter pretrained models.","category":"misc"}
{"instruction":"[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis","context":"https:\/\/arxiv.org\/pdf\/2405.08790","response":"Didn't reply to the parameters question, I think the most I tried was 3000 parameters for Poisson on an A100, in addition to being slow, it had something like 49% relative L2 error. It's a very bad sign that it fails on Poisson. The poisson equation they put in the paper converges to machine precision the minute they stopped learning for activation functions and and they report machine precision in the paper. That's cheating in my book.","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"I understand the paper has ilya on it, and I agree, they might be using a similar technique. But people publish a lot of papers, does not mean you use every technique in every product.\n\nAll I'm saying is it's totally possible to just tack an audio input head onto g4, train it on dialog, and it will likely learn to only output stuff when there is vocal input from the user. If you get a collision where they are both talking, you can use a million strategies to combine the tokens.\n\nI'm 100% *not trying to say* I know what 4o is, and you totally could be right that they're using that they're using some additional head trained with policy gradient to determine when to output speech like they do in that paper (but note, there are no 'hidden states' in transformers, so it would have the be a modified version of the paper anyway)... I'm just trying to say none of us know how much of gpt4 they recycled, and again the outputs are like token for token similar.","category":"misc"}
{"instruction":"[D] Those in the industry, how are you using open source LLMs?","context":"Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: \n\n- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of\n- Do you see any performance\/cost gains using fine tuned open source?\n- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?\n- Anything else that might be relevant to using your own LLMs\n\nI'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","response":"Damn nobody answered the question. Can you try in \/r\/LocalLLaMA? I'm curious to know the answer as well","category":"general_qa"}
{"instruction":"[D] How do you get better at reading proof in the ML papers, with background in CS only?","context":"Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!\n\nEdit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","response":"If possible, can you suggest some general resources? I struggle with understanding proofs establishing bounds\/rate of convergence etc. It\u2019s just like you said - I need some exposure about proving techniques. I\u2019m trying to read papers and learn unknown terms\/concepts which I come across but it\u2019s a rather scattered process.","category":"general_qa"}
{"instruction":"[D] We are forming a study group!","context":"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. \n\nOur aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.\n\nSo if anybody interested you can approach us.\n\nThis group is not only for beginners it's also for pro as well as they can mentor others.\n\nDM me if u want to join!","response":"I'm not a beginner and not a pro in ML. But I do not  want to forget about the Algos. Can i join \ud83d\udc49\ud83c\udffb\ud83d\udc48\ud83c\udffb","category":"misc"}
{"instruction":"[D] The usefulness of the last linear layer of each transformer layer","context":"This is a pretty obvious.\n\nI recently see that the last linear layer of transformer is kind of a waste of parameters.\n\nA transformer model is a stack of many transformer layers.\n\nThese layers starts with 3 QKV Linear Transformation and ends with FFN Network, which consists of two linear layers. The last one costs (d\\_model \\* d\\_dim\\_feedforward) parameter and multiplication and its output is linearly transformed again at the next layer.\n\nWe all know that two consecutive linear transformation is representable by one linear transformation, which is the reason why we use activation functions at all.\n\nSo why we hasn't use a super sparse linear transformation, maybe do convolution by treating the embedding dimension as sequence dimension at that particular linear transformation dimension.","response":"I thought the first linear layer and the QKV linear layers of the next layer can learn to rewire things \/ any permutation.\n\nI am wrong, these permutation learning is not sufficient.\n\n[https:\/\/drive.google.com\/file\/d\/153UGUR8Mn\\_rrtCqoMDi74EFCPzk6R\\_TN\/view?usp=sharing](https:\/\/drive.google.com\/file\/d\/153UGUR8Mn_rrtCqoMDi74EFCPzk6R_TN\/view?usp=sharing)\n\nThe MSE Loss is around 0.002; in the notebook of the small experiment I share.","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"[https:\/\/twitter.com\/LiamFedus\/status\/1790064963966370209](https:\/\/twitter.com\/LiamFedus\/status\/1790064963966370209)","category":"misc"}
{"instruction":"[D] Those in the industry, how are you using open source LLMs?","context":"Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: \n\n- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of\n- Do you see any performance\/cost gains using fine tuned open source?\n- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?\n- Anything else that might be relevant to using your own LLMs\n\nI'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","response":"What? It\u2019s the opposite","category":"general_qa"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"I think the GPT-4 paper made clear it was for both reasons.","category":"misc"}
{"instruction":"[D] Please consider signing this letter to open source AlphaFold3","context":"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g\/viewform\n\nGoogle DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.\n\nAF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.\n\nPlease sign the letter !\n\nAF3 : https:\/\/www.nature.com\/articles\/s41586-024-07487-w","response":"What they will be able to do against some nerds and the internet? If someone open source it its over. With or without patent.","category":"misc"}
{"instruction":"[D] Those in the industry, how are you using open source LLMs?","context":"Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: \n\n- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of\n- Do you see any performance\/cost gains using fine tuned open source?\n- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?\n- Anything else that might be relevant to using your own LLMs\n\nI'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","response":"cool topic","category":"general_qa"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"The natural language rendition of a GPT prompt was super awkward, the AI was clearly not engaged in the conversation and was ready to blurt entire paragraphs of drivel unless interrupted.","category":"misc"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"They confirmed in another comment that it is a linear classifier but maybe because of thresholding it's possible to combine them without just getting another linear model?","category":"misc"}
{"instruction":"[D] How do unets achieve spatial consistency?","context":"Hi,\nI have been reading through unet pytorch implementations here https:\/\/github.com\/lucidrains\/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever \u201eknows\u201c its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?\n\nSo when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?\n\nI think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels \u201enot enough\u201c. \n\nNeither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.\n\nSo How can the unet achieve spatial consistency\/spatial  auto-conditioning?\n\nThanks","response":"Convolutions effectively move this kernel left to right to to bottom, so in the output tensor after the resnet block, features in a spacial region of the output tensor are influenced by pixels in the corresponding spacial region of the input.","category":"general_qa"}
{"instruction":"[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis","context":"https:\/\/arxiv.org\/pdf\/2405.08790","response":"Which PDE benchmarks? How many parameters?","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"Their name is oPeNaI and they claim to be a non-profit organization that wants to accelerate AI research and progress.","category":"misc"}
{"instruction":"[D] How do you get better at reading proof in the ML papers, with background in CS only?","context":"Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!\n\nEdit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","response":"It was proved in 91 (iirc) actually. So more like 3 decades ago. And we use that proof to develop further proofs, such as that Transformers are Turing complete (2018 iirc). Understanding how and why things work is generally useful...","category":"general_qa"}
{"instruction":"[D] What\u2019s the best cloud compute service for hobby projects?","context":"Hi everyone!\n\nI\u2019m a research engineer working mainly on Computer Vision applications. I want to start experimenting with models or tasks I\u2019m not an expert in as a side project, but I don\u2019t have a GPU on my personal laptop, and I\u2019d like to perform some small-to-medium training experiments at least. Just to give you an idea of the models I want to train:\n\n- NeRFs and Gaussian Splats\n- Diffusion models\n- Some small transformer models (Think Llama-3 8b and less).\n\nConsidering the scale of the projects I have in mind, anything above an A100 is probably an overkill.\n\nUntil a few weeks ago I was using colab pro, but I didn\u2019t really like the fact that I had to store stuff on my google drive and I\u2019d like to have something where I can at least access the terminal and not being limited just to jupyter notebooks.\n\nIn your opinion, what\u2019s a good cloud provider at a good cost for these sort of projects?","response":"modal.com is great. Free credits every month and easy to apply in code.","category":"general_qa"}
{"instruction":"[D] Running large models on Mac for prototype\/finetune","context":"I am looking into building a solution to make it easy to use Mac machines with large VRAM to run Pytorch projects for dev\/test purposes. I understand they can't be used for running production inference or large-scale training. However, these machines are more readily available and cheaper than Nvidia GPU instances (A100\/H100) and have large VRAM so they can run Pytorch experiments. I see challenges in terms of their current usability. They can't run Pytorch container environments (similar to running Pytorch containers on Linux instances running Nvidia GPUs), and they have no management tools similar to RAY, Kubernetes so a company can't build a dev\/test rig with multiple machines for their data scientists team. With this solution, I envision companies using hosted Macs or Mac instances on AWS(with 32Gb or more VRAM) for their Data scientists to run Pytorch experiments. Am I thinking about it in the right way?","response":"With no Apple involvement, I wonder if Asahi Linux will get to that point.","category":"misc"}
{"instruction":"[D] Is BERT still relevant in 2024 for an EMNLP submission?","context":"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being \"out of date\"?\n\nMy idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","response":"I don't think people think of BERT as a LLM these days.","category":"misc"}
{"instruction":"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","context":"\\*Update\\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.\n\nI think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.\n\ntl;dr Use the parallel scan\\[1\\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\\\sum\\_{i=0}\\^{j-1} k(i)\\_a\\^n q(j)\\_b\\^m v(i)$$ and the normalization $\\\\sum\\_{i=0}\\^{j-1} k(i)\\_a\\^n q(j)\\_b\\^m$. k(i)\\_a\\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\\_b\\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \\* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.\n\n# Background\n\nI was inspired to think about this because I was implementing MAMBA\\[2\\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \\[a\\_1, a\\_2, a\\_3, a\\_4, ...\\]. You can compute all partial sums by first adding a\\_i to a\\_{i -1}, where a\\_{-1} is zero, and generally a\\_{-n} is defined to be zero. Then take the result, call it r = \\[a\\_1, a\\_1+a\\_2, a\\_2 + a\\_3, ...\\], and compute r\\_i + r\\_{i-2}, which gives \\[a\\_1, a\\_1+a\\_2, a\\_1+a\\_2+a\\_3, ...\\]. The first 4 partial sums are already complete. The next step would be r\\_i + r\\_{i-2\\*\\*2}, and the next step, just increase the power of 2 until i-2\\*\\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.\n\n# The Meat of It\n\nIn the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.\n\nLet's assume we have a tensor x of shape (sequence\\_length, embedding\\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\\[:,i\\]\\*\\*n)\\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\\[y\\[0,:\\], y\\[0,:\\]+y\\[1,:\\], ...\\]. Now multiply the result by q\\[:,j\\]\\*\\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)\\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\\\sum\\_{n, m} A\\_{n, m} q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)\\*v).\n\nWhat is left is to find the Taylor series coefficients A\\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\\\cdot k$ in place of $q\\[:,j,:\\] \\\\cdot k\\[:,i,:\\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\\\cdot k) = 1 + (q \\\\cdot k) + (q \\\\cdot k)\\*\\*2 \/ 2! + ... + (q \\\\cdot k)\\*\\*n \/ n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\\\cdot k)\\*\\*n \/n! for every n. It can be done but I'm not going to do it. Just assume that A\\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\\\sum\\_{n, m} A\\_{n, m} x\\[:,j\\]\\*\\*m \\* ParallelPartialSum((x\\[:,i\\]\\*\\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:\n\n$$  \n(\\\\sum\\_{n, m} A\\_{n, m} q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)\\*v)) \/ (\\\\sum\\_{n, m} A\\_{n, m} q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)))  \n$$\n\nWhere again, A\\_{n, m} are the Taylor series coefficients for exp( q \\\\cdot k).\n\n# Take-Aways\n\nOne big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.\n\nNon-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.\n\n\\[1\\] [https:\/\/en.wikipedia.org\/wiki\/Prefix\\_sum](https:\/\/en.wikipedia.org\/wiki\/Prefix_sum)\n\n\\[2\\] [https:\/\/arxiv.org\/abs\/2312.00752](https:\/\/arxiv.org\/abs\/2312.00752)\n\n# Update\n\nActually there is a better algorithm for the parallel scan given in the wiki link above\\[1\\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.\n\n# Update 2\n\n@[Lajamerr\\_Mittesdine](https:\/\/www.reddit.com\/user\/Lajamerr_Mittesdine\/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\\_Mittesdine](https:\/\/www.reddit.com\/user\/Lajamerr_Mittesdine\/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.\n\n    import numpy as np\n    \n    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.\n    def parallel_partial_sum(arr): \n        \"\"\"Parallel scan (prefix sum) implementation.\"\"\"\n        n = len(arr)\n        steps = np.ceil(np.log2(n))\n        \n        for i in range(steps):\n            # check if this is the numerator or denominator\n            if len(arr.shape)==2:            \n                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)\n            else:\n                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)\n    \n        return arr\n    \n    def compute_taylor_basis_function(q, k, v, n, m, i, j):\n        \"\"\"Compute a Taylor basis function for given powers n and m.\"\"\"\n        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise\n        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise\n        if len(v.shape) == 2:\n            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast\n            q_power = np.expand_dims(q_power, axis=-1)\n        partial_sum_kv = parallel_partial_sum(k_power * v)\n        basis_function = q_power * partial_sum_kv\n        return basis_function\n    \n    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):\n        \"\"\"Compute the causal self-attention using Taylor series approximation.\"\"\"\n        attention_numerator = np.zeros_like(v)\n        attention_denominator = np.zeros_like(v[:,0])\n    \n        for n in range(max_n + 1):\n            for m in range(max_m + 1):\n                for j in range(q.shape[-1]):\n                    for i in range(k.shape[-1]):\n                        # note, either i or j loop can be removed because basis functions can be computed in parallel\n                        A_nmij = 1.0  # Simplified coefficient for illustration\n                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)\n                        attention_numerator += A_nmij * basis_function\n                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)\n                        attention_denominator += A_nmij * normalization_basis_function\n        \n        attention_denominator = np.expand_dims(attention_denominator, axis=-1)\n        attention = attention_numerator \/ attention_denominator\n        return attention\n    \n    # Example usage\n    sequence_length = 10\n    embedding_dim = 4\n    \n    # Randomly initialize q, k, v tensors\n    q = np.random.rand(sequence_length, embedding_dim)\n    k = np.random.rand(sequence_length, embedding_dim)\n    v = np.random.rand(sequence_length, embedding_dim)\n    \n    # Compute the causal self-attention\n    attention_output = compute_causal_self_attention(q, k, v)\n    \n    print(\"Causal Self-Attention Output:\")\n    print(attention_output)","response":"Or even rewrite clearly by hand and upload some photos, if the Latex doesn\u2019t agree with you","category":"misc"}
{"instruction":"[Discussion] event sequence ORDER prediction","context":"I seem to have stumbled upon a problem that i can't google my way out of.\n\n**\\[MY TRAINING DATA\\]**  \nI have a dataset of bunch of sequential events. each event has 30-40 attributes, including the timestamp the event occured.\n\nuser 1: Event 1 > Event 2 > Event 3  \nuser 2: Event 1 > Event 2 > Event 3 > Event 4 > Event 5  \nuser 3: Event 1  \n....\n\n**\\[THE PROBLEM TO SOLVE\\]**  \nI have a dataset of events, but i do not know which events belongs to which users. those users are different than the training set users, but we are infering they behave the same.  \n   \nFor each event X, I need to solve for X. I need to figure out in what order that event occured. is it event 1? event 2? event 3?  \n\nif X > 1, then event X-1 is also present in the dataset, although i have no way of linking them.\n\n**\\[CURRENT APPROACH\\]**  \nmy manager is pushing to use LSTMs or transformers. I don't have much experience with them, but after doing some research i don't think its the correct approach. in fact, my research doesnt seem to have anything on this problem. am i the only one in the world who has it? ideas welcome. thanks (:","response":"thank you (: ! could you elaborate a bit on #2? any specific method i should look at?","category":"misc"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"No, boosting a linear classifier will make it better at handling complex data patterns.","category":"misc"}
{"instruction":"Tips for improving my VAE [Project]","context":"Hi everyone,\n\nI'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?\n\nAlso my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.\n\nhttps:\/\/preview.redd.it\/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557\n\nFor further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.\n\nI appreciate any suggestions!","response":"> It seems that model is overfitting when it comes to reconstruction loss\n\nWhy do you say that?\n\n---\n\nI'm really curious about that sharp inflection point that appears in all three graphs around epoch 8.","category":"misc"}
{"instruction":"[D] GPT-4o \"natively\" multi-modal, what does this actually mean?","context":"What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?\n\nE.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system \"self-select\" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","response":"Makes sense, if the basic concept is just \"tokenize everything, throw it together, apply GPT training recipe\", then doesn't seem particularly groundbreaking (tho I'm sure many sophisticated things layered on to make it work)\n\nDoing token-by-token predict->decode->send for something non-discrete like audio and having it be seamless is pretty slick","category":"general_qa"}
{"instruction":"[D] GPT-4o \"natively\" multi-modal, what does this actually mean?","context":"What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?\n\nE.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system \"self-select\" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","response":"damn good question","category":"general_qa"}
{"instruction":"[D] LWhy are Linear RNNs so performant (in terms of accuracy, not compute)? Looking for mathematical or even intuitive explanations","context":"Trying to familiarise myself with the mamba architecture, hence familiarising myself with SSMs, hence familiarising myself with Linear RNNs. I have looked over resources on SSMs, S4 and Mamba but I\u2019m unable to find an explanation. on why Linear RNNs with SSM parameterization improves performance. I can\u2019t wrap my head around it intuitively either - why are linear transformations sufficient for seq2seq tasks?\n\nAre there any exhaustive mathematical explanations, or even videos on how linear RNNs can outperform transformers on certain tasks?","response":"lol I googled it and there's this paper:\n[The Power of Linear Recurrent Neural Networks](https:\/\/arxiv.org\/pdf\/1802.03308)\n> Recurrent neural networks are a powerful means to cope with time series. We\nshow how autoregressive linear, i.e., linearly activated recurrent neural networks\n(LRNNs) can approximate any time-dependent function f (t).\n\nAs a general matter you can approximate any function by using only linear functions. The key insight is to realize that function composition, e.g. f(g(x)), is a *linear operator*. It's also called the \"Koopman operator\" and you can find machine learning-related work on this.\n\nThis works because, given some function f(x), you can think of the scalar input value x as being an (uncountably) infinite dimensional, one-hot coded vector, and the function f as an (uncountably) infinite dimensional permutation matrix. Adjacency matrices on graphs are the discrete version of this.\n\nEDIT: for the folks thinking \"hey but wait, I thought you needed a nonlinearity somewhere???\", consider this: according to quantum mechanics, the entire universe and everything in it is a linear equation. And then, for the folks thinking, \"hey but wait, isn't quantum measurement nonlinear?\": no, it's *nonunitary*, which is different.","category":"general_qa"}
{"instruction":"[D] Please consider signing this letter to open source AlphaFold3","context":"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g\/viewform\n\nGoogle DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.\n\nAF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.\n\nPlease sign the letter !\n\nAF3 : https:\/\/www.nature.com\/articles\/s41586-024-07487-w","response":"One of the reviewers agreed and was removed. \n\n>But no one besides Novartis& Lilly etc. has access to the ligand structure prediction. All the data on ligand binding in the paper is irreproducible and should not have been published. As Reviewer #3 for @nature I recommended taking it out and saving it for promotional material. https:\/\/twitter.com\/RolandDunbrack\/status\/1789081040281079942\n\n>Possibly this is why they might have demanded that #reviewer3 was removed from review of the revision, a privilege which no other set of authors would be granted by @nature\n. https:\/\/twitter.com\/RolandDunbrack\/status\/1789083883394253096\n\n\n> There is no such thing as AlphaFold3. There is only http:\/\/alphafoldserver.com\/. The difference is throttling rigorous scientific and biomedical research. https:\/\/twitter.com\/RolandDunbrack\/status\/1789884648782205140\n\n\n> I am unhappy with DeepMind on the biological science that will not be accomplished. Ok, we can\u2019t do any ligand because that may deprive them of revenue. But we can\u2019t do high-throughput benchmarks or protocol development applications to cancer or other diseases.\nhttps:\/\/twitter.com\/RolandDunbrack\/status\/1789083096865743018","category":"misc"}
{"instruction":"[D] How do you get better at reading proof in the ML papers, with background in CS only?","context":"Hi everyone, as the title, how do you get better at reading proof in the ML papers? The ML papers I mentioned are those in adversarial ML, e.g. Certified Adversarial Robustness via Randomized Smoothing. For context, I have basic knowledge of calculus, linear algebra, but most of the time when reading the proof, sometime I feel that one line just come out of nowhere, and I can't reason why or how they do it. Maybe because my background is CS, with focus on software, so I'm lacking of the rigorous proof-based math stuff. Please help!!\n\nEdit: Yes I do learn CS theory (convex optimization, discrete math,...) during undergrad, but not to the rigorous level of math in those papers. I think whether math proof is necessary or not depends on the topic, in my case, yes, it is super important, and I need advice on better at it.","response":"You assume that college is the only source of math for the curious undergrad (who, in this scenario, is ambitious enough to fill in gaps of their knowledge).","category":"general_qa"}
{"instruction":"[D] Those in the industry, how are you using open source LLMs?","context":"Wanted to get an idea of how people are using open source LLMs in production. Some of the questions I'd like to see answers to are: \n\n- Would love to get an idea of how you're using smaller models, how you're deploying them, anything else here people might not think of\n- Do you see any performance\/cost gains using fine tuned open source?\n- How tough is it to train and set up your own fine tuning pipelines? Do you use full training or PEFT methods like LoRA?\n- Anything else that might be relevant to using your own LLMs\n\nI'm an engineer trying to figure out the best way to upskill and see if my company could do more with open source models, and this would be very helpful for that. Thanks!","response":"Thanks, this is very helpful!","category":"general_qa"}
{"instruction":"[D] Moving my threshold using few shot examples","context":"I have a BERT based classifier and have decided that I want a different threshold for my model\u2019s decision boundary. I have a only a few (dozen) examples of labels that exemplify this new threshold. It seems to me shifting the last layer predictions to this new decision boundary without gradient training should be easy and wouldn\u2019t need many examples.\nAny ideas on how to implement this?","response":"But how do I edit this threshold into my model?","category":"misc"}
{"instruction":"[R] Our new classification algorithm outperforms CatBoost, XGBoost, LightGBM on five benchmark datasets, on accuracy and response time","context":"Hi All!\n\nWe're happy to share LinearBoost, our latest development in machine learning classification algorithms. LinearBoost is based on boosting a linear classifier to significantly enhance performance. Our testing shows it outperforms traditional GBDT algorithms in terms of accuracy and response time across five well-known datasets.  \nThe key to LinearBoost's enhanced performance lies in its approach at each estimator stage. Unlike decision trees used in GBDTs, which select features sequentially, LinearBoost utilizes a linear classifier as its building block, considering all available features simultaneously. This comprehensive feature integration allows for more robust decision-making processes at every step.\n\nWe believe LinearBoost can be a valuable tool for both academic research and real-world applications. Check out our results and code in our GitHub repo:\u00a0[https:\/\/github.com\/LinearBoost\/linearboost-classifier](https:\/\/github.com\/LinearBoost\/linearboost-classifier) . The algorithm is in its infancy and has certain limitations as reported in the GitHub repo, but we are working on them in future plans.\n\nWe'd love to get your feedback and suggestions for further improvements, as the algorithm is still in its early stages!","response":"I use the defaults for all of the algorithms (the one proposed and the ones referenced). On the larger datasets, thanks for your suggestion! We are planning to have it.","category":"misc"}
{"instruction":"[D] Neurips 2024 submissions","context":"I just submitted an abstract to Neurips 2024. I was so impressed with my self for being two days early, and yet, my paper ID is over 7000. In the past I recall paper IDs were incremented as openreview received more submissions. Surely, this year it\u2019s not the case! 7000 submissions already?!","response":"19k already!","category":"misc"}
{"instruction":"[R] The Platonic Representation Hypothesis","context":"arxiv: [https:\/\/arxiv.org\/pdf\/2405.07987](https:\/\/arxiv.org\/pdf\/2405.07987)  \nProject page: [https:\/\/phillipi.github.io\/prh\/](https:\/\/phillipi.github.io\/prh\/)  \ngithub: [https:\/\/github.com\/minyoungg\/platonic-rep\/](https:\/\/github.com\/minyoungg\/platonic-rep\/)\n\nInteresting positional paper on the convergence of self-supervised, multi-modal representation learning.","response":"if you try all the combinations of modalities like mentioned, the ones that perform well on a metric that (which I believe is COMPETENCE, which, like I said above, is not that clear what exactly they're measuring. I'm presuming it's an MMLU like task-set). I don't think that statement necessarily implies that the hypothesis space of 2d vision WILL overlap with that of audio. But that is precisely the point. The larger the models get, the more training data they will be fed, increasing the probability of the overlap (look to figure 5 and 6 if you would like to understand it visually). My speculation is that more modality doesn't necessarily lead to better general performance, but if two models with different combination of modalities do perform well, they are aligned.","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"Expanding the modalities that a single NN can be trained on from end to end is going to have significant implications, if the scaling up of text only models has shown us anything. \n\nIf there was a doubt that the neural networks we've seen up to now can serve as the basis for agents that contains an internal \"world model\" or \"understanding,\" then true end-to-end multimodality is exactly what is needed to move to the next step in intelligence. \n\nSure, GPT-4o is not 10x smarter than GPT-4 Turbo. But for what it lacks in vertical intelligence gains, it's clearly showing impressive properties in **horizontal** gains -- reasoning across modalities rather than being highly intelligent in one modality only. \n\nI think what strikes me about the new model is that it shows us that true end-to-end multi-modality is possible -- and if pursued seriously, the final product on the other side looks and operate far more elegantly","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"That's a good point. Decoding schemes and hardware optimization should give identical outputs, or at least within a reasonable margin of error. Maybe they don't even want to mess with that.\n\nQuantization would degrade quality, but I wouldn't be surprised if all of the models were already quantized. Seems like an easy lever to pull to reduce serving costs at minimal quality expense, especially at 8 bit.","category":"misc"}
{"instruction":"[D] Have someone tried to implement KANs from scratch?","context":"Recently I have been hearing a lot about this new architecture (kolmogorov-Arnold Networks) that might bring a new revolution in Deep Learning domain. \n\nSince many years MLP was the only architecture that was being used to solve any problem using neural networks, thus announcement of this new architecture is definitely a break through. Though many times in the past, lot of people had tried to do so but unfortunately they were unsuccessful.\n\nIf you still don't know about it, you could take help of following resources \ud83d\udc47\ud83c\udffb\n\nHere is the research paper:\nhttps:\/\/arxiv.org\/abs\/2404.19756\n\nAnd the explanation video of this paper:\nhttps:\/\/youtu.be\/-PFIkkwWdnM\n\nAnd if you have tried to implement it or found some video implementing it from scratch. Consider tagging the link in the comments.","response":"What's the tldr here for monarch matrices. Does it actually replace the mlp or is it a more efficient way of doing mlp operations or something else?","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"All you really need is the audio samples to go with the text. All those audiobooks out there are filled with the data needed to decode emotional content, change tone, etc.\n\nSpeed change seems like it could be a fairly simple set of adjustable parameters that could be tuned through RLHF.","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"I mean, on most benchmarks other than ELO it performs very, very slightly better than GPT-4T. This actually just reduces my trust in lmsys, because GPT-4o still gets very, very basic production code just completely wrong. It\u2019s still bad at math, coding, struggles on the same logic puzzles, and has the same awful writing style. It feels similar to GPT-4T\n\nOn twitter I have seen more people agreeing with my description than with yours.\ud83e\udd37\n\nAlso, I tested your question on GPT-3.5 and it gets it right too. I am still not enthused.","category":"misc"}
{"instruction":"[D] ACL 2024 Decisions","context":"Decisions for papers committed to ACL 2024 are coming out today (15 May 2024)! Are you ready for Bangkok? \ud83c\uddf9\ud83c\udded\ud83d\udc18","response":"I got meta review 4 but rejected.... It is because the paper was short paper. I am disappointed because i think it can go findings","category":"misc"}
{"instruction":"[P] Real Time Emotion Classification with FER-2013 dataset","context":"So I am doing an internship project at a company that is as the title says.I basically need to classify human faces into 7 categories- Anger, disgust, happy, etc. Currently I'm trying to achieve good accuracy on FER 2013 dataset then I'll move to the Real Time capture part\n\nI need to finish this project in like 2 weeks' time. I have tried transfer learning with models like **mobile\\_net, VGG19, ResNet50, Inception, Efficient\\_net** and my **training accuracy has reached to like 87% but validation accuracy is pretty low \\~56%** (MAJOR overfitting, ik).\n\nCan the smart folks here help me out with some suggestions on how to better perform transfer learning, whether I should use data augmentation or not( I have around 28000 training images), and about should I use **vision transformer**, etc. ?\n\nwith VGG19 and Inception , for some reason my validation accuracy gets stuck at 24.71% and doesn't change after it\n\nResNet50, mobile\\_net and Efficient\\_net are giving the metrics as stated above\n\nThis is a sample notebook I've been using for transfer learning  \n[https:\/\/colab.research.google.com\/drive\/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing](https:\/\/colab.research.google.com\/drive\/1DeJzEs7imQy4lItWA11bFB4mSdZ95YgN?usp=sharing)\n\nAny and all help is appreciated!","response":"I have a lot of experience with emotion in general. I know this isn\u2019t what vision scientists want to hear but emotions can be felt without visible changes in face or body movement. Ground truth labels can be highly flawed. I\u2019ve never tried a vision approach specifically for this because I\u2019ve relied on electrodermal feedback signals. That all being said, try ViTs but don\u2019t expect it to be real-time. At least you\u2019ll get a better signal on accuracy with vision I guess.","category":"misc"}
{"instruction":"[D] How do unets achieve spatial consistency?","context":"Hi,\nI have been reading through unet pytorch implementations here https:\/\/github.com\/lucidrains\/denoising-diffusion-pytorch but I do not yet understand how a pixel in the process of denoising ever \u201eknows\u201c its (relative) position in the image. While the amount of noise is conditioned on each pixel using embedding of the time Parameter, this is not done for the spatial position?\n\nSo when denoising an image of the cat starting from pure noise, what makes the unet create the head of the cat on the top and the feet at the bottom of the image? Or denoising portraits, the hair is on top and the neck at the bottom?\n\nI think the convolution kernels might maintain local spatial coherence within their sphere of influence, but this feels \u201enot enough\u201c. \n\nNeither is the input image downsampled into the size of the innermost convolution kernels. In the referred code examples, they sample a128x128 into 8x8 on bottom layer. This is then again 3-convoluted, so not covering the entire area.\n\nSo How can the unet achieve spatial consistency\/spatial  auto-conditioning?\n\nThanks","response":"In general I guess this is correct, in the current case the mem_kv seems to consist of just few extra pixels per attention head. Its not an entire map.","category":"general_qa"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"Because very often, the research uses proprietary code from whatever company is paying for it, or the company decides that keeping the code might be more profitable. Another reason that happens with industrial robotics is that you would need some very platform-specific\/home-made tools that you would aslo need to release.\n\nAlso, releasing and maintaining a decent non-trivial repo requires diverting resources, and not every company can do this.\n\nI think that if the math\/idea looks solid and interesting, not providing code shouldn't be an issue. Especially since people can also be dishonest with their code (e.g. I remember a thread here where people were complaining about some repo where the seeds were carefully cherry picked to hide failure cases)\n\nEdit: I'm not super sure why I'm getting downvoted.","category":"general_qa"}
{"instruction":"Nanogpt alternative [D]","context":"I'm looking for an llm that can be trained from scratch. Like nanogpt. Doesn't have to be an instruction model\n\nAlso, does anyone know if there's an llm that can be trained using c4 or the pile? Without changing the input data? \n\nThanks","response":"Great thanks","category":"misc"}
{"instruction":"[R] Marcus Hutter's work on Universal Artificial Intelligence","context":"Marcus Hutter, a senior researcher at Google DeepMind, has written two books on Universal Artificial Intelligence (UAI), [one](https:\/\/www.amazon.co.uk\/Universal-Artificial-Intelligence-Algorithmic-Probability\/dp\/3540221395) in 2005 and [one](https:\/\/www.amazon.co.uk\/Introduction-Universal-Artificial-Intelligence-Robotics\/dp\/1032607025\/ref=tmm_pap_swatch_0?_encoding=UTF8&dib_tag=se&dib=eyJ2IjoiMSJ9.NQXwV4nHA40OJ9MGbpv5kvf5NlJiziWM4A1lMgIjKwzGjHj071QN20LucGBJIEps.0hshp0D52mU8Bvnfe5mzB-Eb-kylVab2oTDBZKM0dHI&qid=1715416552&sr=1-1) hot off the press in 2024. The main goal of UAI is to develop a mathematical theory for combining sequential prediction (which seeks to predict the distribution of the next observation) together with action (which seeks to maximize expected reward), since these are among the problems that intelligent agents face when interacting in an unknown environment. Solomonoff induction provides a universal approach to sequence prediction in that it constructs an optimal prior (in a certain sense) over the space of all computable distributions of sequences, thus enabling Bayesian updating to enable convergence to the true predictive distribution (assuming the latter is computable). Combining Solomonoff induction with optimal action leads us to an agent known as AIXI, which in this theoretical setting, can be argued to be a mathematical incarnation of artificial general intelligence (AGI): it is an agent which acts optimally in general, unknown environments. More generally, Shane Legg and Marcus Hutter have proposed a definition of \"universal intelligence\" in their paper [https:\/\/arxiv.org\/abs\/0712.3329](https:\/\/arxiv.org\/abs\/0712.3329)\n\nIn my technical whiteboard conversation with Hutter, we cover aspects of Universal AI in detail:\n\nhttps:\/\/preview.redd.it\/o6700v1udrzc1.png?width=3329&format=png&auto=webp&s=c00b825dbd4d7c266ffec5a31d994661348bff49\n\nYoutube:\u00a0[https:\/\/www.youtube.com\/watch?v=7TgOwMW\\_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO](https:\/\/www.youtube.com\/watch?v=7TgOwMW_rnk&list=PL0uWtVBhzF5AzYKq5rI7gom5WU1iwPIZO)\n\n  \nOutline:\n\nI. Introduction\n\n* 00:38 : Biography\n* 01:45 : From Physics to AI\n* 03:05 : Hutter Prize\n* 06:25 : Overview of Universal Artificial Intelligence\n* 11:10 : Technical outline\n\nII. Universal Prediction\n\n* 18:27 : Laplace\u2019s Rule and Bayesian Sequence Prediction\n* 40:54 : Different priors: KT estimator\n* 44:39 : Sequence prediction for countable hypothesis class\n* 53:23 : Generalized Solomonoff Bound (GSB)\n* 57:56 : Example of GSB for uniform prior\n* 1:04:24 : GSB for continuous hypothesis classes\n* 1:08:28 : Context tree weighting\n* 1:12:31 : Kolmogorov complexity\n* 1:19:36 : Solomonoff Bound & Solomonoff Induction\n* 1:21:27 : Optimality of Solomonoff Induction\n* 1:24:48 : Solomonoff a priori distribution in terms of random Turing machines\n* 1:28:37 : Large Language Models (LLMs)\n* 1:37:07 : Using LLMs to emulate Solomonoff induction\n* 1:41:41 : Loss functions\n* 1:50:59 : Optimality of Solomonoff induction revisited\n* 1:51:51 : Marvin Minsky\n\nIII. Universal Agents\n\n* 1:52:42 : Recap and intro\n* 1:55:59 : Setup\n* 2:06:32 : Bayesian mixture environment\n* 2:08:02 : AIxi. Bayes optimal policy vs optimal policy\n* 2:11:27 : AIXI (AIxi with xi = Solomonoff a priori distribution)\n* 2:12:04 : AIXI and AGI 2:12:41 : Legg-Hutter measure of intelligence\n* 2:15:35 : AIXI explicit formula\n* 2:23:53 : Other agents (optimistic agent, Thompson sampling, etc)\n* 2:33:09 : Multiagent setting\n* 2:39:38 : Grain of Truth problem\n* 2:44:38 : Positive solution to Grain of Truth guarantees convergence to a Nash equilibria\n* 2:45:01 : Computable approximations (simplifying assumptions on model classes): MDP, CTW, LLMs\n* 2:56:13 : Outro: Brief philosophical remarks","response":"I know man. I know. Those were very interesting, and hence I put them all up, it is sure shot contradictory.","category":"misc"}
{"instruction":"[Discussion] MICCAI 2024 decisions","context":"Hi all,\n\nI thought this might be a good place to discuss about MICCAI 2024 decisions (early accept, rebuttal, early reject). The email mentions that there were 2869 submissions this year (+21% as compared to last year) and around 54% of them have been invited for rebuttal.\n\nI got a rebuttal invitation for an application paper and all the reviewers mentioned \"lack of technical novelty\" as the weakness, so I ended up getting a Weak Accept (4), Weak Reject (3), and Reject (2). I believe I can write a decent rebuttal countering most of reviewers points. But given the low scores, does anyone think there is any hope for this paper getting accepted? Does the rebuttal make any difference for low scoring papers (after the first round)? What fraction of papers in the rebuttal phase were finally got an acceptance last year?","response":"It's worth trying. I have seen rebuttal increased the scores by 1-2 per reviewer in the previous MICCAI. The rate of acceptance in rebuttal phase might be 19\/54 ~ 35% in my estimation.","category":"misc"}
{"instruction":"[D] Seminal papers list since 2018 that will be considered cannon in the future","context":"Hi there,\n\nA recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML\/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  \n\nWhere should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","response":"You think GPT-2 invented the idea of scale? What kind of kool-aid are you drinking?\n\nWe\u2019ve understood the benefits of scale since Alex-Net. Since earlier even.","category":"misc"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"I submit code with all my papers and it never fails that I get at least one reviewer who says something like \u201cI have a question about *minor thing* and it would help if the authors provided code\u201d and then I get to explain that I did, in fact, submit my code. \ud83d\ude43","category":"general_qa"}
{"instruction":"[D] We are forming a study group!","context":"Hello people we are group of 2 right now who are complete beginner in ML. So before starting we would like to expand our group so we all be in the same boat. \n\nOur aim is to start slow and have deep understanding  of ML algorithms and practicing daily and help each other.\n\nSo if anybody interested you can approach us.\n\nThis group is not only for beginners it's also for pro as well as they can mentor others.\n\nDM me if u want to join!","response":"Sure.Please DM me","category":"misc"}
{"instruction":"[D] Is BERT still relevant in 2024 for an EMNLP submission?","context":"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being \"out of date\"?\n\nMy idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","response":"I don't think it should have ever been considered an LLM. It's not large and not a language model.","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"How does twitter Hype help im-also-a-good-gpt2-chatbot in LMSys Arena? I have not used it, but assume the model name is not shown when the rate is asked to compare the outputs to their promt from two models?","category":"misc"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"Most code written by phds is going to be a spaghetti mess and you wouldn\u2019t be able to use it anyway","category":"general_qa"}
{"instruction":"[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis","context":"https:\/\/arxiv.org\/pdf\/2405.08790","response":"I retired from ML a decade ago and thought this might be a good paper to catch up with what's new in the field. If I understand correctly it seems to be making the claim that piece-wise fitted curves (\"spline\") make better fits than linear fits.\n\nI don't think that's true. If you make the linear fits small enough they can fit anything the splines can do. And probably faster too. Lines are simpler and so will not bog down the computer calculating with curves. \n\nOne of my favorite tools was Ross Quinlan's Cubist (aka M5). It (along with its discrete cousin C5.0) solved a lot of problems for me, quickly. I loved the fact that it spit out rules which essentially explained how each model worked.\n\nOr am I missing what Kolmogorov-Arnold is actually doing here?","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"I wouldn\u2019t dismiss it so easily if I were you, do you have evidence that it disappoints as much as other models when you go beyond primitive tasks? Or are you assuming that\u2019s the case since that\u2019s been the trend with recent models?\n\nThis model seems to prove to be much much better when it comes to unique out of distribution tasks that require complex interactions like real world scenarios that it wasn\u2019t trained on, for example this person has had GPT-4-turbo and Claude Opus attempt to play Pok\u00e9mon red by interacting with buttons and reacting to the latest instance of events happening in the game, the coherence of Claude 3 Opus and GPT-4 breaks down quickly in this task even when a lot of prompt engineering is attempted, but GPT4o seems to handle it not only decently but actually great.  It properly interacts with the components and actions in the game and successfully even seeming to learn and remember the actions as it goes along, at the same time it\u2019s way cheaper and better latency than claude 3 opus and turbo.\n\nhttps:\/\/x.com\/VictorTaelin\/status\/1790185366693024155","category":"misc"}
{"instruction":"[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis","context":"https:\/\/arxiv.org\/pdf\/2405.08790","response":"Perhaps. But we have many more infinitely trivial LLM papers claiming SOTA on this or that benchmark, so I welcome a different approach on datasets that aren\u2019t natural language.","category":"misc"}
{"instruction":"[D] Full causal self-attention layer in O(NlogN) computation steps and O(logN) time rather than O(N^2) computation steps and O(1) time, with a big caveat, but hope for the future.","context":"\\*Update\\*: Actually O(N) computation steps(not O(Nlog N)) and O(log N) time.\n\nI think I figured out how to do self-attention in transformer models in O(NlogN) computation steps rather than O(N\\^2), with a caveat. I'm not trying to be an academic, so I don't care to publish this formally, but I thought that some people might be interested. My construction is not efficient or practical, but the fact that it can be done at all might motivate further work to find efficient alternatives.\n\ntl;dr Use the parallel scan\\[1\\] technique to compute taylor series basis functions needed to compute the causal self-attention layer and sum these together weighted by the values vector and 1 to get the numerator and denominator of the softmax activation of the full causal self-attention layer. The basis functions that you have to compute are both the basis functions for the numerator of the self-attention layer, $$\\\\sum\\_{i=0}\\^{j-1} k(i)\\_a\\^n q(j)\\_b\\^m v(i)$$ and the normalization $\\\\sum\\_{i=0}\\^{j-1} k(i)\\_a\\^n q(j)\\_b\\^m$. k(i)\\_a\\^n is component-a of the ith key vector raised to the power of n multiplied by q(j)\\_b\\^m which is component-b of the jth query vector raised to the power of m, which is multiplied by the value vector at position i in the first equation and by 1 in the second, and all summed together. Once you can do this, you've computed a basis function for a Taylor series. Multiply each basis function by a coefficient and sum them together to create an arbitrary function of k(i) and q(j). Using this technique, we can compute the Taylor series approximation for the numerator and the denominator of the softmax activation each taking logN \\* {number of coefficients} parallel steps, or O(N) sequential steps by treating the accumulation as a type of RNN.\n\n# Background\n\nI was inspired to think about this because I was implementing MAMBA\\[2\\] and trying to understand what kind of non-linearities can be created using the parallel scan technique. The parallel scan technique is a way of parallelizing recursive formulas. If you don't know what parallel scan is, let me demonstrate with an example. The simplest example of the parallel scan technique is computing all partial sums of a sequence of numbers in log(N) time. Imagine you have a sequence \\[a\\_1, a\\_2, a\\_3, a\\_4, ...\\]. You can compute all partial sums by first adding a\\_i to a\\_{i -1}, where a\\_{-1} is zero, and generally a\\_{-n} is defined to be zero. Then take the result, call it r = \\[a\\_1, a\\_1+a\\_2, a\\_2 + a\\_3, ...\\], and compute r\\_i + r\\_{i-2}, which gives \\[a\\_1, a\\_1+a\\_2, a\\_1+a\\_2+a\\_3, ...\\]. The first 4 partial sums are already complete. The next step would be r\\_i + r\\_{i-2\\*\\*2}, and the next step, just increase the power of 2 until i-2\\*\\*power is negative for every i in the sequence. It basically sums groups, and then sums those groups together, and so on and so forth until the partial sum at each position is calculated.  The scan technique is a way to parallelize an RNN. Essentially, you remove some nonlinearities in the RNN so that recurrence equation becomes associative. Once it is associative, you can compute the hidden state at each position of the sequence in log N parallel steps, where each parallel step has O(N) parallel computations.\n\n# The Meat of It\n\nIn the background section, I explained how to compute a partial sum in O(log(N)) time and O(NlogN) computation steps (or O(N) time and O(N) computation steps by using RNNs) using the parallel scan technique. I'll use this now to construct the Taylor series for causal self-attention layer used in transformer models.\n\nLet's assume we have a tensor x of shape (sequence\\_length, embedding\\_dim), and we can compute the query, key and value tensors from x using q=Qx, k=Kx and v=Vx, where Q, K and V are matrices. Compute y = (k\\[:,i\\]\\*\\*n)\\*v. Now use the parallel scan technique to accumulate the partial sums of every vector in y, which will give ParallelPartialSum(y)=\\[y\\[0,:\\], y\\[0,:\\]+y\\[1,:\\], ...\\]. Now multiply the result by q\\[:,j\\]\\*\\*m, and now we have a basis function for a Taylor series expansion. The full formula is q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)\\*v). Next, we can add up these functions for different powers of n and m using coefficients to approximate any function. The final equation is \\\\sum\\_{n, m} A\\_{n, m} q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)\\*v).\n\nWhat is left is to find the Taylor series coefficients A\\_{n, m} and to calculate the normalization for the softmax. I'm not actually going to give an equation for A\\_{n, m}, but I will show that it can be done. First, I'm just going to write $q \\\\cdot k$ in place of $q\\[:,j,:\\] \\\\cdot k\\[:,i,:\\]$ to make it easier to write and read. We want the Taylor series of $exp(q \\\\cdot k) = 1 + (q \\\\cdot k) + (q \\\\cdot k)\\*\\*2 \/ 2! + ... + (q \\\\cdot k)\\*\\*n \/ n! + ...$. To find the Taylor series coefficient for every component of q and component of k and every power of each, you'd have to expand out (q \\\\cdot k)\\*\\*n \/n! for every n. It can be done but I'm not going to do it. Just assume that A\\_{n, m} is equal to these coefficients, and voila, we have the numerator of the softmax equation for self-attention. We still need the denominator. To compute the denominator of the softmax over attention scores, you compute the same sum replacing the value tensor with the number 1. $\\\\sum\\_{n, m} A\\_{n, m} x\\[:,j\\]\\*\\*m \\* ParallelPartialSum((x\\[:,i\\]\\*\\*n))$, where again the value vector at the end of the equation is removed. The final equation for the causal self-attention layer is:\n\n$$  \n(\\\\sum\\_{n, m} A\\_{n, m} q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)\\*v)) \/ (\\\\sum\\_{n, m} A\\_{n, m} q\\[:,j\\]\\*\\*m \\* ParallelPartialSum((k\\[:,i\\]\\*\\*n)))  \n$$\n\nWhere again, A\\_{n, m} are the Taylor series coefficients for exp( q \\\\cdot k).\n\n# Take-Aways\n\nOne big take away from this work, is that since causal self-attention can be calculated using the parallel scan technique, and since a parallel scan can be computed with an RNN, it follows that full causal self-attention can be computed with RNNs. The caveat is that you need many RNNs, one for each Taylor series basis function, so to get a good enough approximation of the softmax activation, you'd probably need a lot of coefficients, more than would be practical. On the other hand, what if there is a related activation that does the job of the softmax, but can be constructed with far fewer parallel scans? Then full causal self-attention could be done using only a few RNNs. Also, there are other basis functions that can be computed with one parallel scan, for instance, basis functions for a Fourier series can be computed with one parallel scan.\n\nNon-linear activations are necessary for neural networks to work well. Linear RNNs can be parallelized using parallel scans, and since it is a linear function, one might think that this technique is not as powerful as other neural network layers. One shouldn't make the mistake to think that only linear RNN can be parallelized with linear scans. Non-linear RNNs can also be parallelized so long as the recursive update rule is associative. One might think that this restriction somehow makes the model weaker, I did, at first. But if associative recursion formulas are enough to create transformers(albeit inefficiently), then it stands to reason that they can do anything a transformer can, which is a lot. The only question is whether it's possible to come up with an efficient activation. Maybe MAMBA already did, maybe there is something better.\n\n\\[1\\] [https:\/\/en.wikipedia.org\/wiki\/Prefix\\_sum](https:\/\/en.wikipedia.org\/wiki\/Prefix_sum)\n\n\\[2\\] [https:\/\/arxiv.org\/abs\/2312.00752](https:\/\/arxiv.org\/abs\/2312.00752)\n\n# Update\n\nActually there is a better algorithm for the parallel scan given in the wiki link above\\[1\\]. That means that causal self-attention can be calculated with O(log N) time and O(N) steps instead of O(NlogN) steps.\n\n# Update 2\n\n@[Lajamerr\\_Mittesdine](https:\/\/www.reddit.com\/user\/Lajamerr_Mittesdine\/) Started some code to implement the algorithm in a comment below. I made some changes to it, and the result is below. Thanks @[Lajamerr\\_Mittesdine](https:\/\/www.reddit.com\/user\/Lajamerr_Mittesdine\/)! Also, I want to reiterate that this is not meant to be an efficient or practical implementation of the self-attention. Each taylor series basis function takes logN time and NlogN computation, but you would need a lot of basis functions to properly approximate the softmax of attention scores. Alternatively, the algorithm can be ran in recursive mode, which turns it into an RNN that runs in O(N) steps. This is more to show that self-attention can be implemented as many RNNs running in parallel. To make this efficient, a different formula for self-attention would have to be used, not the softmax of the dot product of queries and keys, but something else that can be computed with few parallel scans.\n\n    import numpy as np\n    \n    # note, there is a slighlty more efficient algorithm for partial sums that computes in O(log(N)) time and O(N) computation. This one runs in O(log(N)) time and O(NlogN) computation. See the wiki link for the more efficient algorithm.\n    def parallel_partial_sum(arr): \n        \"\"\"Parallel scan (prefix sum) implementation.\"\"\"\n        n = len(arr)\n        steps = np.ceil(np.log2(n))\n        \n        for i in range(steps):\n            # check if this is the numerator or denominator\n            if len(arr.shape)==2:            \n                array += np.concatenate([np.zeros_like(arr[:2**i,:]), arr[(n-2**i):,:]], axis=0)\n            else:\n                array += np.concatenate([np.zeros_like(arr[:2**i]), arr[(n-2**i):]], axis=0)\n    \n        return arr\n    \n    def compute_taylor_basis_function(q, k, v, n, m, i, j):\n        \"\"\"Compute a Taylor basis function for given powers n and m.\"\"\"\n        k_power = np.power(k[:,i], n)  # k[:,i]^n element-wise\n        q_power = np.power(q[:,j], m)  # q[:,j]^m element-wise\n        if len(v.shape) == 2:\n            k_power = np.expand_dims(k_power, axis=-1) # change: maybe needs this to properly broadcast\n            q_power = np.expand_dims(q_power, axis=-1)\n        partial_sum_kv = parallel_partial_sum(k_power * v)\n        basis_function = q_power * partial_sum_kv\n        return basis_function\n    \n    def compute_causal_self_attention(q, k, v, max_n=3, max_m=3):\n        \"\"\"Compute the causal self-attention using Taylor series approximation.\"\"\"\n        attention_numerator = np.zeros_like(v)\n        attention_denominator = np.zeros_like(v[:,0])\n    \n        for n in range(max_n + 1):\n            for m in range(max_m + 1):\n                for j in range(q.shape[-1]):\n                    for i in range(k.shape[-1]):\n                        # note, either i or j loop can be removed because basis functions can be computed in parallel\n                        A_nmij = 1.0  # Simplified coefficient for illustration\n                        basis_function = compute_taylor_basis_function(q, k, v, n, m, i, j)\n                        attention_numerator += A_nmij * basis_function\n                        normalization_basis_function = compute_taylor_basis_function(q, k, np.ones_like(attention_denominator), n, m, i, j)\n                        attention_denominator += A_nmij * normalization_basis_function\n        \n        attention_denominator = np.expand_dims(attention_denominator, axis=-1)\n        attention = attention_numerator \/ attention_denominator\n        return attention\n    \n    # Example usage\n    sequence_length = 10\n    embedding_dim = 4\n    \n    # Randomly initialize q, k, v tensors\n    q = np.random.rand(sequence_length, embedding_dim)\n    k = np.random.rand(sequence_length, embedding_dim)\n    v = np.random.rand(sequence_length, embedding_dim)\n    \n    # Compute the causal self-attention\n    attention_output = compute_causal_self_attention(q, k, v)\n    \n    print(\"Causal Self-Attention Output:\")\n    print(attention_output)","response":"Structured State space model?","category":"misc"}
{"instruction":"[Discussion] What are SOTA Uncertainty Quantification Methods for Neural Networks?","context":"I recently started to look into this topic and I am curious which methods are SOTA and used in production?  To be more specific, I am interested in modeling aleatoric and epistemic uncertainty for a neural network. In an ideal setting my model tells me when it encounters inputs that are out-of-distribution and expresses it's uncertainty for a given input in respect to the systems noise.\n\nEDIT: I am mainly working with regression problems.\n\nThanks in advance! :)","response":"Might be but i guess that there are methods out there that work model-agnostic \/ orthogonal to the neural network architecture you choose.","category":"general_qa"}
{"instruction":"[D] GPT-4o \"natively\" multi-modal, what does this actually mean?","context":"What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?\n\nE.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system \"self-select\" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","response":"They entered the chat because other hardware makers are coming hard.  Everyone else wants to hedge against Nvidia being their only hardware\u2026 they want to hedge against other companies changing hardware.  Also, vertical integration. If companies can pay them what they charge there is a lot of money in it.","category":"general_qa"}
{"instruction":"[D] GPT-4o \"natively\" multi-modal, what does this actually mean?","context":"What are your best guesses on how it works (training and architecture) vs. the typical VL formula of pretrained vision encoder + pretrained LLM -> fine-tune with multimodal tasks?\n\nE.g. Is it fully mixed modality pre-training the entire system? Does model embed all modalities into a shared space for prediction? Does the system \"self-select\" the modality of output tokens (can flexibly choose to output audio vs. text based on input tokens) or is this user specified?","response":"I guess they're doing something along the lines of LanguageBind: https:\/\/arxiv.org\/abs\/2310.01852\n\nUse modality specific encoders with some contrastive losses to learn multimodal relationships. Then fine tune for your task. LanguageBind pairs each modality with language, so you can contrast pairs that don't correspond.","category":"general_qa"}
{"instruction":"Tips for improving my VAE [Project]","context":"Hi everyone,\n\nI'm currently working on a project where I use a VAE to perform inverse design of 3D models (voxels comprised of 1s and 0s). Below, I've attached an image of my loss curve. It seems that model is overfitting when it comes to reconstruction loss, but does well with KL loss. Any suggestions for how I can improve the reconstruction loss?\n\nAlso my loss values are to the scale of 1e6, I'm not sure if this is necessarily a bad thing, but the images generated from the model aren't terrible.\n\nhttps:\/\/preview.redd.it\/phoqiit5no0d1.png?width=1719&format=png&auto=webp&s=a33a7a0468548bf180c81ff506db96e0a91fd557\n\nFor further context, I am using convolutional layers for upsampling and downsampling. I've added KL annealing and a learning rate scheduler. Also, I use BCE loss for my reconstruction loss, I tried MSE loss but performance was worse and it didn't really make sense since the models are binary not continuous.\n\nI appreciate any suggestions!","response":"How many input features do you have ? And what is the size of the latent  vector?it is important to Normalize inputs. How many data do you have for training ? What is the batch size ?to really understand the problem you must check mean and sigma values","category":"misc"}
{"instruction":"[D] Is BERT still relevant in 2024 for an EMNLP submission?","context":"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being \"out of date\"?\n\nMy idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","response":"What's the upside of decoder only? You don't get info about the labels on training?","category":"misc"}
{"instruction":"[D] Seminal papers list since 2018 that will be considered cannon in the future","context":"Hi there,\n\nA recent grad here that finally has some time to learn the actual interesting stuff. I want to get myself familiar with modern machine learning. I read the most well-known paper like Attention is all you Need, CLIP, Vision Transformers, but I am sure that I missed the majority of the important papers. Jumping directly into reading recent ICML\/NIPS won't do me good as I feel like I have much to cover in the fundamentals.  \n\nWhere should I start? I am familiar with ML and DL until 2018-ish, familiar with the vanilla transformer but that is basically it.","response":"The NeRF and guassian splatting papers are big ones too.","category":"misc"}
{"instruction":"[D] Please consider signing this letter to open source AlphaFold3","context":"https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSf6ioZPbxiDZy5h4qxo-bHa0XOTOxEYHObht0SX8EgwfPHY_g\/viewform\n\nGoogle DeepMind very recently released their new iteration of AlphaFold, AF3. AF3 achieves SoTA in predicting unseen protein structures from just the amino acid sequence. This iteration also adds capability for joint structure prediction of various other complexes such as nucleic acids, small molecules, ions, and modified residues.\n\nAF3 is a powerful bioinformatics tool that could help facilitate research worldwide. Unfortunately, Google DeepMind chooses to keep it closed source.\n\nPlease sign the letter !\n\nAF3 : https:\/\/www.nature.com\/articles\/s41586-024-07487-w","response":"Then I fear you don\u2019t understand how patents works.","category":"misc"}
{"instruction":"[D] Is BERT still relevant in 2024 for an EMNLP submission?","context":"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being \"out of date\"?\n\nMy idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","response":"Is bert a \"large\" model ?","category":"misc"}
{"instruction":"[N] GPT-4o","context":"https:\/\/openai.com\/index\/hello-gpt-4o\/\n\n- this is the im-also-a-good-gpt2-chatbot (current chatbot arena sota)\n- multimodal \n- faster and freely available on the web","response":"OpenAI\u2019s description of the model is:\n\n>With GPT-4o, we trained a single new model end-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by the same neural network. Because GPT-4o is our first model combining all of these modalities, we are still just scratching the surface of exploring what the model can do and its limitations.\n\nThat doesn\u2019t sound like an iterative update that tapes and glues together stuff in a nice wrapper \/ gui.","category":"misc"}
{"instruction":"[D] What's up with papers without code?","context":"I recently do a project on face anti spoofing, and during my research, I found that almost no papers provide implementation codes. In a field where reproducibility is so important, why do people still accept papers with no implementation?","response":"code release is not mandatory for most fields. Infact even electrical engineering (not so far removed from CS) it is not commonplace to release code. Other times if the work is from industry there are many hoops to get code released and it may not even be possible. I do agree ideally papers should have code release when possible to minimize noise and value reproducibility.\n\nTry emailing the authors.","category":"general_qa"}
{"instruction":"[D] Is BERT still relevant in 2024 for an EMNLP submission?","context":"Is active learning with BERT (for certain applications) still a relevant paradigm to submit papers under? Or is this like of work likely to be rejected based on being \"out of date\"?\n\nMy idea is related to using BERT for medical classification, and I'm sure that LLMs may perform better. Wondering whether it would be worth it to invest time into a big push to get results for this.","response":"Why would you use causal masking for a classification task?","category":"misc"}
{"instruction":"[P] New KANs paper just dropped: Kolmogorov-Arnold Networks (KANs) for Time Series Analysis","context":"https:\/\/arxiv.org\/pdf\/2405.08790","response":"I beg to differ. We need as much of those as we can and then to compile the results so we don't repeat experiments.","category":"misc"}
